{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j01aH0PR4Sg-"
   },
   "source": [
    "# Nina Stawski's (group 90) final project report [DRAFT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illinois ID: ninas2\n",
    "\n",
    "\n",
    "[GitHub repo link](https://github.com/nstawski/dlh-final-project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sfk8Zrul_E8V"
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# disabling the cell since I am not using it, but keeping in the notebook in case I need it in the future.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "## Background of the problem\n",
    "\n",
    "### Type of problem\n",
    "  \n",
    "  This is a data preparation and processing problem. The authors of the article are testing a common belief that adding more data improves the resulting model performance. Their main hypothesis, which they subsequently prove, is that incorporating more data does not necessary improve the model performance. It can introduce spurious correlations, and hurt the resulting model performance rather than helping it.\n",
    "\n",
    "### What is the importance/meaning of solving the problem\n",
    "  \n",
    "  The paper is challenging a common belief, meaning a lot of researchers are likely trying to incorporate as much data as they can expecting it would improve the performance of their models. The outcome of this research would provide guidance on the possible pitfalls and the cases where you wouldn't want to add external data - so it could set a new standard of processing and incorporating data for everyone in the field.\n",
    "\n",
    "### The difficulty of the problem\n",
    "\n",
    "  The problem is non-obvious and the paper is challenging the common belief held in the industry. The authors are putting a lot of state-of-the-art approaches to the test, and attempt to quantify the results as well as provide new standards and explanations. This is extremely hard to do so I believe the problem is difficult.\n",
    "\n",
    "### The state of the art methods and effectiveness\n",
    "\n",
    "  The \"industry standard\" way of improving model performance is adding more data from additional datasets, which the authors of this article prove to not be effective, and even being harmful in many cases.\n",
    "\n",
    "  One of the main issues causing the model performance decrease when adding more data from other sources is spurious correlations, which in case of x-rays could be coming even from the scanner artifacts, or other hospital-specific data. One of the state-of-the-art ways to mitigate this is balancing a dataset to reduce the influence of hospital-specific factors. While balancing definitely improved the situation, the resulting model performance was still in many cases worse than with a single-hospital dataset.\n",
    "\n",
    "\n",
    "## Paper explanation\n",
    "### What did the paper propose\n",
    "The paper used four most-used chest x-ray datasets - MIMIC-CXR-JPG, CheXpert, PadChest, ChestXray8 - to disprove a popular belief that adding more data always would improve the performance of your model. They postulate that, for the specific x-ray data, even the scanners themselves, the way hospitals produce data, or send specific patients to specific places to do their scan, can introduce spurious correlations which, in many cases, significantly affect the worst group performance.\n",
    "\n",
    "### What is the innovations of the method\n",
    "Existing research (for example, John R Zech, Marcus A Badgeley, Manway Liu, Anthony B Costa, Joseph J Titano, and Eric Karl Oermann. Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study. PLoS medicine, 15(11): e1002683, 2018.) proves that adding a second dataset improves the average per-group accuracy. In contrast, the paper I am reproducing focuses on the worst per-group accuracy.\n",
    "\n",
    "### How well the proposed method work (in its own metrics)\n",
    "According to the article authors, their method works really well and proves that in nearly 50% of cases adding a second dataset, and even balancing it to reduce spurious correllations doesn't get the model to perform better than without that additional dataset. The models pick up on hospital-specific features even if those features weren't explicitly defined in the original data. They postulate that every CNN model, regardless of training disease or datasets, learns embeddings that can distinguish any of the hospital sources with near-perfect accuracy, even if the embeddings were trained via one or two hospitals’ data.\n",
    "\n",
    "### What is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n",
    "\n",
    "The article cautions against blindly adding more datasets, and provides a number of approaches you can take if you still decide to do so. The conclusion is adding more data shouldn't be done blindly. The authors of the article definitely discourage the researchers from the most common approach of throwing data at the problem to improve model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "# Scope of Reproducibility:\n",
    "\n",
    "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
    "\n",
    "![Paper overview](https://raw.githubusercontent.com/basedrhys/ood-generalization/5d8ff09eba4c0b4b20b5ae2814fe865bed1dfb0e/img/high_level_overview.png)\n",
    "\n",
    "## Hypothesis 1\n",
    "\n",
    "In 43% of training dataset/disease tasks, adding data from an external source hurts worst-group performance.\n",
    "\n",
    "\n",
    "## Hypothesis 2\n",
    "\n",
    "Balancing the dataset to reduce spurious correlations is often beneficial, but in the scenarios where adding an additional data source hurts generalization performance, it does not always improve generalization; in some cases, training on a balanced dataset achieves lower worst-group accuracy than training on datasets from one or two hospitals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rRksCB1vbYwJ"
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# disabling the cell since I am not using it, but keeping in the notebook in case I need it in the future.\n",
    "\n",
    "# no code is required for this section\n",
    "'''\n",
    "if you want to use an image outside this notebook for explanaition,\n",
    "you can upload it to your google drive and show it with OpenCV or matplotlib\n",
    "'''\n",
    "# mount this notebook to your google drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# define dirs to workspace and data\n",
    "img_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-your-image>'\n",
    "\n",
    "import cv2\n",
    "img = cv2.imread(img_dir)\n",
    "cv2.imshow(\"Title\", img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "# Methodology\n",
    "\n",
    "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
    "\n",
    "The methodology at least contains two subsections **data** and **model** in your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yu61Jp1xrnKk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Data_Constants' from '/Users/noemi/dlh-final-project/Data_Constants.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import  packages you need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from os.path import exists\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "import random\n",
    "from IPython.display import display\n",
    "# from google.colab import drive\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import Data_Constants as Constants\n",
    "\n",
    "#making sure all referenced files are reloaded\n",
    "import importlib\n",
    "importlib.reload(Constants)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "#  Data\n",
    "The study is using four datasets: MIMIC-CXR-JPG, CheXpert, PadChest, ChestXray8\n",
    "\n",
    "The datasets are being filtered to include only frontal (PA/AP) images. Instances are labeled with one or more pathologies. Each dataset has a different set of diseases but they are preprocessed using code derived from ClinicalDG2 (Zhang et al., 2021) to extract the eight common labels and homogenize the datasets. Additionally, authors of the article created the Any label which indicates a positive label for any of the seven common disease labels, resulting in nine different binary labels. All experiments use the labels in a binary manner; a pathology is chosen as the target label, with an instance labeled 1 if the pathology of interest is present and 0 otherwise. \n",
    "\n",
    "The autors apply an 80%/10%/10% subject-wise train/val/test split, with the same split used across seeds.\n",
    "\n",
    "### MIMIC-CXR\n",
    "\n",
    "1. [Obtain access](https://mimic-cxr.mit.edu/about/access/) to the MIMIC-CXR-JPG Database Database on PhysioNet and download the [dataset](https://physionet.org/content/mimic-cxr-jpg/2.0.0/). The best option is downloading from the GCP bucket:\n",
    "\n",
    "```sh\n",
    "gcloud auth login\n",
    "mkdir MIMIC-CXR-JPG\n",
    "gsutil -m rsync -d -r gs://mimic-cxr-jpg-2.0.0.physionet.org MIMIC-CXR-JPG\n",
    "```\n",
    "\n",
    "2. In order to obtain gender information for each patient, you will need to obtain access to [MIMIC-IV](https://physionet.org/content/mimiciv/0.4/). Download `core/patients.csv.gz` and place the file in the `MIMIC-CXR-JPG` directory.\n",
    "\n",
    "### CheXpert\n",
    "1. Sign up with your email address [here](https://stanfordmlgroup.github.io/competitions/chexpert/).\n",
    "\n",
    "2. Download either the original or the downsampled dataset (we recommend the downsampled version - `CheXpert-v1.0-small.zip`) and extract it.\n",
    "\n",
    "### ChestX-ray8\n",
    "\n",
    "1. Download the `images` folder and `Data_Entry_2017_v2020.csv` from the [NIH website](https://nihcc.app.box.com/v/ChestXray-NIHCC).\n",
    "\n",
    "2. Unzip all of the files in the `images` folder.\n",
    "\n",
    "### PadChest\n",
    "\n",
    "1. The paper uses a resized version of PadChest, which can be downloaded [here](https://academictorrents.com/details/96ebb4f92b85929eadfb16761f310a6d04105797).\n",
    "\n",
    "2. Unzip `images-224.tar`.\n",
    "\n",
    "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
    "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
    "  * Illustration: printing results, plotting figures for illustration.\n",
    "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset.\n",
    "  \n",
    "## Data Processing\n",
    "The original pre-processing for the article was done using the scripts outside of the Jupyter Notebook. Some of them didnt' work for me, and the installation process didn't succeed despite multiple attempts either. Instead, I have adapted some of the original scripts to run in the notebook (with some modifications so they actually work with my data), using the external \"Constants.py\" file that points to the location of the datasets.\n",
    "1. In `./Data_Constants.py`, update `image_paths` to point to each of the four directories that you downloaded.\n",
    "\n",
    "2. Run the next two cells to pre-process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzVUQS0CHry0"
   },
   "source": [
    "## Validating\n",
    "I am using the validation and pre-processing code provided by the authors of the article, with some modifications to make it run as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making sure constants are up to date if they were changed\n",
    "importlib.reload(Constants)\n",
    "\n",
    "def validate_mimic():\n",
    "    img_dir = Path(Constants.image_paths['MIMIC'])\n",
    "    meta_dir = Path(Constants.meta_paths['MIMIC'])\n",
    "    \n",
    "    assert (meta_dir/'mimic-cxr-2.0.0-metadata.csv').is_file()\n",
    "    assert (meta_dir/'mimic-cxr-2.0.0-negbio.csv').is_file()\n",
    "    assert (meta_dir/'patients.csv').is_file()\n",
    "    # modified the file that's being checked since I don't have the full MIMIC-CXR-JPG dataset due to space limitations\n",
    "    # in the original script, the file in p19 was being checked.\n",
    "    assert (img_dir/'p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg').is_file()\n",
    "\n",
    "def validate_cxp():\n",
    "    img_dir = Path(Constants.image_paths['CXP'])\n",
    "    if (img_dir/'CheXpert-v1.0').is_dir():\n",
    "        cxp_subfolder = 'CheXpert-v1.0'\n",
    "    else:\n",
    "        cxp_subfolder = 'CheXpert-v1.0-small'\n",
    "    assert (img_dir/cxp_subfolder/'train.csv').is_file()\n",
    "    assert (img_dir/cxp_subfolder/'train/patient48822/study1/view1_frontal.jpg').is_file()\n",
    "    assert (img_dir/cxp_subfolder/'valid/patient64636/study1/view1_frontal.jpg').is_file()\n",
    "\n",
    "def validate_pad():\n",
    "    img_dir = Path(Constants.image_paths['PAD'])\n",
    "    meta_dir = Path(Constants.meta_paths['PAD'])\n",
    "    assert (meta_dir/'PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv').is_file()\n",
    "    assert (img_dir/'185566798805711692534207714722577525271_qb3lyn.png').is_file()\n",
    "\n",
    "def validate_nih():\n",
    "    img_dir = Path(Constants.image_paths['NIH'])\n",
    "    meta_dir = Path(Constants.meta_paths['NIH'])\n",
    "    assert (meta_dir/'Data_Entry_2017.csv').is_file()\n",
    "    assert (img_dir/'images/00002072_003.png').is_file()\n",
    "\n",
    "def validate_splits():\n",
    "    for dataset in Constants.df_paths:\n",
    "        for split in Constants.df_paths[dataset]:\n",
    "            assert Path(Constants.df_paths[dataset][split]).is_file()\n",
    "\n",
    "\n",
    "def validate_all():\n",
    "    validate_mimic()\n",
    "    validate_cxp()\n",
    "    validate_nih()\n",
    "    validate_pad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating paths...\n",
      "Preprocessing MIMIC-CXR...\n",
      "Preprocessing CheXpert...\n",
      "Preprocessing ChestX-ray8...\n",
      "Preprocessing PadChest... This might take a few minutes...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# making sure constants are up to date if they were changed after running this notebook\n",
    "importlib.reload(Constants)\n",
    "\n",
    "def preprocess_mimic():\n",
    "    img_dir = Path(Constants.image_paths['MIMIC'])\n",
    "    meta_dir = Path(Constants.meta_paths['MIMIC'])\n",
    "    out_folder = meta_dir/'clinicaldg'\n",
    "    out_folder.mkdir(parents = True, exist_ok = True)  \n",
    "\n",
    "    patients = pd.read_csv(meta_dir/'patients.csv')\n",
    "    labels = pd.read_csv(meta_dir/'mimic-cxr-2.0.0-negbio.csv')\n",
    "    meta = pd.read_csv(meta_dir/'mimic-cxr-2.0.0-metadata.csv')\n",
    "\n",
    "    df = meta.merge(patients, on = 'subject_id').merge(labels, on = ['subject_id', 'study_id'])\n",
    "    df['age_decile'] = pd.cut(df['anchor_age'], bins = list(range(0, 100, 10))).apply(lambda x: f'{x.left}-{x.right}').astype(str)\n",
    "    df['frontal'] = df.ViewPosition.isin(['AP', 'PA'])\n",
    "\n",
    "    df['path'] = df.apply(lambda x: os.path.join(f'p{str(x[\"subject_id\"])[:2]}', f'p{x[\"subject_id\"]}', f's{x[\"study_id\"]}', f'{x[\"dicom_id\"]}.jpg'), axis = 1)\n",
    "    df.to_csv(out_folder/\"preprocessed.csv\", index=False)\n",
    "\n",
    "def preprocess_pad():\n",
    "    # I have modified this function from the original one, because I was getting missing/ambiguous Dtype errors\n",
    "    img_dir = Path(Constants.image_paths['PAD'])\n",
    "    meta_dir = Path(Constants.meta_paths['PAD'])\n",
    "    out_folder = meta_dir/'clinicaldg'\n",
    "    out_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    dtype_spec = {\n",
    "        'ImageID': str,\n",
    "        'StudyID': str,\n",
    "        'PatientID': str,\n",
    "        'PatientBirth': str, # converting this to the integer later to avoid processing errors (due some data apparently being saved as float)\n",
    "        'PatientSex_DICOM': str,\n",
    "        'ViewPosition_DICOM': str,\n",
    "        'Projection': str,\n",
    "        'Labels': str,\n",
    "        'WindowCenter_DICOM': str,\n",
    "        'WindowWidth_DICOM': str\n",
    "    }\n",
    "\n",
    "    df = pd.read_csv(meta_dir/'PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv', dtype=dtype_spec)\n",
    "    df = df[['ImageID', 'StudyID', 'PatientID', 'PatientBirth', 'PatientSex_DICOM', 'ViewPosition_DICOM', 'Projection', 'Labels']]\n",
    "    df = df[~df[\"Labels\"].isnull()]\n",
    "    df = df[df[\"ImageID\"].apply(lambda x: os.path.exists(os.path.join(img_dir, x)))]\n",
    "    df = df[df.Projection.isin(['PA', 'L', 'AP_horizontal', 'AP'])]\n",
    "\n",
    "    df['frontal'] = ~(df['Projection'] == 'L')\n",
    "    df = df[~df['Labels'].apply(lambda x: 'exclude' in x or 'unchanged' in x)]\n",
    "\n",
    "    mapping = dict()\n",
    "    mapping['Effusion'] = ['hydropneumothorax', 'empyema', 'hemothorax']\n",
    "    mapping[\"Consolidation\"] = [\"air bronchogram\"]\n",
    "    mapping['No Finding'] = ['normal']\n",
    "\n",
    "    for pathology in Constants.take_labels:\n",
    "        mask = df[\"Labels\"].str.contains(pathology.lower())\n",
    "        if pathology in mapping:\n",
    "            for syn in mapping[pathology]:\n",
    "                mask |= df[\"Labels\"].str.contains(syn.lower())\n",
    "        df[pathology] = mask.astype(int)\n",
    "\n",
    "    df['PatientBirth'] = df['PatientBirth'].dropna().astype(float).astype(int)\n",
    "    df['Age'] = 2017 - df['PatientBirth']\n",
    "    df.reset_index(drop=True).to_csv(out_folder/\"preprocessed.csv\", index=False)\n",
    "\n",
    "\n",
    "def preprocess_cxp():\n",
    "    img_dir = Path(Constants.image_paths['CXP'])\n",
    "    out_folder = img_dir/'clinicaldg'\n",
    "    if (img_dir/'CheXpert-v1.0'/'train.csv').is_file():\n",
    "        df = pd.concat([pd.read_csv(img_dir/'CheXpert-v1.0'/'train.csv'), \n",
    "                        pd.read_csv(img_dir/'CheXpert-v1.0'/'valid.csv')],\n",
    "                        ignore_index = True)\n",
    "    elif (img_dir/'CheXpert-v1.0-small'/'train.csv').is_file(): \n",
    "        df = pd.concat([pd.read_csv(img_dir/'CheXpert-v1.0-small'/'train.csv'),\n",
    "                        pd.read_csv(img_dir/'CheXpert-v1.0-small'/'valid.csv')],\n",
    "                        ignore_index = True)\n",
    "    elif (img_dir/'train.csv').is_file():\n",
    "        raise ValueError('Please set Constants.image_paths[\"CXP\"] to be the PARENT of the current'+\n",
    "                ' directory and rerun this script.')\n",
    "    else:\n",
    "        raise ValueError(\"CheXpert files not found!\")\n",
    "\n",
    "    out_folder.mkdir(parents = True, exist_ok = True)  \n",
    "\n",
    "    df['subject_id'] = df['Path'].apply(lambda x: int(Path(x).parent.parent.name[7:]))\n",
    "    df['Path'] = df['Path'].apply(lambda x: str(x).replace(\"CheXpert-v1.0/\", \"\"))\n",
    "    df.reset_index(drop = True).to_csv(out_folder/\"preprocessed.csv\", index=False)\n",
    "\n",
    "def preprocess_nih():\n",
    "    img_dir = Path(Constants.image_paths['NIH'])\n",
    "    meta_dir = Path(Constants.meta_paths['NIH'])\n",
    "    out_folder = meta_dir/'clinicaldg'\n",
    "    out_folder.mkdir(parents = True, exist_ok = True)  \n",
    "    df = pd.read_csv(meta_dir/\"Data_Entry_2017.csv\")\n",
    "    df['labels'] = df['Finding Labels'].apply(lambda x: x.split('|'))\n",
    "\n",
    "    for label in Constants.take_labels:\n",
    "        df[label] = df['labels'].apply(lambda x: label in x)\n",
    "    df.reset_index(drop = True).to_csv(out_folder/\"preprocessed.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Validating paths...\")\n",
    "    validate_all()\n",
    "    print(\"Preprocessing MIMIC-CXR...\")\n",
    "    preprocess_mimic()\n",
    "    print(\"Preprocessing CheXpert...\")\n",
    "    preprocess_cxp()\n",
    "    print(\"Preprocessing ChestX-ray8...\")\n",
    "    preprocess_nih()\n",
    "    print(\"Preprocessing PadChest... This might take a few minutes...\")\n",
    "    preprocess_pad()\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we need to resize and process the data.\n",
    "I am using the code provided by the authors of the article to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_MIMIC(split, only_frontal):  \n",
    "    copy_subjectid = split['subject_id']     \n",
    "    split = split.drop(columns = ['subject_id']).replace(\n",
    "            [[None], -1, \"[False]\", \"[True]\", \"[ True]\", 'UNABLE TO OBTAIN', 'UNKNOWN', 'MARRIED', 'LIFE PARTNER',\n",
    "             'DIVORCED', 'SEPARATED', '0-10', '10-20', '20-30', '30-40', '40-50', '50-60', '60-70', '70-80', '80-90',\n",
    "             '>=90'],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 'MARRIED/LIFE PARTNER', 'MARRIED/LIFE PARTNER', 'DIVORCED/SEPARATED',\n",
    "             'DIVORCED/SEPARATED', '0-20', '0-20', '20-40', '20-40', '40-60', '40-60', '60-80', '60-80', '80-', '80-'])\n",
    "    \n",
    "    split['subject_id'] = copy_subjectid.astype(str)\n",
    "    split['study_id'] = split['study_id'].astype(str)\n",
    "    split['Age'] = split[\"age_decile\"]\n",
    "    split['Sex'] = split[\"gender\"]\n",
    "    split = split.rename(\n",
    "        columns = {\n",
    "            'Pleural Effusion':'Effusion',   \n",
    "        })\n",
    "    split['path'] = split['path'].astype(str).apply(lambda x: os.path.join(Constants.image_paths['MIMIC'], x))\n",
    "    if only_frontal:\n",
    "        split = split[split.frontal]\n",
    "        \n",
    "    split['env'] = 'MIMIC'  \n",
    "    split.loc[split.Age == 0, 'Age'] = '0-20'\n",
    "    \n",
    "    return split[['subject_id','path','Sex',\"Age\", 'env', 'frontal', 'study_id'] + Constants.take_labels]\n",
    "\n",
    "def process_NIH(split, only_frontal = True):\n",
    "    split['Patient Age'] = np.where(split['Patient Age'].between(0,19), 19, split['Patient Age'])\n",
    "    split['Patient Age'] = np.where(split['Patient Age'].between(20,39), 39, split['Patient Age'])\n",
    "    split['Patient Age'] = np.where(split['Patient Age'].between(40,59), 59, split['Patient Age'])\n",
    "    split['Patient Age'] = np.where(split['Patient Age'].between(60,79), 79, split['Patient Age'])\n",
    "    split['Patient Age'] = np.where(split['Patient Age']>=80, 81, split['Patient Age'])\n",
    "    \n",
    "    copy_subjectid = split['Patient ID'] \n",
    "    \n",
    "    split = split.drop(columns = ['Patient ID']).replace([[None], -1, \"[False]\", \"[True]\", \"[ True]\", 19, 39, 59, 79, 81], \n",
    "                            [0, 0, 0, 1, 1, \"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-\"])\n",
    "   \n",
    "    split['subject_id'] = copy_subjectid.astype(str)\n",
    "    split['Sex'] = split['Patient Gender'] \n",
    "    split['Age'] = split['Patient Age']\n",
    "    split = split.drop(columns=[\"Patient Gender\", 'Patient Age'])\n",
    "    split['path'] = split['Image Index'].astype(str).apply(lambda x: os.path.join(Constants.image_paths['NIH'], 'images', x))\n",
    "    split['env'] = 'NIH'\n",
    "    split['frontal'] = True\n",
    "    split['study_id'] = split['subject_id'].astype(str)\n",
    "    return split[['subject_id','path','Sex',\"Age\", 'env', 'frontal','study_id'] + Constants.take_labels]\n",
    "\n",
    "\n",
    "def process_CXP(split, only_frontal):\n",
    "    split['Age'] = np.where(split['Age'].between(0,19), 19, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(20,39), 39, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(40,59), 59, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(60,79), 79, split['Age'])\n",
    "    split['Age'] = np.where(split['Age']>=80, 81, split['Age'])\n",
    "    \n",
    "    copy_subjectid = split['subject_id'] \n",
    "    split = split.drop(columns = ['subject_id']).replace([[None], -1, \"[False]\", \"[True]\", \"[ True]\", 19, 39, 59, 79, 81], \n",
    "                            [0, 0, 0, 1, 1, \"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-\"])\n",
    "    \n",
    "    split['subject_id'] = copy_subjectid.astype(str)\n",
    "    split['Sex'] = np.where(split['Sex']=='Female', 'F', split['Sex'])\n",
    "    split['Sex'] = np.where(split['Sex']=='Male', 'M', split['Sex'])\n",
    "    split = split.rename(\n",
    "        columns = {\n",
    "            'Pleural Effusion':'Effusion',\n",
    "            'Lung Opacity': 'Airspace Opacity'        \n",
    "        })\n",
    "    split['path'] = split['Path'].astype(str).apply(lambda x: os.path.join(Constants.image_paths['CXP'], x))\n",
    "    split['frontal'] = (split['Frontal/Lateral'] == 'Frontal')\n",
    "    if only_frontal:\n",
    "        split = split[split['frontal']]\n",
    "    split['env'] = 'CXP'\n",
    "    split['study_id'] = split['path'].apply(lambda x: x[x.index('patient'):x.rindex('/')])\n",
    "    return split[['subject_id','path','Sex',\"Age\", 'env', 'frontal','study_id'] + Constants.take_labels]\n",
    "\n",
    "\n",
    "def process_PAD(split, only_frontal):\n",
    "    split['Age'] = np.where(split['Age'].between(0,19), 19, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(20,39), 39, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(40,59), 59, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(60,79), 79, split['Age'])\n",
    "    split['Age'] = np.where(split['Age']>=80, 81, split['Age'])\n",
    "    \n",
    "    split = split.replace([[None], -1, \"[False]\", \"[True]\", \"[ True]\", 19, 39, 59, 79, 81], \n",
    "                            [0, 0, 0, 1, 1, \"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-\"])\n",
    "    \n",
    "    split.loc[split['Age'] == 0.0, 'Age'] = '0-20'\n",
    "    split.loc[split['Age'].isnull(), 'Age'] = '0-20'\n",
    "    split = split.rename(columns = {\n",
    "        'PatientID': 'subject_id',\n",
    "        'StudyID': 'study_id',\n",
    "        'PatientSex_DICOM' :'Sex'        \n",
    "    })\n",
    "    \n",
    "    split.loc[~split['Sex'].isin(['M', 'F', 'O']), 'Sex'] = 'O'\n",
    "    split['path'] =  split['ImageID'].astype(str).apply(lambda x: os.path.join(Constants.image_paths['PAD'], x))\n",
    "    if only_frontal:\n",
    "        split = split[split['frontal']]\n",
    "    split['env'] = 'PAD'\n",
    "    return split[['subject_id','path','Sex',\"Age\", 'env', 'frontal','study_id'] + Constants.take_labels]\n",
    "\n",
    "\n",
    "def split(df, split_portions = (0.8, 0.9), seed=0):\n",
    "    # We don't want the data splits to be affected by seed\n",
    "    # So lets temporarily set the seed to a static value...\n",
    "    rand_state = np.random.get_state()\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Split our data (irrespective of the random seed provided in train.py)\n",
    "    subject_df = pd.DataFrame({'subject_id': np.sort(df['subject_id'].unique())})\n",
    "    subject_df['random_number'] = np.random.uniform(size=len(subject_df))\n",
    "\n",
    "    train_id = subject_df[subject_df['random_number'] <= split_portions[0]].drop(columns=['random_number'])\n",
    "    valid_id = subject_df[(subject_df['random_number'] > split_portions[0]) & (subject_df['random_number'] <= split_portions[1])].drop(columns=['random_number'])\n",
    "    test_id = subject_df[subject_df['random_number'] > split_portions[1]].drop(columns=['random_number'])\n",
    "\n",
    "    train_df = df[df.subject_id.isin(train_id.subject_id)]\n",
    "    valid_df = df[df.subject_id.isin(valid_id.subject_id)]\n",
    "    test_df = df[df.subject_id.isin(test_id.subject_id)]  \n",
    "\n",
    "    # ...then return the random state back to what it was\n",
    "    np.random.set_state(rand_state)\n",
    "\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "def get_process_func(env):\n",
    "    if env == 'MIMIC':\n",
    "        return process_MIMIC\n",
    "    elif env == 'NIH':\n",
    "        return process_NIH\n",
    "    elif env == 'CXP':\n",
    "        return process_CXP\n",
    "    elif env == 'PAD':\n",
    "        return process_PAD\n",
    "    else:\n",
    "        raise NotImplementedError   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show data paths from constants\n",
    "Constants.df_paths\n",
    "\n",
    "def img_exists(path):\n",
    "    return exists(path)\n",
    "\n",
    "def is_diseased(row):\n",
    "    # diseases = Constants.take_labels[1:]\n",
    "    return int((row[Constants.take_labels[1:]]).sum() > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cell is pre-processing the data and will take a long time to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below needs to run once, after that everything is saved into the CSV file and can be loaded from there. this block of code needs to re-run only if the data changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This might take a while.\n",
      "Processing: MIMIC\n",
      "Got processing function, filtering by only frontal...\n",
      "Filtering out the data without images...\n",
      "False    167664\n",
      "True      63047\n",
      "Name: img_exists, dtype: int64\n",
      "Adding \"All\" column...\n",
      "Saving results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>env</th>\n",
       "      <th>frontal</th>\n",
       "      <th>study_id</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>img_exists</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>50414267</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>53189527</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000032</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>53911762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10000032</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>53911762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10000032</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>56699142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97453</th>\n",
       "      <td>12742782</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>54917116</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97456</th>\n",
       "      <td>12742898</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>20-40</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>53339588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97461</th>\n",
       "      <td>12743572</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>51989892</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97463</th>\n",
       "      <td>12743572</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>52648347</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97467</th>\n",
       "      <td>12743572</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>52929291</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63047 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject_id                                               path Sex  \\\n",
       "0       10000032  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "2       10000032  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "4       10000032  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "5       10000032  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "6       10000032  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "...          ...                                                ...  ..   \n",
       "97453   12742782  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "97456   12742898  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "97461   12743572  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "97463   12743572  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "97467   12743572  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "\n",
       "         Age    env  frontal  study_id  No Finding  Atelectasis  Cardiomegaly  \\\n",
       "0      40-60  MIMIC     True  50414267         1.0          0.0           0.0   \n",
       "2      40-60  MIMIC     True  53189527         1.0          0.0           0.0   \n",
       "4      40-60  MIMIC     True  53911762         1.0          0.0           0.0   \n",
       "5      40-60  MIMIC     True  53911762         1.0          0.0           0.0   \n",
       "6      40-60  MIMIC     True  56699142         1.0          0.0           0.0   \n",
       "...      ...    ...      ...       ...         ...          ...           ...   \n",
       "97453  40-60  MIMIC     True  54917116         1.0          0.0           0.0   \n",
       "97456  20-40  MIMIC     True  53339588         0.0          0.0           0.0   \n",
       "97461  60-80  MIMIC     True  51989892         1.0          0.0           0.0   \n",
       "97463  60-80  MIMIC     True  52648347         1.0          0.0           0.0   \n",
       "97467  60-80  MIMIC     True  52929291         1.0          0.0           0.0   \n",
       "\n",
       "       Effusion  Pneumonia  Pneumothorax  Consolidation  Edema  img_exists  \\\n",
       "0           0.0        0.0           0.0            0.0    0.0        True   \n",
       "2           0.0        0.0           0.0            0.0    0.0        True   \n",
       "4           0.0        0.0           0.0            0.0    0.0        True   \n",
       "5           0.0        0.0           0.0            0.0    0.0        True   \n",
       "6           0.0        0.0           0.0            0.0    0.0        True   \n",
       "...         ...        ...           ...            ...    ...         ...   \n",
       "97453       0.0        0.0           0.0            0.0    0.0        True   \n",
       "97456       0.0        1.0           0.0            0.0    0.0        True   \n",
       "97461       0.0        0.0           0.0            0.0    0.0        True   \n",
       "97463       0.0        0.0           0.0            0.0    0.0        True   \n",
       "97467       0.0        0.0           0.0            0.0    0.0        True   \n",
       "\n",
       "       All  \n",
       "0        0  \n",
       "2        0  \n",
       "4        0  \n",
       "5        0  \n",
       "6        0  \n",
       "...    ...  \n",
       "97453    0  \n",
       "97456    1  \n",
       "97461    0  \n",
       "97463    0  \n",
       "97467    0  \n",
       "\n",
       "[63047 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: CXP\n",
      "Got processing function, filtering by only frontal...\n",
      "Filtering out the data without images...\n",
      "True    191229\n",
      "Name: img_exists, dtype: int64\n",
      "Adding \"All\" column...\n",
      "Saving results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>env</th>\n",
       "      <th>frontal</th>\n",
       "      <th>study_id</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>img_exists</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00001/study1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00002/study2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00002/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00003/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>20-40</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00004/study1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223643</th>\n",
       "      <td>64736</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64736/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223644</th>\n",
       "      <td>64737</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64737/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223645</th>\n",
       "      <td>64738</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64738/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223646</th>\n",
       "      <td>64739</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64739/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223647</th>\n",
       "      <td>64740</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>M</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64740/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191229 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id                                               path Sex  \\\n",
       "0               1  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "1               2  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "2               2  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "4               3  /Users/noemi/ood-generalization/data/CheXpert/...   M   \n",
       "5               4  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "...           ...                                                ...  ..   \n",
       "223643      64736  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "223644      64737  /Users/noemi/ood-generalization/data/CheXpert/...   M   \n",
       "223645      64738  /Users/noemi/ood-generalization/data/CheXpert/...   M   \n",
       "223646      64739  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "223647      64740  /Users/noemi/ood-generalization/data/CheXpert/...   M   \n",
       "\n",
       "          Age  env  frontal             study_id  No Finding  Atelectasis  \\\n",
       "0       60-80  CXP     True  patient00001/study1         1.0          0.0   \n",
       "1         80-  CXP     True  patient00002/study2         0.0          0.0   \n",
       "2         80-  CXP     True  patient00002/study1         0.0          0.0   \n",
       "4       40-60  CXP     True  patient00003/study1         0.0          0.0   \n",
       "5       20-40  CXP     True  patient00004/study1         1.0          0.0   \n",
       "...       ...  ...      ...                  ...         ...          ...   \n",
       "223643  40-60  CXP     True  patient64736/study1         0.0          0.0   \n",
       "223644  60-80  CXP     True  patient64737/study1         0.0          0.0   \n",
       "223645  60-80  CXP     True  patient64738/study1         0.0          0.0   \n",
       "223646  40-60  CXP     True  patient64739/study1         0.0          0.0   \n",
       "223647    80-  CXP     True  patient64740/study1         0.0          1.0   \n",
       "\n",
       "        Cardiomegaly  Effusion  Pneumonia  Pneumothorax  Consolidation  Edema  \\\n",
       "0                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "1                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "2                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "4                0.0       0.0        0.0           0.0            0.0    1.0   \n",
       "5                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "...              ...       ...        ...           ...            ...    ...   \n",
       "223643           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "223644           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "223645           1.0       0.0        0.0           0.0            0.0    1.0   \n",
       "223646           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "223647           0.0       1.0        0.0           0.0            0.0    0.0   \n",
       "\n",
       "        img_exists  All  \n",
       "0             True    0  \n",
       "1             True    0  \n",
       "2             True    0  \n",
       "4             True    1  \n",
       "5             True    0  \n",
       "...            ...  ...  \n",
       "223643        True    0  \n",
       "223644        True    0  \n",
       "223645        True    1  \n",
       "223646        True    0  \n",
       "223647        True    1  \n",
       "\n",
       "[191229 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: NIH\n",
      "Got processing function, filtering by only frontal...\n",
      "Filtering out the data without images...\n",
      "True    112120\n",
      "Name: img_exists, dtype: int64\n",
      "Adding \"All\" column...\n",
      "Saving results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>env</th>\n",
       "      <th>frontal</th>\n",
       "      <th>study_id</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>img_exists</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>M</td>\n",
       "      <td>80-</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112115</th>\n",
       "      <td>30801</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>M</td>\n",
       "      <td>20-40</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>30801</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112116</th>\n",
       "      <td>30802</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>M</td>\n",
       "      <td>20-40</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>30802</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112117</th>\n",
       "      <td>30803</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>30803</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112118</th>\n",
       "      <td>30804</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>F</td>\n",
       "      <td>20-40</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>30804</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112119</th>\n",
       "      <td>30805</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>M</td>\n",
       "      <td>20-40</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>30805</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112120 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id                                               path Sex  \\\n",
       "0               1  /Users/noemi/ood-generalization/data/chestxray...   M   \n",
       "1               1  /Users/noemi/ood-generalization/data/chestxray...   M   \n",
       "2               1  /Users/noemi/ood-generalization/data/chestxray...   M   \n",
       "3               2  /Users/noemi/ood-generalization/data/chestxray...   M   \n",
       "4               3  /Users/noemi/ood-generalization/data/chestxray...   F   \n",
       "...           ...                                                ...  ..   \n",
       "112115      30801  /Users/noemi/ood-generalization/data/chestxray...   M   \n",
       "112116      30802  /Users/noemi/ood-generalization/data/chestxray...   M   \n",
       "112117      30803  /Users/noemi/ood-generalization/data/chestxray...   F   \n",
       "112118      30804  /Users/noemi/ood-generalization/data/chestxray...   F   \n",
       "112119      30805  /Users/noemi/ood-generalization/data/chestxray...   M   \n",
       "\n",
       "          Age  env  frontal study_id  No Finding  Atelectasis  Cardiomegaly  \\\n",
       "0       40-60  NIH     True        1       False        False          True   \n",
       "1       40-60  NIH     True        1       False        False          True   \n",
       "2       40-60  NIH     True        1       False        False          True   \n",
       "3         80-  NIH     True        2        True        False         False   \n",
       "4       60-80  NIH     True        3       False        False         False   \n",
       "...       ...  ...      ...      ...         ...          ...           ...   \n",
       "112115  20-40  NIH     True    30801       False        False         False   \n",
       "112116  20-40  NIH     True    30802        True        False         False   \n",
       "112117  40-60  NIH     True    30803        True        False         False   \n",
       "112118  20-40  NIH     True    30804        True        False         False   \n",
       "112119  20-40  NIH     True    30805        True        False         False   \n",
       "\n",
       "        Effusion  Pneumonia  Pneumothorax  Consolidation  Edema  img_exists  \\\n",
       "0          False      False         False          False  False        True   \n",
       "1          False      False         False          False  False        True   \n",
       "2           True      False         False          False  False        True   \n",
       "3          False      False         False          False  False        True   \n",
       "4          False      False         False          False  False        True   \n",
       "...          ...        ...           ...            ...    ...         ...   \n",
       "112115     False       True         False          False  False        True   \n",
       "112116     False      False         False          False  False        True   \n",
       "112117     False      False         False          False  False        True   \n",
       "112118     False      False         False          False  False        True   \n",
       "112119     False      False         False          False  False        True   \n",
       "\n",
       "        All  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "112115    1  \n",
       "112116    0  \n",
       "112117    0  \n",
       "112118    0  \n",
       "112119    0  \n",
       "\n",
       "[112120 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: PAD\n",
      "Got processing function, filtering by only frontal...\n",
      "Filtering out the data without images...\n",
      "True    99827\n",
      "Name: img_exists, dtype: int64\n",
      "Adding \"All\" column...\n",
      "Saving results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>env</th>\n",
       "      <th>frontal</th>\n",
       "      <th>study_id</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>img_exists</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>839860488694292331637988235681460987</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>20536686640136348236148679891455886468</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>313572750430997347502932654319389875966</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>M</td>\n",
       "      <td>80-</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>135803415504923515076821959678074435083</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50783093527901818115346441867348318648</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>113855343774216031107737439268243531979</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>93535126770783451980359712286922420997</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>3137231742710829928-247610802266403640553</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93535126770783451980359712286922420997</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>313723174271082992847610802266403640553</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144479</th>\n",
       "      <td>112930952416074060371371014599496493673</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>1284011361929414522814654121696751542351444145...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144480</th>\n",
       "      <td>282743729971423358706056731890510600934</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>1284011361929414522094646571696751542351444145...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144481</th>\n",
       "      <td>52648743308541843883453242716226652771</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>1284011361929414522086390631696751542351444145...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144482</th>\n",
       "      <td>228646130593152933811948996634154201216</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>1284011361929414522084108901696751542351444145...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144483</th>\n",
       "      <td>137424047230303610602080410284588825286</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>1284011361929414524682119191696751542351444145...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99827 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     subject_id  \\\n",
       "0          839860488694292331637988235681460987   \n",
       "2       313572750430997347502932654319389875966   \n",
       "3        50783093527901818115346441867348318648   \n",
       "6        93535126770783451980359712286922420997   \n",
       "7        93535126770783451980359712286922420997   \n",
       "...                                         ...   \n",
       "144479  112930952416074060371371014599496493673   \n",
       "144480  282743729971423358706056731890510600934   \n",
       "144481   52648743308541843883453242716226652771   \n",
       "144482  228646130593152933811948996634154201216   \n",
       "144483  137424047230303610602080410284588825286   \n",
       "\n",
       "                                                     path Sex    Age  env  \\\n",
       "0       /Users/noemi/ood-generalization/data/PadChest/...   F    80-  PAD   \n",
       "2       /Users/noemi/ood-generalization/data/PadChest/...   M    80-  PAD   \n",
       "3       /Users/noemi/ood-generalization/data/PadChest/...   F    80-  PAD   \n",
       "6       /Users/noemi/ood-generalization/data/PadChest/...   M  60-80  PAD   \n",
       "7       /Users/noemi/ood-generalization/data/PadChest/...   M  60-80  PAD   \n",
       "...                                                   ...  ..    ...  ...   \n",
       "144479  /Users/noemi/ood-generalization/data/PadChest/...   M  60-80  PAD   \n",
       "144480  /Users/noemi/ood-generalization/data/PadChest/...   F  60-80  PAD   \n",
       "144481  /Users/noemi/ood-generalization/data/PadChest/...   M  40-60  PAD   \n",
       "144482  /Users/noemi/ood-generalization/data/PadChest/...   F  60-80  PAD   \n",
       "144483  /Users/noemi/ood-generalization/data/PadChest/...   M  60-80  PAD   \n",
       "\n",
       "        frontal                                           study_id  \\\n",
       "0          True             20536686640136348236148679891455886468   \n",
       "2          True            135803415504923515076821959678074435083   \n",
       "3          True            113855343774216031107737439268243531979   \n",
       "6          True          3137231742710829928-247610802266403640553   \n",
       "7          True            313723174271082992847610802266403640553   \n",
       "...         ...                                                ...   \n",
       "144479     True  1284011361929414522814654121696751542351444145...   \n",
       "144480     True  1284011361929414522094646571696751542351444145...   \n",
       "144481     True  1284011361929414522086390631696751542351444145...   \n",
       "144482     True  1284011361929414522084108901696751542351444145...   \n",
       "144483     True  1284011361929414524682119191696751542351444145...   \n",
       "\n",
       "        No Finding  Atelectasis  Cardiomegaly  Effusion  Pneumonia  \\\n",
       "0                1            0             0         0          0   \n",
       "2                0            0             0         0          0   \n",
       "3                0            0             0         0          0   \n",
       "6                0            1             0         1          0   \n",
       "7                0            0             0         1          0   \n",
       "...            ...          ...           ...       ...        ...   \n",
       "144479           0            0             0         0          0   \n",
       "144480           1            0             0         0          0   \n",
       "144481           0            0             0         0          0   \n",
       "144482           1            0             0         0          0   \n",
       "144483           0            0             0         0          1   \n",
       "\n",
       "        Pneumothorax  Consolidation  Edema  img_exists  All  \n",
       "0                  0              0      0        True    0  \n",
       "2                  0              0      0        True    0  \n",
       "3                  0              0      0        True    0  \n",
       "6                  0              0      0        True    1  \n",
       "7                  0              0      0        True    1  \n",
       "...              ...            ...    ...         ...  ...  \n",
       "144479             0              0      0        True    0  \n",
       "144480             0              0      0        True    0  \n",
       "144481             0              0      0        True    0  \n",
       "144482             0              0      0        True    0  \n",
       "144483             0              0      0        True    1  \n",
       "\n",
       "[99827 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# loads data with random splits\n",
    "print('This might take a while.')\n",
    "\n",
    "for data_env in Constants.df_paths:\n",
    "    print('Processing:', data_env)\n",
    "    func = get_process_func(data_env)\n",
    "    print('Got processing function, filtering by only frontal...')\n",
    "    df_env = func(pd.read_csv(Constants.df_paths[data_env]), only_frontal = True)\n",
    "    print('Filtering out the data without images...')\n",
    "    df_env[\"img_exists\"] = df_env[\"path\"].apply(img_exists)\n",
    "    print(df_env[\"img_exists\"].value_counts())\n",
    "    df_env = df_env[df_env[\"img_exists\"]]\n",
    "    \n",
    "    df_env = df_env.fillna(0)\n",
    "    \n",
    "    print('Adding \"All\" column...')\n",
    "    df_env[\"All\"] = df_env.apply(is_diseased, axis=1)\n",
    "    \n",
    "    print('Saving results...')\n",
    "    df_env.to_csv(f\"{Constants.base_path}/processed/{data_env}.csv\", index=False)\n",
    "    \n",
    "    display(df_env)\n",
    "    \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the data, splitting to all, train, val and test...\n",
      "Source: MIMIC\n",
      "Data length: 63047\n",
      "MIMIC: done.\n",
      "Source: CXP\n",
      "Data length: 191229\n",
      "CXP: done.\n",
      "Source: NIH\n",
      "Data length: 112120\n",
      "NIH: done.\n",
      "Source: PAD\n",
      "Data length: 99827\n",
      "PAD: done.\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "dfs = {}\n",
    "print('Processing the data, splitting to all, train, val and test...')\n",
    "for env in Constants.df_paths:\n",
    "    func = get_process_func(env)\n",
    "    df_env = pd.read_csv(f\"{Constants.base_path}/processed/{env}.csv\")\n",
    "    \n",
    "    print('Source:', env)\n",
    "    print('Data length:', len(df_env))\n",
    "    \n",
    "    train_df, valid_df, test_df = split(df_env)\n",
    "    dfs[env] = {\n",
    "        'all': df_env,\n",
    "        'train': train_df,\n",
    "        'val': valid_df,\n",
    "        'test': test_df\n",
    "    }\n",
    "    print(f'{env}: done.')\n",
    "    \n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prop(df, column=\"Pneumonia\"):\n",
    "    num_instances = len(df)\n",
    "    num_diseased = df[df[column] == 1][column].count()\n",
    "    return num_diseased / (num_instances - num_diseased)\n",
    "\n",
    "def get_resample_class(orig_prop, new_prop, resample_method):\n",
    "    if new_prop > orig_prop:\n",
    "        if resample_method == \"over\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    if new_prop < orig_prop:\n",
    "        if resample_method == \"under\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "def calculate_num_resample(df, orig_prop, new_prop, resample_method):\n",
    "    pass\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def balance_df_label(df, sampler, label_bal=0.05154780337262089, invert=False):\n",
    "    target = df[\"Pneumonia\"] == 1\n",
    "    rus = sampler(random_state=0, sampling_strategy=label_bal if not invert else 1-label_bal - 0.23)\n",
    "    res_df, _ = rus.fit_resample(df, target)\n",
    "\n",
    "    print(f\"Previous pneumonia prop: {get_pneumonia_prop(df)} with {len(df)} instances\")\n",
    "    print(f\"Resampled pneumonia prop: {get_pneumonia_prop(res_df)} with {len(res_df)} instances\")\n",
    "\n",
    "    return res_df\n",
    "\n",
    "def balance_proportion(orig_df, new_df, resample_method=\"over\", column=\"Pneumonia\"):\n",
    "    orig_df = orig_df.fillna(0.0)\n",
    "    orig_prop = get_prop(orig_df, column)\n",
    "    new_prop = get_prop(new_df, column)\n",
    "    assert resample_method in [\"over\", \"under\"]\n",
    "    resample_class = get_resample_class(orig_prop, new_prop, resample_method)\n",
    "    print(f\"Resampling '{column}' via '{resample_method}' on class {resample_class} from {orig_prop} to {new_prop}\")\n",
    "    \n",
    "    # Estimate the number of items we'll need to resample\n",
    "    df_diseased = orig_df[orig_df[column] == 1.0]\n",
    "    df_normal = orig_df[orig_df[column] == 0.0]\n",
    "    num_diseased = len(df_diseased)\n",
    "    num_normal = len(df_normal)\n",
    "    assert num_diseased + num_normal == len(orig_df)\n",
    "    \n",
    "    if resample_method == \"over\":\n",
    "        if resample_class == 0:\n",
    "            new_num_normal = int(num_diseased / new_prop)\n",
    "            print(f\"Resampling normal samples from {num_normal} to {new_num_normal}\")\n",
    "            df_normal_rs = df_normal.sample(new_num_normal, replace=True, random_state=0)\n",
    "            resampled_df = pd.concat([df_normal_rs, df_diseased])\n",
    "        else:\n",
    "            # Resample the pneumonia class\n",
    "            # new_num_diseased = int(new_prop * num_normal)\n",
    "            # print(f\"Resampling diseased samples from {num_diseased} to {new_num_diseased}\")\n",
    "            # df_diseased_rs = df_diseased.sample(new_num_diseased, replace=True, random_state=0)\n",
    "            # resampled_df = pd.concat([df_normal, df_diseased_rs])\n",
    "            target = df[\"Pneumonia\"] == 1\n",
    "            rus = RandomOverSampler(random_state=0, sampling_strategy=new_prop)\n",
    "            resampled_df, _ = rus.fit_resample(df, target)\n",
    "    \n",
    "    resampled_df.sort_index(inplace=True)\n",
    "    print(f\"New df proportion: {get_prop(resampled_df, column)}\")\n",
    "    return resampled_df\n",
    "            \n",
    "# balance_proportion(dfs[\"MIMIC\"][\"train\"], dfs[\"MIMIC\"][\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>env</th>\n",
       "      <th>frontal</th>\n",
       "      <th>study_id</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>img_exists</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00001/study1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00002/study2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00002/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00003/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>20-40</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00004/study1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191222</th>\n",
       "      <td>64734</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64734/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191223</th>\n",
       "      <td>64735</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64735/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191225</th>\n",
       "      <td>64737</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64737/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191227</th>\n",
       "      <td>64739</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64739/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191228</th>\n",
       "      <td>64740</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>M</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64740/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153411 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        subject_id                                               path Sex  \\\n",
       "0                1  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "1                2  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "2                2  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "3                3  /Users/noemi/ood-generalization/data/CheXpert/...   M   \n",
       "4                4  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "...            ...                                                ...  ..   \n",
       "191222       64734  /Users/noemi/ood-generalization/data/CheXpert/...   M   \n",
       "191223       64735  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "191225       64737  /Users/noemi/ood-generalization/data/CheXpert/...   M   \n",
       "191227       64739  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "191228       64740  /Users/noemi/ood-generalization/data/CheXpert/...   M   \n",
       "\n",
       "          Age  env  frontal             study_id  No Finding  Atelectasis  \\\n",
       "0       60-80  CXP     True  patient00001/study1         1.0          0.0   \n",
       "1         80-  CXP     True  patient00002/study2         0.0          0.0   \n",
       "2         80-  CXP     True  patient00002/study1         0.0          0.0   \n",
       "3       40-60  CXP     True  patient00003/study1         0.0          0.0   \n",
       "4       20-40  CXP     True  patient00004/study1         1.0          0.0   \n",
       "...       ...  ...      ...                  ...         ...          ...   \n",
       "191222  40-60  CXP     True  patient64734/study1         0.0          1.0   \n",
       "191223  60-80  CXP     True  patient64735/study1         0.0          1.0   \n",
       "191225  60-80  CXP     True  patient64737/study1         0.0          0.0   \n",
       "191227  40-60  CXP     True  patient64739/study1         0.0          0.0   \n",
       "191228    80-  CXP     True  patient64740/study1         0.0          1.0   \n",
       "\n",
       "        Cardiomegaly  Effusion  Pneumonia  Pneumothorax  Consolidation  Edema  \\\n",
       "0                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "1                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "2                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "3                0.0       0.0        0.0           0.0            0.0    1.0   \n",
       "4                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "...              ...       ...        ...           ...            ...    ...   \n",
       "191222           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "191223           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "191225           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "191227           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "191228           0.0       1.0        0.0           0.0            0.0    0.0   \n",
       "\n",
       "        img_exists  All  \n",
       "0             True    0  \n",
       "1             True    0  \n",
       "2             True    0  \n",
       "3             True    1  \n",
       "4             True    0  \n",
       "...            ...  ...  \n",
       "191222        True    1  \n",
       "191223        True    1  \n",
       "191225        True    0  \n",
       "191227        True    0  \n",
       "191228        True    1  \n",
       "\n",
       "[153411 rows x 17 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"CXP\"][\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjW9bCkouv8O"
   },
   "source": [
    "### Metrics to evaluate my model\n",
    "\n",
    "Similar to the original paper, for each base hospital I plan to choose one additional hospital to include in evaluation (for example, evaluate a model trained on MIMIC data using MIMIC and PAD data).\n",
    "\n",
    "* analyse accuracies within each class for each hospital - the result is a group for the disease class from hospital A, the non-disease class from hospital A, the disease class from hospital B, and the non-disease class from hospital B\n",
    "* Track the worst accuracy of the four groups\n",
    "* Compute AUROC\n",
    "\n",
    "I plan to plot the results and compare them to the results provided in the paper.\n",
    "\n",
    "Since I wasn't yet able to fully complete the previous steps, and instead am stuck on the training portion, this section is a ToDo. I plan to complete it by week of April 21.\n",
    "\n",
    "There are two alternative approaches I can take, depending on the situation:\n",
    "\n",
    "* If I manage to run the original paper code, then this is what I will do, since it should be closest to the original paper\n",
    "* If I won't be able to run the original paper training and validation code on my machine, I will update the code I wrote for training and validation to take it as close as possible to the intent of the original researchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing...\n",
      "Previous pneumonia prop: 0.07020832445788779 with 50242 instances\n",
      "Resampled pneumonia prop: 0.7184424658117837 with 80674 instances\n",
      "Previous pneumonia prop: 0.02491281516815649 with 153411 instances\n",
      "Resampled pneumonia prop: 0.051542603653077855 with 157397 instances\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def balance_df_label(df, sampler, label_bal=0.05154780337262089, invert=False):\n",
    "    target = df[\"Pneumonia\"] == (1 if not invert else 0)\n",
    "    rus = sampler(random_state=42, sampling_strategy=label_bal if not invert else 1-label_bal - 0.23)\n",
    "    res_df, _ = rus.fit_resample(df, target)\n",
    "\n",
    "    print(f\"Previous pneumonia prop: {get_prop(df)} with {len(df)} instances\")\n",
    "    print(f\"Resampled pneumonia prop: {get_prop(res_df)} with {len(res_df)} instances\")\n",
    "\n",
    "    return res_df\n",
    "\n",
    "print('Balancing...')\n",
    "mimic_balanced = balance_df_label(dfs[\"MIMIC\"][\"train\"], RandomOverSampler, invert=True)\n",
    "cxp_balanced = balance_df_label(dfs[\"CXP\"][\"train\"], RandomOverSampler, invert=False)\n",
    "print('Done.')\n",
    "\n",
    "# # Balance the size of the two datasets\n",
    "# n = len(cxp_balanced)\n",
    "# mimic_balanced = mimic_balanced.sample(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a5c8b_row0_col0, #T_a5c8b_row6_col2 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #ecf4fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row0_col1, #T_a5c8b_row0_col2, #T_a5c8b_row5_col0, #T_a5c8b_row5_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #f7fbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row0_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #deebf7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row1_col0 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #bad6eb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row1_col1 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #dae8f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row1_col2 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #f3f8fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row1_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #c7dcef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row2_col0, #T_a5c8b_row4_col2 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #d6e5f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row2_col1, #T_a5c8b_row4_col0 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #a8cee4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row2_col2, #T_a5c8b_row6_col0 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #f5f9fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row2_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #f2f8fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row3_col0 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #8fc2de;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row3_col1 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #58a1cf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a5c8b_row3_col2 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #d0e1f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row3_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #d8e7f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row4_col1 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #d1e2f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row4_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #dbe9f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row5_col1 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #e3eef9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row5_col2, #T_a5c8b_row6_col1 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #eaf3fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row6_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #f1f7fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row7_col0, #T_a5c8b_row7_col1, #T_a5c8b_row8_col2, #T_a5c8b_row8_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #08306b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a5c8b_row7_col2 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #68acd5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a5c8b_row7_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #4090c5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a5c8b_row8_col0 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #3c8cc3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a5c8b_row8_col1 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #e4eff9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row9_col0, #T_a5c8b_row9_col1, #T_a5c8b_row9_col2, #T_a5c8b_row9_col3 {\n",
       "  background-color: none;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a5c8b_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Dataset</th>\n",
       "      <th class=\"col_heading level0 col0\" >MIMIC</th>\n",
       "      <th class=\"col_heading level0 col1\" >CXP</th>\n",
       "      <th class=\"col_heading level0 col2\" >NIH</th>\n",
       "      <th class=\"col_heading level0 col3\" >PAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row0\" class=\"row_heading level0 row0\" >Pneumonia</th>\n",
       "      <td id=\"T_a5c8b_row0_col0\" class=\"data row0 col0\" >6.60%</td>\n",
       "      <td id=\"T_a5c8b_row0_col1\" class=\"data row0 col1\" >2.45%</td>\n",
       "      <td id=\"T_a5c8b_row0_col2\" class=\"data row0 col2\" >1.28%</td>\n",
       "      <td id=\"T_a5c8b_row0_col3\" class=\"data row0 col3\" >4.90%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row1\" class=\"row_heading level0 row1\" >Cardiomegaly</th>\n",
       "      <td id=\"T_a5c8b_row1_col0\" class=\"data row1 col0\" >17.64%</td>\n",
       "      <td id=\"T_a5c8b_row1_col1\" class=\"data row1 col1\" >12.26%</td>\n",
       "      <td id=\"T_a5c8b_row1_col2\" class=\"data row1 col2\" >2.48%</td>\n",
       "      <td id=\"T_a5c8b_row1_col3\" class=\"data row1 col3\" >9.08%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row2\" class=\"row_heading level0 row2\" >Edema</th>\n",
       "      <td id=\"T_a5c8b_row2_col0\" class=\"data row2 col0\" >12.05%</td>\n",
       "      <td id=\"T_a5c8b_row2_col1\" class=\"data row2 col1\" >26.00%</td>\n",
       "      <td id=\"T_a5c8b_row2_col2\" class=\"data row2 col2\" >2.05%</td>\n",
       "      <td id=\"T_a5c8b_row2_col3\" class=\"data row2 col3\" >1.20%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row3\" class=\"row_heading level0 row3\" >Effusion</th>\n",
       "      <td id=\"T_a5c8b_row3_col0\" class=\"data row3 col0\" >23.46%</td>\n",
       "      <td id=\"T_a5c8b_row3_col1\" class=\"data row3 col1\" >40.25%</td>\n",
       "      <td id=\"T_a5c8b_row3_col2\" class=\"data row3 col2\" >11.88%</td>\n",
       "      <td id=\"T_a5c8b_row3_col3\" class=\"data row3 col3\" >6.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row4\" class=\"row_heading level0 row4\" >Atelectasis</th>\n",
       "      <td id=\"T_a5c8b_row4_col0\" class=\"data row4 col0\" >20.30%</td>\n",
       "      <td id=\"T_a5c8b_row4_col1\" class=\"data row4 col1\" >15.58%</td>\n",
       "      <td id=\"T_a5c8b_row4_col2\" class=\"data row4 col2\" >10.31%</td>\n",
       "      <td id=\"T_a5c8b_row4_col3\" class=\"data row4 col3\" >5.49%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row5\" class=\"row_heading level0 row5\" >Pneumothorax</th>\n",
       "      <td id=\"T_a5c8b_row5_col0\" class=\"data row5 col0\" >3.99%</td>\n",
       "      <td id=\"T_a5c8b_row5_col1\" class=\"data row5 col1\" >9.26%</td>\n",
       "      <td id=\"T_a5c8b_row5_col2\" class=\"data row5 col2\" >4.73%</td>\n",
       "      <td id=\"T_a5c8b_row5_col3\" class=\"data row5 col3\" >0.35%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row6\" class=\"row_heading level0 row6\" >Consolidation</th>\n",
       "      <td id=\"T_a5c8b_row6_col0\" class=\"data row6 col0\" >4.64%</td>\n",
       "      <td id=\"T_a5c8b_row6_col1\" class=\"data row6 col1\" >6.81%</td>\n",
       "      <td id=\"T_a5c8b_row6_col2\" class=\"data row6 col2\" >4.16%</td>\n",
       "      <td id=\"T_a5c8b_row6_col3\" class=\"data row6 col3\" >1.56%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row7\" class=\"row_heading level0 row7\" >Any</th>\n",
       "      <td id=\"T_a5c8b_row7_col0\" class=\"data row7 col0\" >51.21%</td>\n",
       "      <td id=\"T_a5c8b_row7_col1\" class=\"data row7 col1\" >70.35%</td>\n",
       "      <td id=\"T_a5c8b_row7_col2\" class=\"data row7 col2\" >28.02%</td>\n",
       "      <td id=\"T_a5c8b_row7_col3\" class=\"data row7 col3\" >23.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row8\" class=\"row_heading level0 row8\" >No Finding</th>\n",
       "      <td id=\"T_a5c8b_row8_col0\" class=\"data row8 col0\" >34.60%</td>\n",
       "      <td id=\"T_a5c8b_row8_col1\" class=\"data row8 col1\" >8.89%</td>\n",
       "      <td id=\"T_a5c8b_row8_col2\" class=\"data row8 col2\" >53.84%</td>\n",
       "      <td id=\"T_a5c8b_row8_col3\" class=\"data row8 col3\" >36.19%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row9\" class=\"row_heading level0 row9\" >Num Instances</th>\n",
       "      <td id=\"T_a5c8b_row9_col0\" class=\"data row9 col0\" >63,047</td>\n",
       "      <td id=\"T_a5c8b_row9_col1\" class=\"data row9 col1\" >191,229</td>\n",
       "      <td id=\"T_a5c8b_row9_col2\" class=\"data row9 col2\" >112,120</td>\n",
       "      <td id=\"T_a5c8b_row9_col3\" class=\"data row9 col3\" >99,827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fbfb3f13ca0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_rows = []\n",
    "num_instances = []\n",
    "\n",
    "disease_labels = [\"Pneumonia\", \"Cardiomegaly\", \"Edema\", \"Effusion\", \"Atelectasis\", \"Pneumothorax\", \"Consolidation\"]\n",
    "target_labels = disease_labels + [\"Any\", \"No Finding\"]\n",
    "all_labels = target_labels + [\"Num Instances\"]\n",
    "\n",
    "for env in dfs:\n",
    "    df = dfs[env]['all']\n",
    "    df['Any'] = (df[disease_labels] > 0).any(axis=1).astype(int)\n",
    "    totals = {}\n",
    "    totals['Dataset'] = env\n",
    "    totals['Num Instances'] = len(df)\n",
    "    num_instances.append(totals['Num Instances'])\n",
    "\n",
    "    for label in target_labels:\n",
    "        if label in df.columns:\n",
    "            totals[label] = df[label].sum() / len(df)\n",
    "        else:\n",
    "            totals[label] = 0.0\n",
    "\n",
    "    stat_rows.append(totals)\n",
    "\n",
    "stat_df = pd.DataFrame(stat_rows)\n",
    "stat_df.set_index('Dataset', inplace=True)\n",
    "\n",
    "ordered_cols = all_labels\n",
    "stat_df = stat_df[ordered_cols]\n",
    "\n",
    "transposed_stat_df = stat_df.T\n",
    "\n",
    "# styled_stat_df = stat_df.style.background_gradient(cmap='Blues', subset=target_labels)\\\n",
    "#     .format({label: \"{:.2%}\" for label in target_labels})\n",
    "\n",
    "styled_transposed_stat_df = transposed_stat_df.style.apply(\n",
    "    lambda x: [\"background-color: lightblue\" if x.name != 'Num Instances' else \"background-color: none\" for i in x],\n",
    "    axis=1\n",
    ").background_gradient(cmap='Blues', subset=pd.IndexSlice[target_labels, :])\n",
    "styled_transposed_stat_df = styled_transposed_stat_df.format(\"{:.2%}\", subset=pd.IndexSlice[target_labels, :])\n",
    "styled_transposed_stat_df = styled_transposed_stat_df.format(\"{:,.0f}\", subset=pd.IndexSlice['Num Instances', :])\n",
    "\n",
    "styled_transposed_stat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the table from the article for comparison:\n",
    "\n",
    "![Table 1](Table_1_article.png)\n",
    "\n",
    "Looks like the distribution of the labels in the original dataset, while not the same, still is close enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": [
    "##   Model\n",
    "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
    "\n",
    "### Model architecture\n",
    "In the article, the authors use the same model architecture as Zhang et al. (2021): a **DenseNet-121** network (Huang et al., 2017) **initialized with pre-trained weights from ImageNet** (Deng et al., 2009). The final layer is replaced with a **two-output linear layer** (for binary classification). For simplicity, the authors only consider binary disease classification.\n",
    "\n",
    "### Model Training\n",
    "For training the network, all images are resized to **224 × 224** and normalized to the ImageNet (Deng et al., 2009) mean and standard deviation.\n",
    "\n",
    "During training, the following image augmentations are applied:\n",
    "* random horizontal flip\n",
    "* random rotation up to 10 degrees\n",
    "* a crop of random size (75% - 100%) and aspect ratio (3/4 to 4/3)\n",
    "\n",
    "All runs use **Adam** with **lr = 1e-5** and **batch size = 128**, which was found to be a performant configuration in early tuning ((Zhang et al., 2021) use lr = 5e-4 and batch size = 32).\n",
    "\n",
    "_[This part I haven't implemented yet]_ Training runs for **a maximum of 20k steps**, with validation occurring every 500 steps and an early stopping patience of 10 validations.\n",
    "\n",
    "All test results are obtained using the optimal model found during training as measured by the highest validation macro-F1 score (following (Fiorillo et al., 2021; Berenguer et al., 2022)) as it gives a robust ranking of model performance under imbalanced labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details for model info:\n",
    "    \n",
    "  * Model architecture: layer number/size/type, activation function, etc\n",
    "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
    "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
    "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
    "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gBdVZoTvsSFV"
   },
   "outputs": [],
   "source": [
    "# This is the model defined and provided by the autors of the article.\n",
    "# While they are using densenet 121 for the article, the provided model code includes other options.\n",
    "\n",
    "class EmbModel(nn.Module):\n",
    "    # I had to add the num_labels parameter to reduce the resulting response to the number of labels we use\n",
    "    def __init__(self, emb_type, feature_size_override, pretrain, concat_features = 0, num_labels = 8):\n",
    "        super().__init__()\n",
    "        self.emb_type = emb_type\n",
    "        self.pretrain = pretrain\n",
    "        self.concat_features = concat_features\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        assert emb_type in [\"densenet121\", \"densenet201\", \"resnet\"], f\"Invalid emb type: {emb_type}\"\n",
    "\n",
    "        if emb_type == 'densenet121':\n",
    "            model = models.densenet121()\n",
    "            self.encoder = nn.Sequential(*list(model.children())[:-1]) #https://discuss.pytorch.org/t/densenet-transfer-learning/7776/2\n",
    "            self.emb_dim = model.classifier.in_features\n",
    "        elif emb_type == 'densenet201':\n",
    "            model = models.densenet201()\n",
    "            self.encoder = nn.Sequential(*list(model.children())[:-1]) #https://discuss.pytorch.org/t/densenet-transfer-learning/7776/2\n",
    "            self.emb_dim = model.classifier.in_features\n",
    "        elif emb_type == 'resnet':\n",
    "            model = models.resnet50()\n",
    "            self.encoder = nn.Sequential(*list(model.children())[:-1])\n",
    "            self.emb_dim = list(model.children())[-1].in_features\n",
    "\n",
    "        print(\"\\nEmb Dim:\")\n",
    "        print(self.emb_dim)\n",
    "\n",
    "        if feature_size_override:\n",
    "            print(f\"Manually setting output dim to {feature_size_override}\")\n",
    "            self.emb_dim = feature_size_override\n",
    "            print(self.emb_dim)\n",
    "            \n",
    "        self.n_outputs = self.emb_dim + concat_features\n",
    "        self.final_layer = nn.Linear(self.n_outputs, self.num_labels)\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.final_layer.weight, mode='fan_out', nonlinearity='sigmoid')\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        if isinstance(inp, dict): # dict with image and additional feature(s) to concat to embedding\n",
    "            x = inp['img']\n",
    "            concat = inp['concat']\n",
    "            assert(concat.shape[-1] == self.concat_features)\n",
    "        else: # tensor image\n",
    "            assert(self.concat_features == 0)\n",
    "            x = inp\n",
    "        \n",
    "        x = self.encoder(x).squeeze(-1).squeeze(-1)\n",
    "        if \"densenet\" in self.emb_type:\n",
    "            x = F.relu(x)\n",
    "            x = F.avg_pool2d(x, kernel_size = 7).view(x.size(0), -1)\n",
    "        \n",
    "        if isinstance(inp, dict):\n",
    "            x = torch.cat([x, concat], dim = -1)\n",
    "            \n",
    "        x = self.final_layer(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Haven't figured out how to make the training from the supplied code work yet, so I am writing my own training code using the standard approach learned in class and homeworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a data loader\n",
    "The authors of the article have a script to load the data in different configurations. I am reusing it partially but wasn't able to make it work yet because of the errors, so I am creating my own Dataset class and a data loader that can deal with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEnvDataset(Dataset):\n",
    "    def __init__(self, dataframes, subset='train', envs=None, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with data from multiple environments and a specific subset.\n",
    "        :param dataframes: A dictionary with environment keys, each containing another dict with subsets as DataFrames.\n",
    "        :param subset: The subset to load ('train', 'val', or 'test').\n",
    "        :param envs: A list of environment names to include. If None, include all.\n",
    "        :param transform: PyTorch transforms to apply to the images.\n",
    "        \"\"\"\n",
    "        if envs is None:\n",
    "            envs = list(dataframes.keys())\n",
    "        \n",
    "        self.data = pd.concat([dataframes[env][subset] for env in envs if env in dataframes], ignore_index=True)\n",
    "        \n",
    "        self.label_columns = [\"No Finding\", \"Atelectasis\", \"Cardiomegaly\", \"Effusion\", \"Pneumonia\", \n",
    "                              \"Pneumothorax\", \"Consolidation\", \"Edema\"]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['path']\n",
    "        image = Image.open(img_path).convert('RGB')  # Convert to RGB to handle potential grayscale images\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        labels = torch.tensor(self.data.iloc[idx][self.label_columns].values.astype(float), dtype=torch.float32)\n",
    "        if np.isnan(labels).any():\n",
    "            raise ValueError(\"NaN values found in labels\")\n",
    "        \n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = MultiEnvDataset(dfs, subset='val', envs=['MIMIC', 'CXP'], transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "val_dataset = MultiEnvDataset(dfs, subset='test', envs=['MIMIC', 'CXP'], transform=transform)\n",
    "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have not been able to make the training work yet, see the issue below. The original paper provides separate scripts to do the training, which require some packages that seem to be not compatible with my platform. Still figuring out how to either make the original scripts work, or write my own training in a way that it provides results similar to the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emb Dim:\n",
      "1024\n",
      "Manually setting output dim to 1024\n",
      "1024\n",
      "running loss 263.2264404296875\n",
      "running loss 514.5382080078125\n",
      "running loss 767.6939086914062\n",
      "running loss 1016.5777893066406\n",
      "running loss 1267.855239868164\n",
      "running loss 1511.8975677490234\n",
      "running loss 1747.3689422607422\n",
      "running loss 1981.6729736328125\n",
      "running loss 2208.825973510742\n",
      "running loss 2434.0503845214844\n",
      "running loss 2658.7277221679688\n",
      "running loss 2872.757781982422\n",
      "running loss 3078.2845611572266\n",
      "running loss 3282.9491424560547\n",
      "running loss 3500.4393920898438\n",
      "running loss 3702.9176330566406\n",
      "running loss 3903.872085571289\n",
      "running loss 4100.029113769531\n",
      "running loss 4295.556610107422\n",
      "running loss 4467.040328979492\n",
      "running loss 4652.415817260742\n",
      "running loss 4831.527069091797\n",
      "running loss 5008.1651611328125\n",
      "running loss 5164.4727783203125\n",
      "running loss 5334.4490966796875\n",
      "running loss 5519.285690307617\n",
      "running loss 5691.533920288086\n",
      "running loss 5856.534912109375\n",
      "running loss 6021.951202392578\n",
      "running loss 6167.427459716797\n",
      "running loss 6322.334060668945\n",
      "running loss 6465.437225341797\n",
      "running loss 6621.335006713867\n",
      "running loss 6755.909362792969\n",
      "running loss 6893.804168701172\n",
      "running loss 7029.446838378906\n",
      "running loss 7159.954345703125\n",
      "running loss 7293.2901611328125\n",
      "running loss 7420.679954528809\n",
      "running loss 7557.0693435668945\n",
      "running loss 7660.711280822754\n",
      "running loss 7779.509391784668\n",
      "running loss 7895.223670959473\n",
      "running loss 8005.603904724121\n",
      "running loss 8114.697242736816\n",
      "running loss 8226.752082824707\n",
      "running loss 8323.348770141602\n",
      "running loss 8421.37954711914\n",
      "running loss 8513.052742004395\n",
      "running loss 8617.317276000977\n",
      "running loss 8731.37776184082\n",
      "running loss 8809.246154785156\n",
      "running loss 8884.446670532227\n",
      "running loss 8969.108947753906\n",
      "running loss 9052.517570495605\n",
      "running loss 9125.895866394043\n",
      "running loss 9200.753684997559\n",
      "running loss 9287.989990234375\n",
      "running loss 9366.517456054688\n",
      "running loss 9451.8193359375\n",
      "running loss 9539.018249511719\n",
      "running loss 9611.504661560059\n",
      "running loss 9692.316856384277\n",
      "running loss 9756.752700805664\n",
      "running loss 9822.772445678711\n",
      "running loss 9882.612689971924\n",
      "running loss 9945.622009277344\n",
      "running loss 10013.120765686035\n",
      "running loss 10074.977058410645\n",
      "running loss 10135.496696472168\n",
      "running loss 10204.92961883545\n",
      "running loss 10266.638999938965\n",
      "running loss 10324.971649169922\n",
      "running loss 10388.593769073486\n",
      "running loss 10450.188953399658\n",
      "running loss 10507.183879852295\n",
      "running loss 10560.791805267334\n",
      "running loss 10615.801383972168\n",
      "running loss 10676.982074737549\n",
      "running loss 10736.466766357422\n",
      "running loss 10793.28406906128\n",
      "running loss 10854.838947296143\n",
      "running loss 10911.342350006104\n",
      "running loss 10966.253261566162\n",
      "running loss 11014.77388381958\n",
      "running loss 11065.643642425537\n",
      "running loss 11118.004154205322\n",
      "running loss 11173.210079193115\n",
      "running loss 11230.999080657959\n",
      "running loss 11280.911113739014\n",
      "running loss 11331.638748168945\n",
      "running loss 11389.954246520996\n",
      "running loss 11448.507640838623\n",
      "running loss 11498.174304962158\n",
      "running loss 11547.400192260742\n",
      "running loss 11599.760276794434\n",
      "running loss 11650.623268127441\n",
      "running loss 11698.527076721191\n",
      "running loss 11748.927616119385\n",
      "running loss 11796.289585113525\n",
      "running loss 11848.02409362793\n",
      "running loss 11899.132415771484\n",
      "running loss 11948.436485290527\n",
      "running loss 11998.694435119629\n",
      "running loss 12049.453651428223\n",
      "running loss 12099.234474182129\n",
      "running loss 12152.207180023193\n",
      "running loss 12202.978454589844\n",
      "running loss 12253.137775421143\n",
      "running loss 12303.006317138672\n",
      "running loss 12350.035808563232\n",
      "running loss 12402.813835144043\n",
      "running loss 12448.386596679688\n",
      "running loss 12496.214660644531\n",
      "running loss 12544.965816497803\n",
      "running loss 12588.70524597168\n",
      "running loss 12640.012870788574\n",
      "running loss 12690.197288513184\n",
      "running loss 12744.215274810791\n",
      "running loss 12799.199321746826\n",
      "running loss 12847.462581634521\n",
      "running loss 12895.682788848877\n",
      "running loss 12944.68099975586\n",
      "running loss 12992.084686279297\n",
      "running loss 13040.460922241211\n",
      "running loss 13087.255855560303\n",
      "running loss 13137.719722747803\n",
      "running loss 13183.335948944092\n",
      "running loss 13233.157001495361\n",
      "running loss 13280.848705291748\n",
      "running loss 13330.21603012085\n",
      "running loss 13378.96224975586\n",
      "running loss 13426.184028625488\n",
      "running loss 13475.988376617432\n",
      "running loss 13529.004596710205\n",
      "running loss 13576.091049194336\n",
      "running loss 13623.205905914307\n",
      "running loss 13675.88892364502\n",
      "running loss 13725.351512908936\n",
      "running loss 13771.685642242432\n",
      "running loss 13817.531429290771\n",
      "running loss 13864.055099487305\n",
      "running loss 13913.31328201294\n",
      "running loss 13962.460708618164\n",
      "running loss 14013.788032531738\n",
      "running loss 14067.27596282959\n",
      "running loss 14116.08564376831\n",
      "running loss 14172.31273651123\n",
      "running loss 14227.02986907959\n",
      "running loss 14278.583694458008\n",
      "running loss 14324.254917144775\n",
      "running loss 14371.17650604248\n",
      "running loss 14417.864459991455\n",
      "running loss 14464.447261810303\n",
      "running loss 14512.727481842041\n",
      "running loss 14559.30135345459\n",
      "running loss 14609.74719619751\n",
      "running loss 14663.944187164307\n",
      "running loss 14712.865699768066\n",
      "running loss 14760.463802337646\n",
      "running loss 14807.770729064941\n",
      "running loss 14854.171257019043\n",
      "running loss 14903.6439743042\n",
      "running loss 14949.490509033203\n",
      "running loss 15000.562110900879\n",
      "running loss 15049.299255371094\n",
      "running loss 15096.13342666626\n",
      "running loss 15146.884189605713\n",
      "running loss 15194.760795593262\n",
      "running loss 15239.684482574463\n",
      "running loss 15282.924556732178\n",
      "running loss 15332.531700134277\n",
      "running loss 15381.179214477539\n",
      "running loss 15428.891220092773\n",
      "running loss 15479.853492736816\n",
      "running loss 15526.856483459473\n",
      "running loss 15575.951271057129\n",
      "running loss 15622.238121032715\n",
      "running loss 15671.231842041016\n",
      "running loss 15716.44707107544\n",
      "running loss 15768.276332855225\n",
      "running loss 15816.30612564087\n",
      "running loss 15867.786472320557\n",
      "running loss 15916.656875610352\n",
      "running loss 15971.457942962646\n",
      "running loss 16021.628757476807\n",
      "running loss 16069.260257720947\n",
      "running loss 16116.124195098877\n",
      "running loss 16165.465824127197\n",
      "running loss 16211.13319015503\n",
      "running loss 16257.81816482544\n",
      "running loss 16304.021411895752\n",
      "running loss 16350.321594238281\n",
      "running loss 16400.017135620117\n",
      "running loss 16413.089469909668\n",
      "Epoch: 1.00, Train Loss: 0.66, Validation Loss: 0.37\n",
      "running loss 42.45112228393555\n",
      "running loss 89.3318977355957\n",
      "running loss 132.06999588012695\n",
      "running loss 177.13267517089844\n",
      "running loss 223.38393020629883\n",
      "running loss 267.8280029296875\n",
      "running loss 314.33104705810547\n",
      "running loss 360.1613578796387\n",
      "running loss 408.38878631591797\n",
      "running loss 454.4367370605469\n",
      "running loss 502.46258544921875\n",
      "running loss 549.7138519287109\n",
      "running loss 593.5496826171875\n",
      "running loss 642.1297874450684\n",
      "running loss 687.5514221191406\n",
      "running loss 729.4453964233398\n",
      "running loss 775.2053871154785\n",
      "running loss 818.8695793151855\n",
      "running loss 864.7737312316895\n",
      "running loss 912.6025466918945\n",
      "running loss 961.8764610290527\n",
      "running loss 1010.7445297241211\n",
      "running loss 1057.4910278320312\n",
      "running loss 1105.6694564819336\n",
      "running loss 1149.8942260742188\n",
      "running loss 1197.8647918701172\n",
      "running loss 1244.286148071289\n",
      "running loss 1293.9929275512695\n",
      "running loss 1340.5834655761719\n",
      "running loss 1388.1890411376953\n",
      "running loss 1435.6765747070312\n",
      "running loss 1483.9369812011719\n",
      "running loss 1529.9716682434082\n",
      "running loss 1573.0248947143555\n",
      "running loss 1616.699306488037\n",
      "running loss 1661.393653869629\n",
      "running loss 1711.4137344360352\n",
      "running loss 1759.8415985107422\n",
      "running loss 1810.2802124023438\n",
      "running loss 1858.2897911071777\n",
      "running loss 1907.5538215637207\n",
      "running loss 1949.5085258483887\n",
      "running loss 1992.4625129699707\n",
      "running loss 2040.8167190551758\n",
      "running loss 2087.316436767578\n",
      "running loss 2134.2613677978516\n",
      "running loss 2179.969467163086\n",
      "running loss 2222.3659286499023\n",
      "running loss 2266.685199737549\n",
      "running loss 2315.743137359619\n",
      "running loss 2359.655216217041\n",
      "running loss 2401.856159210205\n",
      "running loss 2447.4629707336426\n",
      "running loss 2497.6397857666016\n",
      "running loss 2541.120719909668\n",
      "running loss 2588.4553871154785\n",
      "running loss 2636.037281036377\n",
      "running loss 2678.1400566101074\n",
      "running loss 2727.0403747558594\n",
      "running loss 2771.373073577881\n",
      "running loss 2817.019229888916\n",
      "running loss 2864.090675354004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss 2912.8115234375\n",
      "running loss 2960.2079696655273\n",
      "running loss 3009.55073928833\n",
      "running loss 3054.4355545043945\n",
      "running loss 3101.1442489624023\n",
      "running loss 3144.3744506835938\n",
      "running loss 3190.251163482666\n",
      "running loss 3237.353401184082\n",
      "running loss 3286.4815063476562\n",
      "running loss 3331.6801681518555\n",
      "running loss 3380.8242988586426\n",
      "running loss 3428.854206085205\n",
      "running loss 3475.562110900879\n",
      "running loss 3518.5119819641113\n",
      "running loss 3565.770092010498\n",
      "running loss 3610.592571258545\n",
      "running loss 3659.9098587036133\n",
      "running loss 3710.173740386963\n",
      "running loss 3751.1669731140137\n",
      "running loss 3799.3882331848145\n",
      "running loss 3849.5416984558105\n",
      "running loss 3895.4034461975098\n",
      "running loss 3937.5432624816895\n",
      "running loss 3984.0265464782715\n",
      "running loss 4027.1926651000977\n",
      "running loss 4072.9758529663086\n",
      "running loss 4117.023204803467\n",
      "running loss 4163.359088897705\n",
      "running loss 4211.375995635986\n",
      "running loss 4258.674472808838\n",
      "running loss 4303.471160888672\n",
      "running loss 4347.153945922852\n",
      "running loss 4396.068622589111\n",
      "running loss 4441.4503746032715\n",
      "running loss 4488.311561584473\n",
      "running loss 4535.040584564209\n",
      "running loss 4585.439701080322\n",
      "running loss 4628.101001739502\n",
      "running loss 4674.413074493408\n",
      "running loss 4721.819255828857\n",
      "running loss 4765.396369934082\n",
      "running loss 4814.362762451172\n",
      "running loss 4865.417552947998\n",
      "running loss 4913.82780456543\n",
      "running loss 4953.691452026367\n",
      "running loss 5001.301746368408\n",
      "running loss 5043.974155426025\n",
      "running loss 5092.115978240967\n",
      "running loss 5136.269771575928\n",
      "running loss 5182.309257507324\n",
      "running loss 5231.833297729492\n",
      "running loss 5276.650459289551\n",
      "running loss 5326.066158294678\n",
      "running loss 5368.054489135742\n",
      "running loss 5412.519458770752\n",
      "running loss 5457.377510070801\n",
      "running loss 5504.551322937012\n",
      "running loss 5547.9135818481445\n",
      "running loss 5596.989971160889\n",
      "running loss 5639.785087585449\n",
      "running loss 5684.6651611328125\n",
      "running loss 5726.015930175781\n",
      "running loss 5777.912857055664\n",
      "running loss 5820.463039398193\n",
      "running loss 5867.232128143311\n",
      "running loss 5913.773197174072\n",
      "running loss 5960.416213989258\n",
      "running loss 6002.496231079102\n",
      "running loss 6046.907760620117\n",
      "running loss 6089.70414352417\n",
      "running loss 6137.69002532959\n",
      "running loss 6182.910911560059\n",
      "running loss 6231.793472290039\n",
      "running loss 6275.7735023498535\n",
      "running loss 6320.52876663208\n",
      "running loss 6368.511116027832\n",
      "running loss 6411.466915130615\n",
      "running loss 6456.929683685303\n",
      "running loss 6501.755310058594\n",
      "running loss 6549.1900062561035\n",
      "running loss 6593.416694641113\n",
      "running loss 6642.116436004639\n",
      "running loss 6686.643672943115\n",
      "running loss 6737.117595672607\n",
      "running loss 6782.315643310547\n",
      "running loss 6829.9642906188965\n",
      "running loss 6876.279556274414\n",
      "running loss 6922.847454071045\n",
      "running loss 6965.512577056885\n",
      "running loss 7016.5288734436035\n",
      "running loss 7059.05184173584\n",
      "running loss 7107.086235046387\n",
      "running loss 7152.692604064941\n",
      "running loss 7202.879848480225\n",
      "running loss 7247.961334228516\n",
      "running loss 7293.595348358154\n",
      "running loss 7341.429763793945\n",
      "running loss 7385.623558044434\n",
      "running loss 7429.6084060668945\n",
      "running loss 7474.953426361084\n",
      "running loss 7522.121597290039\n",
      "running loss 7570.539932250977\n",
      "running loss 7614.110954284668\n",
      "running loss 7663.822456359863\n",
      "running loss 7714.057922363281\n",
      "running loss 7756.040481567383\n",
      "running loss 7799.134952545166\n",
      "running loss 7848.331470489502\n",
      "running loss 7898.281356811523\n",
      "running loss 7941.817085266113\n",
      "running loss 7989.635982513428\n",
      "running loss 8035.302082061768\n",
      "running loss 8084.002025604248\n",
      "running loss 8132.990497589111\n",
      "running loss 8179.127983093262\n",
      "running loss 8223.65592956543\n",
      "running loss 8269.752140045166\n",
      "running loss 8311.863334655762\n",
      "running loss 8359.11184310913\n",
      "running loss 8403.826725006104\n",
      "running loss 8448.430995941162\n",
      "running loss 8491.54602432251\n",
      "running loss 8537.93673324585\n",
      "running loss 8583.768077850342\n",
      "running loss 8628.840282440186\n",
      "running loss 8672.730026245117\n",
      "running loss 8715.987743377686\n",
      "running loss 8764.307682037354\n",
      "running loss 8811.981819152832\n",
      "running loss 8855.474704742432\n",
      "running loss 8898.440982818604\n",
      "running loss 8947.426219940186\n",
      "running loss 8959.548305511475\n",
      "Epoch: 2.00, Train Loss: 0.36, Validation Loss: 0.35\n",
      "running loss 49.94085693359375\n",
      "running loss 95.90224075317383\n",
      "running loss 139.07593536376953\n",
      "running loss 182.99963760375977\n",
      "running loss 224.75207138061523\n",
      "running loss 268.5487747192383\n",
      "running loss 312.6132507324219\n",
      "running loss 361.052734375\n",
      "running loss 405.16189193725586\n",
      "running loss 446.12106704711914\n",
      "running loss 488.1340141296387\n",
      "running loss 533.8617134094238\n",
      "running loss 578.3231658935547\n",
      "running loss 621.1857414245605\n",
      "running loss 665.8056030273438\n",
      "running loss 706.7164649963379\n",
      "running loss 749.3440589904785\n",
      "running loss 791.0528221130371\n",
      "running loss 835.6126327514648\n",
      "running loss 878.6394271850586\n",
      "running loss 919.8011131286621\n",
      "running loss 963.3469581604004\n",
      "running loss 1007.040885925293\n",
      "running loss 1053.5988960266113\n",
      "running loss 1095.165355682373\n",
      "running loss 1138.5593872070312\n",
      "running loss 1187.3323135375977\n",
      "running loss 1230.631175994873\n",
      "running loss 1273.8234481811523\n",
      "running loss 1318.7287101745605\n",
      "running loss 1362.8906288146973\n",
      "running loss 1406.4963836669922\n",
      "running loss 1449.9808654785156\n",
      "running loss 1491.9702072143555\n",
      "running loss 1533.6049575805664\n",
      "running loss 1574.8961906433105\n",
      "running loss 1621.0353240966797\n",
      "running loss 1662.3045196533203\n",
      "running loss 1708.4794998168945\n",
      "running loss 1753.2756004333496\n",
      "running loss 1793.2715454101562\n",
      "running loss 1838.692226409912\n",
      "running loss 1881.5979042053223\n",
      "running loss 1929.3169898986816\n",
      "running loss 1971.6958198547363\n",
      "running loss 2013.6122016906738\n",
      "running loss 2056.7598838806152\n",
      "running loss 2106.730655670166\n",
      "running loss 2151.210262298584\n",
      "running loss 2192.895622253418\n",
      "running loss 2240.9788513183594\n",
      "running loss 2285.271656036377\n",
      "running loss 2333.161937713623\n",
      "running loss 2377.56303024292\n",
      "running loss 2422.810302734375\n",
      "running loss 2464.067974090576\n",
      "running loss 2505.8624992370605\n",
      "running loss 2549.2979011535645\n",
      "running loss 2592.026382446289\n",
      "running loss 2637.574977874756\n",
      "running loss 2682.752368927002\n",
      "running loss 2726.6964111328125\n",
      "running loss 2772.318141937256\n",
      "running loss 2814.8941764831543\n",
      "running loss 2856.075973510742\n",
      "running loss 2902.760025024414\n",
      "running loss 2949.4825172424316\n",
      "running loss 2997.0591735839844\n",
      "running loss 3041.4344367980957\n",
      "running loss 3086.7606773376465\n",
      "running loss 3133.320152282715\n",
      "running loss 3172.9343605041504\n",
      "running loss 3214.5795555114746\n",
      "running loss 3257.5335807800293\n",
      "running loss 3305.2371368408203\n",
      "running loss 3351.283561706543\n",
      "running loss 3393.0545616149902\n",
      "running loss 3439.6179237365723\n",
      "running loss 3486.7165336608887\n",
      "running loss 3528.6696243286133\n",
      "running loss 3572.65869140625\n",
      "running loss 3616.2461280822754\n",
      "running loss 3661.946075439453\n",
      "running loss 3705.9155311584473\n",
      "running loss 3750.4896240234375\n",
      "running loss 3795.92081451416\n",
      "running loss 3847.5032958984375\n",
      "running loss 3895.412208557129\n",
      "running loss 3937.7052001953125\n",
      "running loss 3987.8180236816406\n",
      "running loss 4031.1435775756836\n",
      "running loss 4074.186004638672\n",
      "running loss 4118.525730133057\n",
      "running loss 4163.033676147461\n",
      "running loss 4202.978187561035\n",
      "running loss 4243.133758544922\n",
      "running loss 4286.5073318481445\n",
      "running loss 4329.074462890625\n",
      "running loss 4374.16036605835\n",
      "running loss 4414.300983428955\n",
      "running loss 4457.003643035889\n",
      "running loss 4499.272624969482\n",
      "running loss 4544.1667556762695\n",
      "running loss 4586.593536376953\n",
      "running loss 4631.046257019043\n",
      "running loss 4668.923889160156\n",
      "running loss 4715.022403717041\n",
      "running loss 4758.2625160217285\n",
      "running loss 4804.040412902832\n",
      "running loss 4848.232963562012\n",
      "running loss 4888.146522521973\n",
      "running loss 4934.745281219482\n",
      "running loss 4979.273212432861\n",
      "running loss 5021.179546356201\n",
      "running loss 5063.756446838379\n",
      "running loss 5108.867458343506\n",
      "running loss 5154.636257171631\n",
      "running loss 5199.928981781006\n",
      "running loss 5243.447887420654\n",
      "running loss 5288.169906616211\n",
      "running loss 5334.329704284668\n",
      "running loss 5379.8822593688965\n",
      "running loss 5422.1452560424805\n",
      "running loss 5468.478458404541\n",
      "running loss 5513.3642501831055\n",
      "running loss 5557.411926269531\n",
      "running loss 5600.857593536377\n",
      "running loss 5645.116016387939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss 5689.621334075928\n",
      "running loss 5732.998680114746\n",
      "running loss 5779.542163848877\n",
      "running loss 5824.523334503174\n",
      "running loss 5868.1151695251465\n",
      "running loss 5907.39794921875\n",
      "running loss 5952.935791015625\n",
      "running loss 5997.213611602783\n",
      "running loss 6038.619159698486\n",
      "running loss 6083.554286956787\n",
      "running loss 6129.154834747314\n",
      "running loss 6174.640716552734\n",
      "running loss 6221.292434692383\n",
      "running loss 6264.858768463135\n",
      "running loss 6308.549289703369\n",
      "running loss 6350.9405517578125\n",
      "running loss 6390.905708312988\n",
      "running loss 6437.264144897461\n",
      "running loss 6478.856475830078\n",
      "running loss 6523.783653259277\n",
      "running loss 6565.374969482422\n",
      "running loss 6609.161895751953\n",
      "running loss 6658.484294891357\n",
      "running loss 6701.743152618408\n",
      "running loss 6743.8024253845215\n",
      "running loss 6789.678802490234\n",
      "running loss 6832.383739471436\n",
      "running loss 6871.555847167969\n",
      "running loss 6915.127052307129\n",
      "running loss 6957.738067626953\n",
      "running loss 7003.04736328125\n",
      "running loss 7045.705368041992\n",
      "running loss 7091.437454223633\n",
      "running loss 7134.83390045166\n",
      "running loss 7181.653015136719\n",
      "running loss 7225.308082580566\n",
      "running loss 7271.288391113281\n",
      "running loss 7315.1060218811035\n",
      "running loss 7358.926986694336\n",
      "running loss 7404.687286376953\n",
      "running loss 7453.575634002686\n",
      "running loss 7495.0266761779785\n",
      "running loss 7540.473461151123\n",
      "running loss 7584.040603637695\n",
      "running loss 7629.148166656494\n",
      "running loss 7671.832893371582\n",
      "running loss 7714.3554611206055\n",
      "running loss 7757.040897369385\n",
      "running loss 7803.471336364746\n",
      "running loss 7848.278785705566\n",
      "running loss 7891.615577697754\n",
      "running loss 7936.598133087158\n",
      "running loss 7977.759197235107\n",
      "running loss 8018.676139831543\n",
      "running loss 8061.10046005249\n",
      "running loss 8101.3135414123535\n",
      "running loss 8148.416828155518\n",
      "running loss 8191.093669891357\n",
      "running loss 8236.411590576172\n",
      "running loss 8277.928569793701\n",
      "running loss 8320.792602539062\n",
      "running loss 8361.158107757568\n",
      "running loss 8411.006397247314\n",
      "running loss 8456.27014541626\n",
      "running loss 8498.416744232178\n",
      "running loss 8544.528759002686\n",
      "running loss 8554.44721031189\n",
      "Epoch: 3.00, Train Loss: 0.34, Validation Loss: 0.33\n",
      "running loss 41.18947982788086\n",
      "running loss 87.17002487182617\n",
      "running loss 129.9527587890625\n",
      "running loss 173.93370056152344\n",
      "running loss 215.1764030456543\n",
      "running loss 255.76974487304688\n",
      "running loss 298.78936767578125\n",
      "running loss 340.2360305786133\n",
      "running loss 384.34336853027344\n",
      "running loss 426.6022644042969\n",
      "running loss 470.8429069519043\n",
      "running loss 512.4915351867676\n",
      "running loss 555.9266242980957\n",
      "running loss 597.4990463256836\n",
      "running loss 641.4945526123047\n",
      "running loss 683.6529235839844\n",
      "running loss 728.2890319824219\n",
      "running loss 773.3676376342773\n",
      "running loss 813.6434173583984\n",
      "running loss 859.3113479614258\n",
      "running loss 901.6591491699219\n",
      "running loss 944.6091156005859\n",
      "running loss 987.3636016845703\n",
      "running loss 1029.411060333252\n",
      "running loss 1072.2092399597168\n",
      "running loss 1114.223789215088\n",
      "running loss 1153.8773727416992\n",
      "running loss 1196.7885055541992\n",
      "running loss 1239.5033836364746\n",
      "running loss 1281.575798034668\n",
      "running loss 1323.0339965820312\n",
      "running loss 1366.7731590270996\n",
      "running loss 1409.3797187805176\n",
      "running loss 1453.666976928711\n",
      "running loss 1493.5717697143555\n",
      "running loss 1533.635654449463\n",
      "running loss 1576.4195861816406\n",
      "running loss 1618.0896797180176\n",
      "running loss 1659.1030502319336\n",
      "running loss 1709.5479011535645\n",
      "running loss 1752.865550994873\n",
      "running loss 1795.9756164550781\n",
      "running loss 1840.7121086120605\n",
      "running loss 1881.6939315795898\n",
      "running loss 1923.9812393188477\n",
      "running loss 1965.8331756591797\n",
      "running loss 2007.0134963989258\n",
      "running loss 2048.347198486328\n",
      "running loss 2091.4468536376953\n",
      "running loss 2133.5594749450684\n",
      "running loss 2178.079906463623\n",
      "running loss 2224.6899757385254\n",
      "running loss 2264.991180419922\n",
      "running loss 2306.1880111694336\n",
      "running loss 2351.675937652588\n",
      "running loss 2395.797016143799\n",
      "running loss 2437.5290718078613\n",
      "running loss 2479.8498764038086\n",
      "running loss 2523.2617225646973\n",
      "running loss 2565.2543144226074\n",
      "running loss 2605.9864349365234\n",
      "running loss 2649.747859954834\n",
      "running loss 2693.3534507751465\n",
      "running loss 2738.226703643799\n",
      "running loss 2781.7743530273438\n",
      "running loss 2822.454849243164\n",
      "running loss 2867.225528717041\n",
      "running loss 2909.355308532715\n",
      "running loss 2951.483726501465\n",
      "running loss 2996.503589630127\n",
      "running loss 3044.1170806884766\n",
      "running loss 3093.595054626465\n",
      "running loss 3135.850715637207\n",
      "running loss 3179.301315307617\n",
      "running loss 3219.0935592651367\n",
      "running loss 3261.2993698120117\n",
      "running loss 3306.871307373047\n",
      "running loss 3346.943489074707\n",
      "running loss 3389.130386352539\n",
      "running loss 3431.5049896240234\n",
      "running loss 3475.177749633789\n",
      "running loss 3518.03218460083\n",
      "running loss 3563.105140686035\n",
      "running loss 3605.253204345703\n",
      "running loss 3647.01167678833\n",
      "running loss 3688.3970489501953\n",
      "running loss 3730.556167602539\n",
      "running loss 3771.225917816162\n",
      "running loss 3811.774028778076\n",
      "running loss 3855.79451751709\n",
      "running loss 3897.2787322998047\n",
      "running loss 3938.675338745117\n",
      "running loss 3978.6808700561523\n",
      "running loss 4020.0367889404297\n",
      "running loss 4062.5963096618652\n",
      "running loss 4101.851459503174\n",
      "running loss 4146.749675750732\n",
      "running loss 4191.102451324463\n",
      "running loss 4232.43025970459\n",
      "running loss 4276.6813888549805\n",
      "running loss 4323.357894897461\n",
      "running loss 4362.480785369873\n",
      "running loss 4403.481937408447\n",
      "running loss 4446.849498748779\n",
      "running loss 4489.044887542725\n",
      "running loss 4529.074913024902\n",
      "running loss 4572.099067687988\n",
      "running loss 4613.967205047607\n",
      "running loss 4659.073699951172\n",
      "running loss 4703.2463455200195\n",
      "running loss 4745.384872436523\n",
      "running loss 4790.691394805908\n",
      "running loss 4832.300769805908\n",
      "running loss 4872.625019073486\n",
      "running loss 4913.210620880127\n",
      "running loss 4955.91686630249\n",
      "running loss 5000.513011932373\n",
      "running loss 5042.054908752441\n",
      "running loss 5082.651313781738\n",
      "running loss 5125.607494354248\n",
      "running loss 5170.525161743164\n",
      "running loss 5207.75989151001\n",
      "running loss 5250.188228607178\n",
      "running loss 5292.55891418457\n",
      "running loss 5336.289653778076\n",
      "running loss 5380.5402755737305\n",
      "running loss 5424.561252593994\n",
      "running loss 5471.214084625244\n",
      "running loss 5513.562046051025\n",
      "running loss 5555.412868499756\n",
      "running loss 5596.325736999512\n",
      "running loss 5640.606498718262\n",
      "running loss 5685.206962585449\n",
      "running loss 5728.871688842773\n",
      "running loss 5770.220840454102\n",
      "running loss 5812.643417358398\n",
      "running loss 5853.370712280273\n",
      "running loss 5896.284595489502\n",
      "running loss 5938.844966888428\n",
      "running loss 5978.223289489746\n",
      "running loss 6020.9674949646\n",
      "running loss 6062.750564575195\n",
      "running loss 6106.390594482422\n",
      "running loss 6149.682464599609\n",
      "running loss 6194.275142669678\n",
      "running loss 6234.701541900635\n",
      "running loss 6277.050247192383\n",
      "running loss 6320.127853393555\n",
      "running loss 6360.396114349365\n",
      "running loss 6404.1782302856445\n",
      "running loss 6445.848876953125\n",
      "running loss 6483.894752502441\n",
      "running loss 6525.27254486084\n",
      "running loss 6566.331920623779\n",
      "running loss 6611.362892150879\n",
      "running loss 6649.711776733398\n",
      "running loss 6691.17892074585\n",
      "running loss 6734.775074005127\n",
      "running loss 6778.339542388916\n",
      "running loss 6821.045497894287\n",
      "running loss 6858.650672912598\n",
      "running loss 6899.771266937256\n",
      "running loss 6941.6305809021\n",
      "running loss 6981.072635650635\n",
      "running loss 7021.700065612793\n",
      "running loss 7062.001201629639\n",
      "running loss 7103.643062591553\n",
      "running loss 7147.451488494873\n",
      "running loss 7190.52082824707\n",
      "running loss 7232.772266387939\n",
      "running loss 7275.938510894775\n",
      "running loss 7314.883838653564\n",
      "running loss 7358.471675872803\n",
      "running loss 7402.752056121826\n",
      "running loss 7443.48904800415\n",
      "running loss 7484.143474578857\n",
      "running loss 7528.66145324707\n",
      "running loss 7569.887847900391\n",
      "running loss 7610.228618621826\n",
      "running loss 7653.65030670166\n",
      "running loss 7694.485645294189\n",
      "running loss 7740.881977081299\n",
      "running loss 7782.665077209473\n",
      "running loss 7826.282222747803\n",
      "running loss 7866.728607177734\n",
      "running loss 7909.29133605957\n",
      "running loss 7954.346221923828\n",
      "running loss 7996.979022979736\n",
      "running loss 8041.309680938721\n",
      "running loss 8082.544464111328\n",
      "running loss 8122.500354766846\n",
      "running loss 8163.686584472656\n",
      "running loss 8206.961791992188\n",
      "running loss 8251.316013336182\n",
      "running loss 8260.478974342346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4.00, Train Loss: 0.33, Validation Loss: 0.32\n",
      "running loss 38.593536376953125\n",
      "running loss 79.80635070800781\n",
      "running loss 118.28771209716797\n",
      "running loss 161.52361297607422\n",
      "running loss 201.97427368164062\n",
      "running loss 241.73269653320312\n",
      "running loss 284.0415458679199\n",
      "running loss 327.25940322875977\n",
      "running loss 368.10621643066406\n",
      "running loss 409.38758850097656\n",
      "running loss 453.68349838256836\n",
      "running loss 493.1384735107422\n",
      "running loss 533.0269470214844\n",
      "running loss 573.8311462402344\n",
      "running loss 611.2271919250488\n",
      "running loss 651.2573852539062\n",
      "running loss 692.973518371582\n",
      "running loss 734.2861747741699\n",
      "running loss 775.8254318237305\n",
      "running loss 820.8542404174805\n",
      "running loss 861.7502784729004\n",
      "running loss 905.4426193237305\n",
      "running loss 947.1706314086914\n",
      "running loss 985.9681205749512\n",
      "running loss 1027.06644821167\n",
      "running loss 1067.0166702270508\n",
      "running loss 1107.8537521362305\n",
      "running loss 1145.9228134155273\n",
      "running loss 1184.701171875\n",
      "running loss 1231.4221229553223\n",
      "running loss 1271.8739318847656\n",
      "running loss 1316.7839660644531\n",
      "running loss 1356.259750366211\n",
      "running loss 1398.4926528930664\n",
      "running loss 1441.2961616516113\n",
      "running loss 1481.381202697754\n",
      "running loss 1523.3688049316406\n",
      "running loss 1562.5279006958008\n",
      "running loss 1604.9380378723145\n",
      "running loss 1648.669589996338\n",
      "running loss 1687.7249221801758\n",
      "running loss 1727.54691696167\n",
      "running loss 1770.8037796020508\n",
      "running loss 1814.8448448181152\n",
      "running loss 1856.6525039672852\n",
      "running loss 1894.1731071472168\n",
      "running loss 1935.254524230957\n",
      "running loss 1976.5617294311523\n",
      "running loss 2019.604866027832\n",
      "running loss 2061.0817680358887\n",
      "running loss 2101.793933868408\n",
      "running loss 2144.828254699707\n",
      "running loss 2186.1407928466797\n",
      "running loss 2224.6240310668945\n",
      "running loss 2265.4519424438477\n",
      "running loss 2305.3725090026855\n",
      "running loss 2346.4776725769043\n",
      "running loss 2386.647289276123\n",
      "running loss 2428.303955078125\n",
      "running loss 2473.2815895080566\n",
      "running loss 2515.720428466797\n",
      "running loss 2559.832790374756\n",
      "running loss 2599.8743171691895\n",
      "running loss 2643.188091278076\n",
      "running loss 2680.750385284424\n",
      "running loss 2721.140422821045\n",
      "running loss 2761.5280990600586\n",
      "running loss 2799.4111328125\n",
      "running loss 2839.7294158935547\n",
      "running loss 2882.447032928467\n",
      "running loss 2921.0350608825684\n",
      "running loss 2964.8259468078613\n",
      "running loss 3004.2935180664062\n",
      "running loss 3050.0118103027344\n",
      "running loss 3088.822956085205\n",
      "running loss 3129.977680206299\n",
      "running loss 3170.457836151123\n",
      "running loss 3210.7448196411133\n",
      "running loss 3253.768653869629\n",
      "running loss 3293.593505859375\n",
      "running loss 3336.93253326416\n",
      "running loss 3380.060832977295\n",
      "running loss 3419.9592247009277\n",
      "running loss 3460.752742767334\n",
      "running loss 3506.5731811523438\n",
      "running loss 3548.670639038086\n",
      "running loss 3590.4848251342773\n",
      "running loss 3629.279499053955\n",
      "running loss 3667.0670013427734\n",
      "running loss 3711.5184936523438\n",
      "running loss 3755.069290161133\n",
      "running loss 3795.334213256836\n",
      "running loss 3841.1186180114746\n",
      "running loss 3881.518714904785\n",
      "running loss 3920.895050048828\n",
      "running loss 3960.4593353271484\n",
      "running loss 4003.077335357666\n",
      "running loss 4044.729579925537\n",
      "running loss 4085.3891677856445\n",
      "running loss 4125.733070373535\n",
      "running loss 4165.270359039307\n",
      "running loss 4209.216472625732\n",
      "running loss 4246.434917449951\n",
      "running loss 4287.888160705566\n",
      "running loss 4327.290657043457\n",
      "running loss 4371.352653503418\n",
      "running loss 4413.4606857299805\n",
      "running loss 4456.755683898926\n",
      "running loss 4497.238906860352\n",
      "running loss 4538.362586975098\n",
      "running loss 4580.682762145996\n",
      "running loss 4621.184703826904\n",
      "running loss 4661.650970458984\n",
      "running loss 4702.381866455078\n",
      "running loss 4743.630168914795\n",
      "running loss 4781.66450881958\n",
      "running loss 4823.075191497803\n",
      "running loss 4865.554492950439\n",
      "running loss 4907.074722290039\n",
      "running loss 4951.083904266357\n",
      "running loss 4991.360557556152\n",
      "running loss 5033.127498626709\n",
      "running loss 5076.779140472412\n",
      "running loss 5124.766429901123\n",
      "running loss 5163.158317565918\n",
      "running loss 5203.706630706787\n",
      "running loss 5246.0440101623535\n",
      "running loss 5292.252277374268\n",
      "running loss 5331.283260345459\n",
      "running loss 5374.924770355225\n",
      "running loss 5422.1623458862305\n",
      "running loss 5463.853141784668\n",
      "running loss 5501.420616149902\n",
      "running loss 5539.978584289551\n",
      "running loss 5578.123336791992\n",
      "running loss 5622.017463684082\n",
      "running loss 5662.663177490234\n",
      "running loss 5700.706157684326\n",
      "running loss 5739.636817932129\n",
      "running loss 5781.730041503906\n",
      "running loss 5824.363265991211\n",
      "running loss 5864.592792510986\n",
      "running loss 5902.439769744873\n",
      "running loss 5946.718334197998\n",
      "running loss 5987.144142150879\n",
      "running loss 6028.406646728516\n",
      "running loss 6071.225143432617\n",
      "running loss 6112.071701049805\n",
      "running loss 6156.443359375\n",
      "running loss 6195.08065032959\n",
      "running loss 6238.432590484619\n",
      "running loss 6279.634113311768\n",
      "running loss 6318.960105895996\n",
      "running loss 6357.915752410889\n",
      "running loss 6397.374187469482\n",
      "running loss 6439.719470977783\n",
      "running loss 6481.442665100098\n",
      "running loss 6522.752975463867\n",
      "running loss 6563.20724105835\n",
      "running loss 6605.710021972656\n",
      "running loss 6648.56489944458\n",
      "running loss 6689.781650543213\n",
      "running loss 6731.768047332764\n",
      "running loss 6776.000999450684\n",
      "running loss 6819.074378967285\n",
      "running loss 6858.591163635254\n",
      "running loss 6899.006881713867\n",
      "running loss 6940.825145721436\n",
      "running loss 6977.441165924072\n",
      "running loss 7018.247718811035\n",
      "running loss 7062.231536865234\n",
      "running loss 7105.0692710876465\n",
      "running loss 7147.735057830811\n",
      "running loss 7183.146831512451\n",
      "running loss 7227.579689025879\n",
      "running loss 7267.606929779053\n",
      "running loss 7307.467559814453\n",
      "running loss 7353.923553466797\n",
      "running loss 7395.644718170166\n",
      "running loss 7437.85782623291\n",
      "running loss 7476.497783660889\n",
      "running loss 7518.390846252441\n",
      "running loss 7557.523357391357\n",
      "running loss 7598.205978393555\n",
      "running loss 7639.847785949707\n",
      "running loss 7682.103652954102\n",
      "running loss 7724.170616149902\n",
      "running loss 7767.660259246826\n",
      "running loss 7804.09020614624\n",
      "running loss 7844.664081573486\n",
      "running loss 7883.729015350342\n",
      "running loss 7927.219268798828\n",
      "running loss 7967.8028564453125\n",
      "running loss 8010.178615570068\n",
      "running loss 8019.445241928101\n",
      "Epoch: 5.00, Train Loss: 0.32, Validation Loss: 0.31\n",
      "running loss 41.38581848144531\n",
      "running loss 81.64828872680664\n",
      "running loss 123.34829330444336\n",
      "running loss 164.00070190429688\n",
      "running loss 205.23523712158203\n",
      "running loss 240.0392723083496\n",
      "running loss 278.594783782959\n",
      "running loss 316.8402214050293\n",
      "running loss 357.10802459716797\n",
      "running loss 400.61344146728516\n",
      "running loss 436.8734817504883\n",
      "running loss 475.53066635131836\n",
      "running loss 518.9438514709473\n",
      "running loss 561.5698356628418\n",
      "running loss 599.2744522094727\n",
      "running loss 643.3855285644531\n",
      "running loss 684.2551803588867\n",
      "running loss 722.7015342712402\n",
      "running loss 765.8712730407715\n",
      "running loss 804.3473892211914\n",
      "running loss 847.1560287475586\n",
      "running loss 889.8939552307129\n",
      "running loss 924.488109588623\n",
      "running loss 960.1145057678223\n",
      "running loss 997.9880905151367\n",
      "running loss 1038.108772277832\n",
      "running loss 1081.1892890930176\n",
      "running loss 1123.4156532287598\n",
      "running loss 1166.7850341796875\n",
      "running loss 1208.6157150268555\n",
      "running loss 1248.0945739746094\n",
      "running loss 1288.486644744873\n",
      "running loss 1325.5509071350098\n",
      "running loss 1363.2624435424805\n",
      "running loss 1404.6500701904297\n",
      "running loss 1445.5623474121094\n",
      "running loss 1484.7736778259277\n",
      "running loss 1524.1351203918457\n",
      "running loss 1564.5414581298828\n",
      "running loss 1605.3281631469727\n",
      "running loss 1642.8769493103027\n",
      "running loss 1682.7840156555176\n",
      "running loss 1721.813877105713\n",
      "running loss 1766.2764205932617\n",
      "running loss 1800.5888481140137\n",
      "running loss 1838.0498962402344\n",
      "running loss 1877.8677024841309\n",
      "running loss 1916.2961807250977\n",
      "running loss 1956.9043731689453\n",
      "running loss 1999.3326568603516\n",
      "running loss 2038.9113006591797\n",
      "running loss 2077.631996154785\n",
      "running loss 2119.239326477051\n",
      "running loss 2160.0174446105957\n",
      "running loss 2204.428207397461\n",
      "running loss 2245.9287719726562\n",
      "running loss 2289.2493019104004\n",
      "running loss 2328.573143005371\n",
      "running loss 2367.674560546875\n",
      "running loss 2406.1483154296875\n",
      "running loss 2448.283477783203\n",
      "running loss 2491.228282928467\n",
      "running loss 2529.9368743896484\n",
      "running loss 2568.8061866760254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss 2605.1737785339355\n",
      "running loss 2646.219020843506\n",
      "running loss 2686.4856147766113\n",
      "running loss 2727.995018005371\n",
      "running loss 2770.1272773742676\n",
      "running loss 2808.1138610839844\n",
      "running loss 2843.28133392334\n",
      "running loss 2883.220916748047\n",
      "running loss 2922.2128105163574\n",
      "running loss 2963.2904357910156\n",
      "running loss 3002.86376953125\n",
      "running loss 3042.169403076172\n",
      "running loss 3081.483573913574\n",
      "running loss 3117.3093452453613\n",
      "running loss 3155.9879760742188\n",
      "running loss 3198.9158782958984\n",
      "running loss 3236.3839263916016\n",
      "running loss 3280.529541015625\n",
      "running loss 3319.4276580810547\n",
      "running loss 3360.5132637023926\n",
      "running loss 3401.02689743042\n",
      "running loss 3444.268321990967\n",
      "running loss 3484.1763191223145\n",
      "running loss 3521.5175590515137\n",
      "running loss 3561.6572074890137\n",
      "running loss 3603.7103996276855\n",
      "running loss 3641.8960151672363\n",
      "running loss 3680.3850593566895\n",
      "running loss 3720.34077835083\n",
      "running loss 3763.7473793029785\n",
      "running loss 3805.6953315734863\n",
      "running loss 3844.8934745788574\n",
      "running loss 3885.979820251465\n",
      "running loss 3928.4952239990234\n",
      "running loss 3967.1328086853027\n",
      "running loss 4003.876262664795\n",
      "running loss 4044.483669281006\n",
      "running loss 4086.059127807617\n",
      "running loss 4127.268951416016\n",
      "running loss 4166.889892578125\n",
      "running loss 4206.628646850586\n",
      "running loss 4249.254356384277\n",
      "running loss 4287.852390289307\n",
      "running loss 4330.279838562012\n",
      "running loss 4371.678047180176\n",
      "running loss 4410.080722808838\n",
      "running loss 4449.569637298584\n",
      "running loss 4489.229969024658\n",
      "running loss 4532.752147674561\n",
      "running loss 4573.504699707031\n",
      "running loss 4612.944221496582\n",
      "running loss 4654.698974609375\n",
      "running loss 4693.033256530762\n",
      "running loss 4727.946834564209\n",
      "running loss 4770.139797210693\n",
      "running loss 4811.546775817871\n",
      "running loss 4850.978874206543\n",
      "running loss 4887.190505981445\n",
      "running loss 4930.948581695557\n",
      "running loss 4977.28182220459\n",
      "running loss 5015.867691040039\n",
      "running loss 5057.074745178223\n",
      "running loss 5094.745803833008\n",
      "running loss 5132.997184753418\n",
      "running loss 5174.0296630859375\n",
      "running loss 5213.719760894775\n",
      "running loss 5252.523693084717\n",
      "running loss 5289.850315093994\n",
      "running loss 5331.3121910095215\n",
      "running loss 5373.981906890869\n",
      "running loss 5415.390029907227\n",
      "running loss 5456.744823455811\n",
      "running loss 5497.069263458252\n",
      "running loss 5538.282283782959\n",
      "running loss 5573.799461364746\n",
      "running loss 5615.136257171631\n",
      "running loss 5656.210975646973\n",
      "running loss 5694.639953613281\n",
      "running loss 5732.167694091797\n",
      "running loss 5776.090843200684\n",
      "running loss 5817.234073638916\n",
      "running loss 5859.229869842529\n",
      "running loss 5899.368732452393\n",
      "running loss 5936.438438415527\n",
      "running loss 5975.76842880249\n",
      "running loss 6014.761360168457\n",
      "running loss 6057.044116973877\n",
      "running loss 6096.786853790283\n",
      "running loss 6139.198318481445\n",
      "running loss 6179.956604003906\n",
      "running loss 6221.316051483154\n",
      "running loss 6259.934917449951\n",
      "running loss 6299.255748748779\n",
      "running loss 6336.729663848877\n",
      "running loss 6378.687450408936\n",
      "running loss 6418.526569366455\n",
      "running loss 6457.749210357666\n",
      "running loss 6498.879543304443\n",
      "running loss 6537.665561676025\n",
      "running loss 6577.162979125977\n",
      "running loss 6617.867874145508\n",
      "running loss 6656.980621337891\n",
      "running loss 6698.151805877686\n",
      "running loss 6738.616386413574\n",
      "running loss 6775.136451721191\n",
      "running loss 6816.396240234375\n",
      "running loss 6857.3577003479\n",
      "running loss 6896.543201446533\n",
      "running loss 6935.856781005859\n",
      "running loss 6975.737167358398\n",
      "running loss 7019.809097290039\n",
      "running loss 7057.680549621582\n",
      "running loss 7099.713760375977\n",
      "running loss 7142.581039428711\n",
      "running loss 7183.432445526123\n",
      "running loss 7226.60120010376\n",
      "running loss 7268.797115325928\n",
      "running loss 7311.531368255615\n",
      "running loss 7353.385509490967\n",
      "running loss 7391.733310699463\n",
      "running loss 7433.747825622559\n",
      "running loss 7473.795555114746\n",
      "running loss 7509.76505279541\n",
      "running loss 7548.751441955566\n",
      "running loss 7587.807731628418\n",
      "running loss 7629.872093200684\n",
      "running loss 7667.307266235352\n",
      "running loss 7705.650524139404\n",
      "running loss 7746.483726501465\n",
      "running loss 7786.178512573242\n",
      "running loss 7796.769708633423\n",
      "Epoch: 6.00, Train Loss: 0.31, Validation Loss: 0.30\n",
      "running loss 36.93911361694336\n",
      "running loss 73.69384765625\n",
      "running loss 112.1192626953125\n",
      "running loss 150.37221145629883\n",
      "running loss 189.23056030273438\n",
      "running loss 228.56620407104492\n",
      "running loss 266.85187911987305\n",
      "running loss 306.257022857666\n",
      "running loss 343.9132270812988\n",
      "running loss 383.8136787414551\n",
      "running loss 422.2934226989746\n",
      "running loss 462.8901786804199\n",
      "running loss 502.08666229248047\n",
      "running loss 544.4308090209961\n",
      "running loss 586.6655082702637\n",
      "running loss 623.3801383972168\n",
      "running loss 665.4622955322266\n",
      "running loss 703.340633392334\n",
      "running loss 740.5809211730957\n",
      "running loss 779.4991912841797\n",
      "running loss 817.0913848876953\n",
      "running loss 856.8617324829102\n",
      "running loss 895.8626480102539\n",
      "running loss 933.452278137207\n",
      "running loss 972.2128372192383\n",
      "running loss 1011.8568229675293\n",
      "running loss 1050.6358451843262\n",
      "running loss 1091.4345054626465\n",
      "running loss 1130.1463890075684\n",
      "running loss 1169.4651336669922\n",
      "running loss 1208.0076522827148\n",
      "running loss 1246.283748626709\n",
      "running loss 1283.854824066162\n",
      "running loss 1320.053134918213\n",
      "running loss 1358.10835647583\n",
      "running loss 1402.1125679016113\n",
      "running loss 1442.3371849060059\n",
      "running loss 1481.8077430725098\n",
      "running loss 1518.634407043457\n",
      "running loss 1557.2385635375977\n",
      "running loss 1596.515209197998\n",
      "running loss 1634.3102493286133\n",
      "running loss 1670.2990798950195\n",
      "running loss 1707.2601890563965\n",
      "running loss 1747.499179840088\n",
      "running loss 1786.3393478393555\n",
      "running loss 1828.2701034545898\n",
      "running loss 1864.991138458252\n",
      "running loss 1902.8824996948242\n",
      "running loss 1943.856258392334\n",
      "running loss 1982.0559692382812\n",
      "running loss 2016.1157150268555\n",
      "running loss 2055.240161895752\n",
      "running loss 2093.453769683838\n",
      "running loss 2130.768871307373\n",
      "running loss 2169.268238067627\n",
      "running loss 2203.2315559387207\n",
      "running loss 2239.7740440368652\n",
      "running loss 2275.2299728393555\n",
      "running loss 2315.274101257324\n",
      "running loss 2356.3593711853027\n",
      "running loss 2396.6703300476074\n",
      "running loss 2441.487236022949\n",
      "running loss 2477.980049133301\n",
      "running loss 2515.333869934082\n",
      "running loss 2554.039623260498\n",
      "running loss 2595.3028106689453\n",
      "running loss 2633.7620697021484\n",
      "running loss 2670.431930541992\n",
      "running loss 2708.7815589904785\n",
      "running loss 2747.7089347839355\n",
      "running loss 2787.155132293701\n",
      "running loss 2827.0640716552734\n",
      "running loss 2866.5884742736816\n",
      "running loss 2906.58890914917\n",
      "running loss 2944.7332649230957\n",
      "running loss 2983.4271697998047\n",
      "running loss 3021.9352798461914\n",
      "running loss 3063.1022567749023\n",
      "running loss 3102.155490875244\n",
      "running loss 3141.417263031006\n",
      "running loss 3185.5249977111816\n",
      "running loss 3222.5263633728027\n",
      "running loss 3263.9582328796387\n",
      "running loss 3301.255603790283\n",
      "running loss 3338.4784393310547\n",
      "running loss 3379.7481536865234\n",
      "running loss 3414.1983184814453\n",
      "running loss 3454.8422660827637\n",
      "running loss 3496.8575897216797\n",
      "running loss 3532.858917236328\n",
      "running loss 3570.8310737609863\n",
      "running loss 3608.7132453918457\n",
      "running loss 3648.7031860351562\n",
      "running loss 3688.572410583496\n",
      "running loss 3725.2681274414062\n",
      "running loss 3765.829128265381\n",
      "running loss 3807.9059562683105\n",
      "running loss 3845.8009300231934\n",
      "running loss 3881.5961952209473\n",
      "running loss 3924.3941078186035\n",
      "running loss 3961.538677215576\n",
      "running loss 3999.327693939209\n",
      "running loss 4039.1020545959473\n",
      "running loss 4079.374855041504\n",
      "running loss 4116.867038726807\n",
      "running loss 4155.603553771973\n",
      "running loss 4192.208717346191\n",
      "running loss 4230.388710021973\n",
      "running loss 4270.484657287598\n",
      "running loss 4310.499420166016\n",
      "running loss 4348.405700683594\n",
      "running loss 4386.5330810546875\n",
      "running loss 4423.842796325684\n",
      "running loss 4462.031177520752\n",
      "running loss 4500.717140197754\n",
      "running loss 4540.035717010498\n",
      "running loss 4579.814170837402\n",
      "running loss 4617.460826873779\n",
      "running loss 4654.632411956787\n",
      "running loss 4691.719913482666\n",
      "running loss 4733.819076538086\n",
      "running loss 4773.644233703613\n",
      "running loss 4809.430564880371\n",
      "running loss 4852.639301300049\n",
      "running loss 4892.172790527344\n",
      "running loss 4933.15096282959\n",
      "running loss 4975.784866333008\n",
      "running loss 5018.094966888428\n",
      "running loss 5058.842990875244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss 5098.584079742432\n",
      "running loss 5134.391632080078\n",
      "running loss 5173.300006866455\n",
      "running loss 5214.765548706055\n",
      "running loss 5255.124706268311\n",
      "running loss 5294.878391265869\n",
      "running loss 5336.253772735596\n",
      "running loss 5374.375747680664\n",
      "running loss 5416.610633850098\n",
      "running loss 5453.957981109619\n",
      "running loss 5493.806594848633\n",
      "running loss 5530.628074645996\n",
      "running loss 5568.334156036377\n",
      "running loss 5608.455974578857\n",
      "running loss 5646.343994140625\n",
      "running loss 5684.200927734375\n",
      "running loss 5722.530235290527\n",
      "running loss 5759.930233001709\n",
      "running loss 5802.933483123779\n",
      "running loss 5843.647167205811\n",
      "running loss 5882.516418457031\n",
      "running loss 5918.356052398682\n",
      "running loss 5955.599109649658\n",
      "running loss 5993.897830963135\n",
      "running loss 6035.803203582764\n",
      "running loss 6072.701053619385\n",
      "running loss 6112.268966674805\n",
      "running loss 6153.920742034912\n",
      "running loss 6192.319843292236\n",
      "running loss 6233.731899261475\n",
      "running loss 6274.972999572754\n",
      "running loss 6313.797870635986\n",
      "running loss 6353.276378631592\n",
      "running loss 6392.732181549072\n",
      "running loss 6432.553340911865\n",
      "running loss 6472.8159255981445\n",
      "running loss 6513.430610656738\n",
      "running loss 6553.861404418945\n",
      "running loss 6593.599540710449\n",
      "running loss 6632.89493560791\n",
      "running loss 6669.438571929932\n",
      "running loss 6707.313808441162\n",
      "running loss 6743.89448928833\n",
      "running loss 6784.833106994629\n",
      "running loss 6824.801998138428\n",
      "running loss 6862.428195953369\n",
      "running loss 6902.705368041992\n",
      "running loss 6941.337921142578\n",
      "running loss 6981.565841674805\n",
      "running loss 7020.634124755859\n",
      "running loss 7062.867736816406\n",
      "running loss 7100.763488769531\n",
      "running loss 7141.936668395996\n",
      "running loss 7180.555904388428\n",
      "running loss 7217.880939483643\n",
      "running loss 7254.884223937988\n",
      "running loss 7292.656517028809\n",
      "running loss 7332.811729431152\n",
      "running loss 7374.882484436035\n",
      "running loss 7413.058216094971\n",
      "running loss 7450.589126586914\n",
      "running loss 7488.882118225098\n",
      "running loss 7527.946826934814\n",
      "running loss 7566.066570281982\n",
      "running loss 7574.929167747498\n",
      "Epoch: 7.00, Train Loss: 0.30, Validation Loss: 0.29\n",
      "running loss 40.23060607910156\n",
      "running loss 78.63333511352539\n",
      "running loss 117.09349822998047\n",
      "running loss 156.99222946166992\n",
      "running loss 191.72979736328125\n",
      "running loss 229.6933479309082\n",
      "running loss 262.2954750061035\n",
      "running loss 296.66247177124023\n",
      "running loss 332.29129791259766\n",
      "running loss 367.6815643310547\n",
      "running loss 404.1531066894531\n",
      "running loss 440.8195266723633\n",
      "running loss 478.9263496398926\n",
      "running loss 517.0952186584473\n",
      "running loss 552.9694442749023\n",
      "running loss 589.08642578125\n",
      "running loss 626.4015235900879\n",
      "running loss 666.8501052856445\n",
      "running loss 705.0298614501953\n",
      "running loss 743.380313873291\n",
      "running loss 780.0233383178711\n",
      "running loss 817.4305000305176\n",
      "running loss 851.3826713562012\n",
      "running loss 890.5162162780762\n",
      "running loss 927.6102333068848\n",
      "running loss 968.290096282959\n",
      "running loss 1005.6170043945312\n",
      "running loss 1041.2564582824707\n",
      "running loss 1080.6057815551758\n",
      "running loss 1122.2700271606445\n",
      "running loss 1162.042953491211\n",
      "running loss 1198.8606910705566\n",
      "running loss 1237.1554908752441\n",
      "running loss 1275.9541015625\n",
      "running loss 1311.9623718261719\n",
      "running loss 1350.5857772827148\n",
      "running loss 1386.5364456176758\n",
      "running loss 1425.7194938659668\n",
      "running loss 1464.5370445251465\n",
      "running loss 1505.1304664611816\n",
      "running loss 1543.172519683838\n",
      "running loss 1578.1798934936523\n",
      "running loss 1618.342056274414\n",
      "running loss 1654.9812202453613\n",
      "running loss 1693.7797889709473\n",
      "running loss 1731.2452659606934\n",
      "running loss 1768.5531120300293\n",
      "running loss 1811.3854637145996\n",
      "running loss 1849.2215957641602\n",
      "running loss 1889.5732688903809\n",
      "running loss 1927.9967575073242\n",
      "running loss 1967.9052200317383\n",
      "running loss 2006.0968551635742\n",
      "running loss 2043.9166069030762\n",
      "running loss 2081.8680114746094\n",
      "running loss 2115.559314727783\n",
      "running loss 2152.3680152893066\n",
      "running loss 2189.564426422119\n",
      "running loss 2224.7925758361816\n",
      "running loss 2264.3071365356445\n",
      "running loss 2303.0088958740234\n",
      "running loss 2339.584426879883\n",
      "running loss 2375.7830505371094\n",
      "running loss 2412.829704284668\n",
      "running loss 2453.010025024414\n",
      "running loss 2491.5391883850098\n",
      "running loss 2531.876735687256\n",
      "running loss 2569.9718742370605\n",
      "running loss 2605.869426727295\n",
      "running loss 2644.055244445801\n",
      "running loss 2683.7614974975586\n",
      "running loss 2719.3781509399414\n",
      "running loss 2756.446460723877\n",
      "running loss 2795.949722290039\n",
      "running loss 2834.060707092285\n",
      "running loss 2872.921905517578\n",
      "running loss 2912.797294616699\n",
      "running loss 2950.642910003662\n",
      "running loss 2991.524486541748\n",
      "running loss 3029.264488220215\n",
      "running loss 3070.578266143799\n",
      "running loss 3110.942886352539\n",
      "running loss 3148.192768096924\n",
      "running loss 3185.8562622070312\n",
      "running loss 3220.98929977417\n",
      "running loss 3258.113498687744\n",
      "running loss 3294.2801246643066\n",
      "running loss 3333.9204597473145\n",
      "running loss 3370.986415863037\n",
      "running loss 3404.499599456787\n",
      "running loss 3441.176315307617\n",
      "running loss 3480.0841789245605\n",
      "running loss 3518.1583671569824\n",
      "running loss 3561.0433616638184\n",
      "running loss 3597.3185844421387\n",
      "running loss 3634.011203765869\n",
      "running loss 3672.417106628418\n",
      "running loss 3715.316638946533\n",
      "running loss 3751.5429077148438\n",
      "running loss 3789.716209411621\n",
      "running loss 3827.704055786133\n",
      "running loss 3864.071533203125\n",
      "running loss 3898.855422973633\n",
      "running loss 3937.802989959717\n",
      "running loss 3976.597343444824\n",
      "running loss 4013.1773681640625\n",
      "running loss 4052.5799140930176\n",
      "running loss 4088.0934524536133\n",
      "running loss 4132.027942657471\n",
      "running loss 4166.510219573975\n",
      "running loss 4201.877346038818\n",
      "running loss 4241.05717086792\n",
      "running loss 4280.064250946045\n",
      "running loss 4315.9916343688965\n",
      "running loss 4353.4943923950195\n",
      "running loss 4389.476787567139\n",
      "running loss 4423.288787841797\n",
      "running loss 4458.992359161377\n",
      "running loss 4496.655712127686\n",
      "running loss 4536.462982177734\n",
      "running loss 4572.774475097656\n",
      "running loss 4608.379871368408\n",
      "running loss 4645.0927085876465\n",
      "running loss 4683.585105895996\n",
      "running loss 4720.96826171875\n",
      "running loss 4760.5904541015625\n",
      "running loss 4802.434246063232\n",
      "running loss 4838.952922821045\n",
      "running loss 4873.720836639404\n",
      "running loss 4911.300842285156\n",
      "running loss 4950.654159545898\n",
      "running loss 4991.513439178467\n",
      "running loss 5030.495323181152\n",
      "running loss 5072.790428161621\n",
      "running loss 5113.278964996338\n",
      "running loss 5153.64359664917\n",
      "running loss 5198.038333892822\n",
      "running loss 5233.058795928955\n",
      "running loss 5269.391204833984\n",
      "running loss 5309.9969482421875\n",
      "running loss 5346.056259155273\n",
      "running loss 5382.364841461182\n",
      "running loss 5419.683982849121\n",
      "running loss 5456.550888061523\n",
      "running loss 5494.1256980896\n",
      "running loss 5528.184215545654\n",
      "running loss 5568.983997344971\n",
      "running loss 5607.64107131958\n",
      "running loss 5645.057987213135\n",
      "running loss 5681.619918823242\n",
      "running loss 5719.02555847168\n",
      "running loss 5751.252735137939\n",
      "running loss 5787.547534942627\n",
      "running loss 5823.143398284912\n",
      "running loss 5863.846977233887\n",
      "running loss 5902.644298553467\n",
      "running loss 5936.918548583984\n",
      "running loss 5975.717487335205\n",
      "running loss 6013.37540435791\n",
      "running loss 6049.520797729492\n",
      "running loss 6089.842906951904\n",
      "running loss 6128.681041717529\n",
      "running loss 6167.513378143311\n",
      "running loss 6205.393611907959\n",
      "running loss 6241.659034729004\n",
      "running loss 6276.926422119141\n",
      "running loss 6317.649417877197\n",
      "running loss 6354.060924530029\n",
      "running loss 6393.052219390869\n",
      "running loss 6434.847244262695\n",
      "running loss 6473.778999328613\n",
      "running loss 6512.796184539795\n",
      "running loss 6551.167572021484\n",
      "running loss 6590.023170471191\n",
      "running loss 6625.903335571289\n",
      "running loss 6666.821720123291\n",
      "running loss 6704.459167480469\n",
      "running loss 6742.851459503174\n",
      "running loss 6780.555423736572\n",
      "running loss 6817.899452209473\n",
      "running loss 6857.818790435791\n",
      "running loss 6893.78360748291\n",
      "running loss 6932.670875549316\n",
      "running loss 6966.700592041016\n",
      "running loss 7001.067474365234\n",
      "running loss 7037.561264038086\n",
      "running loss 7075.840297698975\n",
      "running loss 7111.250988006592\n",
      "running loss 7150.380718231201\n",
      "running loss 7191.649116516113\n",
      "running loss 7228.874988555908\n",
      "running loss 7270.791069030762\n",
      "running loss 7309.764259338379\n",
      "running loss 7349.201030731201\n",
      "running loss 7358.64658164978\n",
      "Epoch: 8.00, Train Loss: 0.30, Validation Loss: 0.28\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss 37.63560485839844\n",
      "running loss 75.1103401184082\n",
      "running loss 110.65984344482422\n",
      "running loss 149.52994537353516\n",
      "running loss 187.6104393005371\n",
      "running loss 223.59476470947266\n",
      "running loss 258.3505973815918\n",
      "running loss 294.6509819030762\n",
      "running loss 337.399845123291\n",
      "running loss 371.1471824645996\n",
      "running loss 407.09703063964844\n",
      "running loss 443.8332977294922\n",
      "running loss 485.7428169250488\n",
      "running loss 523.7224349975586\n",
      "running loss 560.7072792053223\n",
      "running loss 594.5687675476074\n",
      "running loss 630.1642570495605\n",
      "running loss 668.4862174987793\n",
      "running loss 705.9333152770996\n",
      "running loss 745.0536727905273\n",
      "running loss 780.8441925048828\n",
      "running loss 817.2839241027832\n",
      "running loss 849.0338840484619\n",
      "running loss 884.2321033477783\n",
      "running loss 921.5321140289307\n",
      "running loss 957.5655422210693\n",
      "running loss 997.2073993682861\n",
      "running loss 1033.356824874878\n",
      "running loss 1066.4153156280518\n",
      "running loss 1101.6791667938232\n",
      "running loss 1139.6920337677002\n",
      "running loss 1176.3328304290771\n",
      "running loss 1210.8951358795166\n",
      "running loss 1250.450475692749\n",
      "running loss 1288.0434017181396\n",
      "running loss 1324.9229640960693\n",
      "running loss 1360.5430278778076\n",
      "running loss 1396.3976345062256\n",
      "running loss 1432.3994808197021\n",
      "running loss 1468.818223953247\n",
      "running loss 1507.943796157837\n",
      "running loss 1543.646188735962\n",
      "running loss 1578.701208114624\n",
      "running loss 1612.6548099517822\n",
      "running loss 1646.2181034088135\n",
      "running loss 1681.7008838653564\n",
      "running loss 1718.5434665679932\n",
      "running loss 1754.1757678985596\n",
      "running loss 1790.2577457427979\n",
      "running loss 1828.647367477417\n",
      "running loss 1870.757028579712\n",
      "running loss 1908.4760074615479\n",
      "running loss 1942.0251712799072\n",
      "running loss 1983.8415355682373\n",
      "running loss 2019.7010402679443\n",
      "running loss 2050.553050994873\n",
      "running loss 2086.1317405700684\n",
      "running loss 2120.9655990600586\n",
      "running loss 2154.3178329467773\n",
      "running loss 2191.058292388916\n",
      "running loss 2225.2876777648926\n",
      "running loss 2262.9276084899902\n",
      "running loss 2300.45064163208\n",
      "running loss 2337.1559257507324\n",
      "running loss 2375.001148223877\n",
      "running loss 2414.456111907959\n",
      "running loss 2453.9575004577637\n",
      "running loss 2489.2207107543945\n",
      "running loss 2527.7418937683105\n",
      "running loss 2564.646240234375\n",
      "running loss 2601.8367500305176\n",
      "running loss 2639.577117919922\n",
      "running loss 2673.3992042541504\n",
      "running loss 2709.693645477295\n",
      "running loss 2744.111373901367\n",
      "running loss 2782.6589965820312\n",
      "running loss 2820.3430557250977\n",
      "running loss 2856.2297744750977\n",
      "running loss 2891.905960083008\n",
      "running loss 2933.196277618408\n",
      "running loss 2965.9257774353027\n",
      "running loss 3002.102710723877\n",
      "running loss 3035.890697479248\n",
      "running loss 3071.3579139709473\n",
      "running loss 3108.0198669433594\n",
      "running loss 3144.129596710205\n",
      "running loss 3181.4587745666504\n",
      "running loss 3217.1907958984375\n",
      "running loss 3254.291648864746\n",
      "running loss 3290.4986534118652\n",
      "running loss 3324.84561920166\n",
      "running loss 3365.019218444824\n",
      "running loss 3405.5022506713867\n",
      "running loss 3441.7303314208984\n",
      "running loss 3477.025718688965\n",
      "running loss 3515.8803329467773\n",
      "running loss 3551.5065994262695\n",
      "running loss 3587.3348236083984\n",
      "running loss 3623.917526245117\n",
      "running loss 3659.070152282715\n",
      "running loss 3695.1398735046387\n",
      "running loss 3732.737293243408\n",
      "running loss 3770.2644653320312\n",
      "running loss 3805.8091011047363\n",
      "running loss 3845.524856567383\n",
      "running loss 3881.245101928711\n",
      "running loss 3916.6945571899414\n",
      "running loss 3954.438995361328\n",
      "running loss 3991.748825073242\n",
      "running loss 4029.015426635742\n",
      "running loss 4066.6074409484863\n",
      "running loss 4102.085315704346\n",
      "running loss 4140.616313934326\n",
      "running loss 4177.388603210449\n",
      "running loss 4211.99254989624\n",
      "running loss 4249.519878387451\n",
      "running loss 4284.017322540283\n",
      "running loss 4321.7396240234375\n",
      "running loss 4356.328266143799\n",
      "running loss 4395.156276702881\n",
      "running loss 4431.623840332031\n",
      "running loss 4469.4118576049805\n",
      "running loss 4503.376052856445\n",
      "running loss 4538.656059265137\n",
      "running loss 4573.8969078063965\n",
      "running loss 4609.650241851807\n",
      "running loss 4648.850421905518\n",
      "running loss 4684.774635314941\n",
      "running loss 4720.749019622803\n",
      "running loss 4756.165596008301\n",
      "running loss 4793.452045440674\n",
      "running loss 4829.629302978516\n",
      "running loss 4870.994281768799\n",
      "running loss 4910.343372344971\n",
      "running loss 4946.565273284912\n",
      "running loss 4984.978210449219\n",
      "running loss 5027.421875\n",
      "running loss 5065.655723571777\n",
      "running loss 5101.281646728516\n",
      "running loss 5137.469570159912\n",
      "running loss 5174.416168212891\n",
      "running loss 5212.462242126465\n",
      "running loss 5251.201847076416\n",
      "running loss 5293.882476806641\n",
      "running loss 5331.244426727295\n",
      "running loss 5364.9071617126465\n",
      "running loss 5401.4106521606445\n",
      "running loss 5442.749931335449\n",
      "running loss 5476.404830932617\n",
      "running loss 5511.103626251221\n",
      "running loss 5547.747417449951\n",
      "running loss 5585.060897827148\n",
      "running loss 5623.146667480469\n",
      "running loss 5662.094707489014\n",
      "running loss 5698.391098022461\n",
      "running loss 5733.129253387451\n",
      "running loss 5767.074104309082\n",
      "running loss 5805.204174041748\n",
      "running loss 5841.4904136657715\n",
      "running loss 5876.342555999756\n",
      "running loss 5911.906894683838\n",
      "running loss 5950.354358673096\n",
      "running loss 5987.380489349365\n",
      "running loss 6027.48775100708\n",
      "running loss 6064.008464813232\n",
      "running loss 6101.128170013428\n",
      "running loss 6139.644996643066\n",
      "running loss 6178.15731048584\n",
      "running loss 6216.629940032959\n",
      "running loss 6251.187801361084\n",
      "running loss 6288.259632110596\n",
      "running loss 6325.40246963501\n",
      "running loss 6361.497360229492\n",
      "running loss 6401.3316650390625\n",
      "running loss 6438.525985717773\n",
      "running loss 6473.402683258057\n",
      "running loss 6513.70227432251\n",
      "running loss 6550.858345031738\n",
      "running loss 6584.862251281738\n",
      "running loss 6621.6477127075195\n",
      "running loss 6660.683895111084\n",
      "running loss 6697.6383934021\n",
      "running loss 6736.441860198975\n",
      "running loss 6774.307582855225\n",
      "running loss 6817.266090393066\n",
      "running loss 6852.219200134277\n",
      "running loss 6885.744655609131\n",
      "running loss 6921.6017990112305\n",
      "running loss 6957.046497344971\n",
      "running loss 6999.467082977295\n",
      "running loss 7035.613666534424\n",
      "running loss 7074.360897064209\n",
      "running loss 7107.0217933654785\n",
      "running loss 7143.520904541016\n",
      "running loss 7153.870723724365\n",
      "Epoch: 9.00, Train Loss: 0.29, Validation Loss: 0.27\n",
      "running loss 36.167198181152344\n",
      "running loss 70.95988845825195\n",
      "running loss 105.80581283569336\n",
      "running loss 140.27857208251953\n",
      "running loss 174.25353240966797\n",
      "running loss 209.89667510986328\n",
      "running loss 246.5400161743164\n",
      "running loss 283.0936813354492\n",
      "running loss 314.3974723815918\n",
      "running loss 350.4191703796387\n",
      "running loss 386.26818466186523\n",
      "running loss 421.1354179382324\n",
      "running loss 457.1015090942383\n",
      "running loss 492.7146224975586\n",
      "running loss 525.6085777282715\n",
      "running loss 562.6210975646973\n",
      "running loss 601.2445373535156\n",
      "running loss 633.3419494628906\n",
      "running loss 668.8661270141602\n",
      "running loss 705.9396553039551\n",
      "running loss 740.6852645874023\n",
      "running loss 775.7385749816895\n",
      "running loss 813.2028961181641\n",
      "running loss 848.4446296691895\n",
      "running loss 883.6840553283691\n",
      "running loss 924.4864273071289\n",
      "running loss 959.3035316467285\n",
      "running loss 997.8023109436035\n",
      "running loss 1034.5555229187012\n",
      "running loss 1069.0079765319824\n",
      "running loss 1107.0265350341797\n",
      "running loss 1146.3906440734863\n",
      "running loss 1179.7490577697754\n",
      "running loss 1214.1092834472656\n",
      "running loss 1250.7605323791504\n",
      "running loss 1283.4818382263184\n",
      "running loss 1317.4582786560059\n",
      "running loss 1353.1794242858887\n",
      "running loss 1389.2186965942383\n",
      "running loss 1424.7936935424805\n",
      "running loss 1462.242332458496\n",
      "running loss 1497.5654373168945\n",
      "running loss 1533.9245338439941\n",
      "running loss 1571.6539764404297\n",
      "running loss 1609.0253677368164\n",
      "running loss 1641.762351989746\n",
      "running loss 1680.4156913757324\n",
      "running loss 1718.6948204040527\n",
      "running loss 1754.8117752075195\n",
      "running loss 1792.7383613586426\n",
      "running loss 1828.8333168029785\n",
      "running loss 1861.7558708190918\n",
      "running loss 1898.1781425476074\n",
      "running loss 1932.7743911743164\n",
      "running loss 1965.785083770752\n",
      "running loss 2000.789966583252\n",
      "running loss 2037.1225776672363\n",
      "running loss 2073.1358604431152\n",
      "running loss 2109.241729736328\n",
      "running loss 2141.1946811676025\n",
      "running loss 2174.210168838501\n",
      "running loss 2209.3847522735596\n",
      "running loss 2243.606813430786\n",
      "running loss 2278.72580909729\n",
      "running loss 2316.7297229766846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss 2348.4608058929443\n",
      "running loss 2385.1130771636963\n",
      "running loss 2421.824338912964\n",
      "running loss 2456.19317817688\n",
      "running loss 2494.824415206909\n",
      "running loss 2531.4554119110107\n",
      "running loss 2571.204916000366\n",
      "running loss 2605.895589828491\n",
      "running loss 2639.5273876190186\n",
      "running loss 2671.379873275757\n",
      "running loss 2706.238721847534\n",
      "running loss 2745.1213970184326\n",
      "running loss 2780.94122505188\n",
      "running loss 2816.276819229126\n",
      "running loss 2848.159803390503\n",
      "running loss 2885.544927597046\n",
      "running loss 2921.286973953247\n",
      "running loss 2955.8790950775146\n",
      "running loss 2990.1678218841553\n",
      "running loss 3026.8144931793213\n",
      "running loss 3066.6211109161377\n",
      "running loss 3105.237314224243\n",
      "running loss 3138.036195755005\n",
      "running loss 3176.3031253814697\n",
      "running loss 3217.9384937286377\n",
      "running loss 3252.7213344573975\n",
      "running loss 3286.295534133911\n",
      "running loss 3320.558156967163\n",
      "running loss 3355.019811630249\n",
      "running loss 3395.367654800415\n",
      "running loss 3431.0824871063232\n",
      "running loss 3470.2549800872803\n",
      "running loss 3506.657548904419\n",
      "running loss 3541.668561935425\n",
      "running loss 3576.214403152466\n",
      "running loss 3611.196828842163\n",
      "running loss 3642.4694423675537\n",
      "running loss 3679.2511501312256\n",
      "running loss 3713.4722805023193\n",
      "running loss 3747.6877574920654\n",
      "running loss 3784.1804485321045\n",
      "running loss 3819.5332775115967\n",
      "running loss 3854.3371715545654\n",
      "running loss 3890.7414569854736\n",
      "running loss 3925.854051589966\n",
      "running loss 3963.036912918091\n",
      "running loss 3999.352876663208\n",
      "running loss 4035.226537704468\n",
      "running loss 4069.648675918579\n",
      "running loss 4103.31113243103\n",
      "running loss 4140.761312484741\n",
      "running loss 4172.889497756958\n",
      "running loss 4205.370252609253\n",
      "running loss 4240.9624881744385\n",
      "running loss 4277.498289108276\n",
      "running loss 4313.135744094849\n",
      "running loss 4351.050855636597\n",
      "running loss 4386.4523067474365\n",
      "running loss 4422.2133502960205\n",
      "running loss 4455.419313430786\n",
      "running loss 4491.698656082153\n",
      "running loss 4531.040166854858\n",
      "running loss 4568.331304550171\n",
      "running loss 4604.672903060913\n",
      "running loss 4643.028356552124\n",
      "running loss 4680.747312545776\n",
      "running loss 4717.132291793823\n",
      "running loss 4756.364835739136\n",
      "running loss 4795.961294174194\n",
      "running loss 4830.942949295044\n",
      "running loss 4868.51376914978\n",
      "running loss 4905.1444454193115\n",
      "running loss 4939.099950790405\n",
      "running loss 4974.028116226196\n",
      "running loss 5007.606187820435\n",
      "running loss 5043.36863899231\n",
      "running loss 5078.197954177856\n",
      "running loss 5113.215539932251\n",
      "running loss 5152.385293960571\n",
      "running loss 5191.400636672974\n",
      "running loss 5227.404653549194\n",
      "running loss 5261.150194168091\n",
      "running loss 5295.004503250122\n",
      "running loss 5330.9652462005615\n",
      "running loss 5369.39052772522\n",
      "running loss 5404.61626625061\n",
      "running loss 5440.067407608032\n",
      "running loss 5474.3625774383545\n",
      "running loss 5510.557046890259\n",
      "running loss 5546.09148979187\n",
      "running loss 5581.297315597534\n",
      "running loss 5615.066717147827\n",
      "running loss 5646.354444503784\n",
      "running loss 5680.981595993042\n",
      "running loss 5716.597471237183\n",
      "running loss 5750.143423080444\n",
      "running loss 5783.801870346069\n",
      "running loss 5816.364698410034\n",
      "running loss 5854.556314468384\n",
      "running loss 5890.506349563599\n",
      "running loss 5922.4298305511475\n",
      "running loss 5962.06561088562\n",
      "running loss 5995.656141281128\n",
      "running loss 6030.717161178589\n",
      "running loss 6064.235990524292\n",
      "running loss 6098.973024368286\n",
      "running loss 6133.065523147583\n",
      "running loss 6167.72264289856\n",
      "running loss 6203.504076004028\n",
      "running loss 6240.702260971069\n",
      "running loss 6278.989770889282\n",
      "running loss 6315.556886672974\n",
      "running loss 6350.599870681763\n",
      "running loss 6387.032976150513\n",
      "running loss 6419.06544303894\n",
      "running loss 6454.568204879761\n",
      "running loss 6488.42774772644\n",
      "running loss 6525.113710403442\n",
      "running loss 6563.165235519409\n",
      "running loss 6599.941282272339\n",
      "running loss 6636.618284225464\n",
      "running loss 6675.428541183472\n",
      "running loss 6706.100351333618\n",
      "running loss 6743.5892124176025\n",
      "running loss 6777.625894546509\n",
      "running loss 6810.9663944244385\n",
      "running loss 6846.28652381897\n",
      "running loss 6880.721788406372\n",
      "running loss 6914.02822303772\n",
      "running loss 6924.9957818984985\n",
      "Epoch: 10.00, Train Loss: 0.28, Validation Loss: 0.26\n"
     ]
    }
   ],
   "source": [
    "model = EmbModel(emb_type=\"densenet121\", feature_size_override=1024, pretrain=False, num_labels=8)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "def train_model_one_epoch(model, train_loader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if torch.isnan(outputs).any():\n",
    "            raise ValueError(\"NaN detected in model outputs\")\n",
    "        \n",
    "        loss = loss_func(outputs, labels)\n",
    "        if torch.isnan(loss).any():\n",
    "            raise ValueError(\"NaN detected in loss computation\")\n",
    "            \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        print('running loss', running_loss)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def validate_model(model, val_loader, loss_func):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "num_epoch = 10\n",
    "# model training loop: it is better to print the training/validation losses during the training\n",
    "for i in range(num_epoch):\n",
    "    train_loss = train_model_one_epoch(model, train_loader, loss_func, optimizer)\n",
    "    valid_loss = validate_model(model, val_loader, loss_func)\n",
    "    print(\"Epoch: %.2f, Train Loss: %.2f, Validation Loss: %.2f\" % (i+1, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "\n",
    "torch.save(model.state_dict(), \"model/model-snapshot-\" + dt_string + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code doesn't work yet, I am currently figuring out how to make it work. Use the training block in the cell above (it is very slow-running though)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "import json\n",
    "\n",
    "def train_models(df, pred_col, pred_vals, fix_col, fix_vals, results_dict, task_type):   \n",
    "    for fix_val in tqdm(fix_vals):\n",
    "        # Subset the dataframe to just have this val in the column\n",
    "        df_fix = df[df[fix_col] == fix_val]\n",
    "        # Drop this column to avoid any oddities during training\n",
    "        df_fix = df_fix.drop(columns=fix_col)\n",
    "        \n",
    "        # Get just the 2/4 classes that we're trying to \n",
    "        for pred_val in tqdm(pred_vals):\n",
    "            df_pred = df_fix[df_fix[pred_col].isin(pred_val)]\n",
    "            \n",
    "            # We have the final dataframe, but we need to create a perfectly balanced \n",
    "            # version of it\n",
    "            grouped = df_pred.groupby(pred_col)\n",
    "            # print(\"Count per class:\", grouped[\"emb0\"].count())\n",
    "            min_group_size = grouped.count()[\"emb0\"].min()\n",
    "            df_bal = grouped.sample(n=min_group_size, random_state=0)\n",
    "            # print(\"Count per class after balancing:\", df_bal.groupby(pred_col)[\"emb0\"].count())\n",
    "            \n",
    "            # Note that we may have a single class remaining in our dataset (if we're doing the baseline \n",
    "            # CXP vs CXP prediction, for example). We need to check that and manually change our dataset\n",
    "            # If that is the case\n",
    "            df_bal = df_bal.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "            \n",
    "            if len(pred_val) == 1:\n",
    "                print(f\"INFO: SINGLE PRED VAL: {pred_val} for col: {pred_col}... Subsetting through the middle\")\n",
    "                mid_val = len(df_bal) // 2\n",
    "                \n",
    "                df_bal.loc[:mid_val, pred_col] = \"0\"\n",
    "                df_bal.loc[mid_val:, pred_col] = \"1\"\n",
    "            \n",
    "            # Now lets pass this dataframe into our train method\n",
    "            acc = train_model(df_bal, pred_col)\n",
    "            \n",
    "            # Store the results in our global dictionary\n",
    "            results_dict[task_type].append({\n",
    "                \"fix_val\": fix_val,\n",
    "                \"pred_val\": pred_val,\n",
    "                \"min_group_size\": min_group_size,\n",
    "                \"df_size\": len(df_bal),\n",
    "                \"acc\": acc,\n",
    "            })\n",
    "            \n",
    "def train_model(df, pred_col, max_iter=5000):\n",
    "    X, y = df.drop(columns=pred_col), df[pred_col]\n",
    "    \n",
    "    model = make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-5, max_iter=max_iter))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    return acc            \n",
    "            \n",
    "def main():\n",
    "#     wandb.init(project=\"ood-generalization\",\n",
    "#             job_type=\"emb_train\", \n",
    "#             entity=\"basedrhys\",\n",
    "#               name=f\"row {row_idx}\")\n",
    "    \n",
    "    results_dict = {}\n",
    "    results_dict[\"env_pred\"] = []\n",
    "    results_dict[\"label_pred\"] = []\n",
    "\n",
    "    # Environment Prediction Task\n",
    "    env_fix_col = \"targets\"\n",
    "    env_fix_vals = [0, 1]\n",
    "\n",
    "    env_pred_col = \"env\"\n",
    "    env_pred_vals = [(\"CXP\", ), (\"MIMIC\", ), (\"NIH\", ), (\"PAD\", ), (\"CXP\",\"NIH\"), (\"CXP\",\"PAD\"), (\"MIMIC\",\"CXP\"), (\"MIMIC\",\"NIH\"), (\"MIMIC\",\"PAD\"), (\"NIH\",\"PAD\"), (\"CXP\", \"MIMIC\", \"NIH\", \"PAD\")]\n",
    "\n",
    "    train_models(df=ml_df, \n",
    "                 pred_col=env_pred_col,\n",
    "                 pred_vals=env_pred_vals,\n",
    "                 fix_col=env_fix_col,\n",
    "                 fix_vals=env_fix_vals,\n",
    "                 results_dict=results_dict,\n",
    "                 task_type=\"env_pred\")\n",
    "    \n",
    "    # Label prediction task\n",
    "    label_fix_col = \"env\"\n",
    "    label_fix_vals = [\"CXP\", \"MIMIC\", \"NIH\", \"PAD\"]\n",
    "\n",
    "    label_pred_col = \"targets\"\n",
    "    label_pred_vals = [(0, 1), (0, ), (1, )]\n",
    "    \n",
    "    train_models(df=ml_df,\n",
    "                 pred_col=label_pred_col,\n",
    "                 pred_vals=label_pred_vals,\n",
    "                 fix_col=label_fix_col,\n",
    "                 fix_vals=label_fix_vals,\n",
    "                 results_dict=results_dict,\n",
    "                 task_type=\"label_pred\")\n",
    "    \n",
    "#     wandb.log(results_dict)\n",
    "    \n",
    "    output_dir = row[\"output_dir\"]\n",
    "    \n",
    "    print(\"Outputting JSON to\", output_dir)\n",
    "    \n",
    "    with open(f\"{output_dir}/emb_test_results.json\", mode=\"w\") as f:\n",
    "        json.dump(results_dict, f, indent=True)\n",
    "    \n",
    "    return results_dict\n",
    "row_idx = 2\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOdhGrbwwG71"
   },
   "source": [
    "_You don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper_\n",
    "\n",
    "Once available, I plan to compare my model performance using different datasets with the results posted in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "# Discussion\n",
    "\n",
    "## Is the paper reproducible?\n",
    "It is too early to tell right now, but at least a portion of the code provided is runnable with minimal updates. I was able to reproduce the initial dataset statistics, so at least that portion is definitely reproducible. The rest will depend on whether I am able to solve and run the training.\n",
    "\n",
    "## If the paper is _not_ reproducible, explain the results\n",
    "TBD depending on whether the paper results will be reproducible or not.\n",
    "\n",
    "## What was easy and what was difficult\n",
    "The authors did a great job documenting some parts of the project, for example, access to data. Following the instructions was very easy, and while MIMIC-CXR-JPG dataset access took some time to get, overall the process was a breeze.\n",
    "\n",
    "Downloading the datasets is a hassle though, I ran out of space on my laptop, had to buy an external drive and restart the download process for MIMIC-CXR-JPG a few times.\n",
    "\n",
    "There are a few notebooks and standalone scripts provided to process the data. While it is possible to figure out what steps need to be done in what order, many of the parts of the process are not documented. 'pyproject.toml' did not run successfully for me, and I've been stuck trying to figure out why and how to run it (I have a suspicion my processor architecture is not supported, but not enough experience to tell for sure yet).\n",
    "\n",
    "In parallel, I opted to re-implement the training and model validation myself. There is code for training and validation in the project, which has a lot of comments (great!), but the process itself is not well documented, so the reproducer is left figuring out which steps in the code are needed and which are not, and how to adapt it to use for their experiment. The code is very general and there is a lot of it. There are some pointers in the readme, but they are at this point not sufficient for reproducing things successfully without additional modification.\n",
    "\n",
    "wandb isn't really working for me either yet, and I am yet to figure out why it is needed and whether it is necessary to reproduce the results. \n",
    "\n",
    "The data is not processed evenly / equally for each dataset, there are different values for the same labels (NaN, True/False, 1/0, 1.1/0.0). I had to write some processing code to make sure we mitigate those differences. \n",
    "\n",
    "Additional complication is due to the fact that the amount of data is very large. Any training or processing takes a long time, the notebook kernel dies frequently and the overall process is frustrating.\n",
    "\n",
    "I tried to avoid multiple separate files and scripts, and pulled many of the data preprocessing into my notebook. However, this increased the runtime of the notebook significantly. Additional factor affecting the runtime is the size of the input data, even when working on one dataset. I doubt it would be possible to achieve the 8 minute runtime, but will try to do so.\n",
    "  \n",
    "## Suggestions for the author\n",
    "\n",
    "Trim the codebase leaving only relevant parts. Add documentation for the training and validation process. Add some background on why wandb is used and how to use it for this project correctly. Provide a suggested order of execution for the notebooks.\n",
    "\n",
    "## Plans for the next phase\n",
    "\n",
    "In the remaining time until the final submission May 7 deadline, my plans are:\n",
    "* Further update the data processing functions so they are producing similar type results (right now while compatible, it's a mix of _int_, _float_ and _True/False_, I would like to homogenize the resulting dataset further)\n",
    "* Finalize the training for the model and compute worst per-group accuracy for all data combinations listed in the article (so far I had the most issues with training, as the code supplied with the article didn't work and I had to come up with my own in which I try to replicate the experiment as close to the article description as possible)\n",
    "* Plot the results and finalize the writeup (compute both worst per-group accuracy, and AUROC and compare to the results of the article)\n",
    "* Prepare a subset of data and optimize the notebook to run under 8 minutes if at all possible (as per the original requirements) - this might be complicated as the main focus of this article is dealing with more data and all datasets are quite large. Randomly picking samples from each dataset might further introduce some unintended spurious correlations\n",
    "* Check the main hypothesis on both balanced and unbalanced datasets, time permitting \n",
    "* Prepare a video presentation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# References\n",
    "\n",
    "1.   Rhys Compton; Lily Zhang; Aahlad Puli; Rajesh Ranganath, When More is Less: Incorporating Additional Datasets Can Hurt Performance By Introducing Spurious Correlations, arXiv preprint, 2023-08-09, Accepted at MLHC 2023, doi: [10.48550/arXiv.2308.04431](https://doi.org/10.48550/arXiv.2308.04431)\n",
    "2.   Haoran Zhang, Natalie Dullerud, Laleh Seyyed-Kalantari, Quaid Morris, Shalmali Joshi, and Marzyeh Ghassemi. An empirical framework for domain generalization in clinical settings. In Proceedings of the Conference on Health, Inference, and Learning, pages 279–290, 2021, doi: [10.48550/arXiv.2103.11163](https://doi.org/10.48550/arXiv.2103.11163)\n",
    "3.   Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708, 2017, doi: [10.48550/arXiv.1608.06993](https://doi.org/10.48550/arXiv.1608.06993)\n",
    "4.   Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large- scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255, 2009, doi: [10.1109/CVPR.2009.5206848](https://doi.org/10.1109/CVPR.2009.5206848)\n",
    "5.   John R Zech, Marcus A Badgeley, Manway Liu, Anthony B Costa, Joseph J Titano, and Eric Karl Oermann. Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study. PLoS medicine, 15(11): e1002683, 2018, doi: [10.1371/journal.pmed.1002683](https://doi.org/10.1371/journal.pmed.1002683)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
