{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j01aH0PR4Sg-"
   },
   "source": [
    "# Nina Stawski's (group 90) final project report [DRAFT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illinois ID: ninas2\n",
    "\n",
    "\n",
    "[GitHub repo link](https://github.com/nstawski/dlh-final-project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sfk8Zrul_E8V"
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# disabling the cell since I am not using it, but keeping in the notebook in case I need it in the future.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "## Background of the problem\n",
    "\n",
    "### Type of problem\n",
    "  \n",
    "  This is a data preparation and processing problem. The authors of the article are testing a common belief that adding more data improves the resulting model performance. Their main hypothesis, which they subsequently prove, is that incorporating more data does not necessary improve the model performance. It can introduce spurious correlations, and hurt the resulting model performance rather than helping it.\n",
    "\n",
    "### What is the importance/meaning of solving the problem\n",
    "  \n",
    "  The paper is challenging a common belief, meaning a lot of researchers are likely trying to incorporate as much data as they can expecting it would improve the performance of their models. The outcome of this research would provide guidance on the possible pitfalls and the cases where you wouldn't want to add external data - so it could set a new standard of processing and incorporating data for everyone in the field.\n",
    "\n",
    "### The difficulty of the problem\n",
    "\n",
    "  The problem is non-obvious and the paper is challenging the common belief held in the industry. The authors are putting a lot of state-of-the-art approaches to the test, and attempt to quantify the results as well as provide new standards and explanations. This is extremely hard to do so I believe the problem is difficult.\n",
    "\n",
    "### The state of the art methods and effectiveness\n",
    "\n",
    "  The \"industry standard\" way of improving model performance is adding more data from additional datasets, which the authors of this article prove to not be effective, and even being harmful in many cases.\n",
    "\n",
    "  One of the main issues causing the model performance decrease when adding more data from other sources is spurious correlations, which in case of x-rays could be coming even from the scanner artifacts, or other hospital-specific data. One of the state-of-the-art ways to mitigate this is balancing a dataset to reduce the influence of hospital-specific factors. While balancing definitely improved the situation, the resulting model performance was still in many cases worse than with a single-hospital dataset.\n",
    "\n",
    "\n",
    "## Paper explanation\n",
    "### What did the paper propose\n",
    "The paper used four most-used chest x-ray datasets - MIMIC-CXR-JPG, CheXpert, PadChest, ChestXray8 - to disprove a popular belief that adding more data always would improve the performance of your model. They postulate that, for the specific x-ray data, even the scanners themselves, the way hospitals produce data, or send specific patients to specific places to do their scan, can introduce spurious correlations which, in many cases, significantly affect the worst group performance.\n",
    "\n",
    "### What is the innovations of the method\n",
    "Existing research (for example, John R Zech, Marcus A Badgeley, Manway Liu, Anthony B Costa, Joseph J Titano, and Eric Karl Oermann. Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study. PLoS medicine, 15(11): e1002683, 2018.) proves that adding a second dataset improves the average per-group accuracy. In contrast, the paper I am reproducing focuses on the worst per-group accuracy.\n",
    "\n",
    "### How well the proposed method work (in its own metrics)\n",
    "According to the article authors, their method works really well and proves that in nearly 50% of cases adding a second dataset, and even balancing it to reduce spurious correllations doesn't get the model to perform better than without that additional dataset. The models pick up on hospital-specific features even if those features weren't explicitly defined in the original data. They postulate that every CNN model, regardless of training disease or datasets, learns embeddings that can distinguish any of the hospital sources with near-perfect accuracy, even if the embeddings were trained via one or two hospitals’ data.\n",
    "\n",
    "### What is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n",
    "\n",
    "The article cautions against blindly adding more datasets, and provides a number of approaches you can take if you still decide to do so. The conclusion is adding more data shouldn't be done blindly. The authors of the article definitely discourage the researchers from the most common approach of throwing data at the problem to improve model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "# Scope of Reproducibility:\n",
    "\n",
    "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
    "\n",
    "![Paper overview](https://raw.githubusercontent.com/basedrhys/ood-generalization/5d8ff09eba4c0b4b20b5ae2814fe865bed1dfb0e/img/high_level_overview.png)\n",
    "\n",
    "## Hypothesis 1\n",
    "\n",
    "In 43% of training dataset/disease tasks, adding data from an external source hurts worst-group performance.\n",
    "\n",
    "\n",
    "## Hypothesis 2\n",
    "\n",
    "Balancing the dataset to reduce spurious correlations is often beneficial, but in the scenarios where adding an additional data source hurts generalization performance, it does not always improve generalization; in some cases, training on a balanced dataset achieves lower worst-group accuracy than training on datasets from one or two hospitals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rRksCB1vbYwJ"
   },
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# disabling the cell since I am not using it, but keeping in the notebook in case I need it in the future.\n",
    "\n",
    "# no code is required for this section\n",
    "'''\n",
    "if you want to use an image outside this notebook for explanaition,\n",
    "you can upload it to your google drive and show it with OpenCV or matplotlib\n",
    "'''\n",
    "# mount this notebook to your google drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# define dirs to workspace and data\n",
    "img_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-your-image>'\n",
    "\n",
    "import cv2\n",
    "img = cv2.imread(img_dir)\n",
    "cv2.imshow(\"Title\", img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "# Methodology\n",
    "\n",
    "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
    "\n",
    "The methodology at least contains two subsections **data** and **model** in your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "yu61Jp1xrnKk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Data_Constants' from '/Users/noemi/dlh-final-project/Data_Constants.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import  packages you need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from os.path import exists\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "import random\n",
    "from IPython.display import display\n",
    "# from google.colab import drive\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import Data_Constants as Constants\n",
    "\n",
    "#making sure all referenced files are reloaded\n",
    "import importlib\n",
    "importlib.reload(Constants)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "#  Data\n",
    "The study is using four datasets: MIMIC-CXR-JPG, CheXpert, PadChest, ChestXray8\n",
    "\n",
    "The datasets are being filtered to include only frontal (PA/AP) images. Instances are labeled with one or more pathologies. Each dataset has a different set of diseases but they are preprocessed using code derived from ClinicalDG2 (Zhang et al., 2021) to extract the eight common labels and homogenize the datasets. Additionally, authors of the article created the Any label which indicates a positive label for any of the seven common disease labels, resulting in nine different binary labels. All experiments use the labels in a binary manner; a pathology is chosen as the target label, with an instance labeled 1 if the pathology of interest is present and 0 otherwise. \n",
    "\n",
    "The autors apply an 80%/10%/10% subject-wise train/val/test split, with the same split used across seeds.\n",
    "\n",
    "### MIMIC-CXR\n",
    "\n",
    "1. [Obtain access](https://mimic-cxr.mit.edu/about/access/) to the MIMIC-CXR-JPG Database Database on PhysioNet and download the [dataset](https://physionet.org/content/mimic-cxr-jpg/2.0.0/). The best option is downloading from the GCP bucket:\n",
    "\n",
    "```sh\n",
    "gcloud auth login\n",
    "mkdir MIMIC-CXR-JPG\n",
    "gsutil -m rsync -d -r gs://mimic-cxr-jpg-2.0.0.physionet.org MIMIC-CXR-JPG\n",
    "```\n",
    "\n",
    "2. In order to obtain gender information for each patient, you will need to obtain access to [MIMIC-IV](https://physionet.org/content/mimiciv/0.4/). Download `core/patients.csv.gz` and place the file in the `MIMIC-CXR-JPG` directory.\n",
    "\n",
    "### CheXpert\n",
    "1. Sign up with your email address [here](https://stanfordmlgroup.github.io/competitions/chexpert/).\n",
    "\n",
    "2. Download either the original or the downsampled dataset (we recommend the downsampled version - `CheXpert-v1.0-small.zip`) and extract it.\n",
    "\n",
    "### ChestX-ray8\n",
    "\n",
    "1. Download the `images` folder and `Data_Entry_2017_v2020.csv` from the [NIH website](https://nihcc.app.box.com/v/ChestXray-NIHCC).\n",
    "\n",
    "2. Unzip all of the files in the `images` folder.\n",
    "\n",
    "### PadChest\n",
    "\n",
    "1. The paper uses a resized version of PadChest, which can be downloaded [here](https://academictorrents.com/details/96ebb4f92b85929eadfb16761f310a6d04105797).\n",
    "\n",
    "2. Unzip `images-224.tar`.\n",
    "\n",
    "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
    "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
    "  * Illustration: printing results, plotting figures for illustration.\n",
    "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset.\n",
    "  \n",
    "## Data Processing\n",
    "The original pre-processing for the article was done using the scripts outside of the Jupyter Notebook. Some of them didnt' work for me, and the installation process didn't succeed despite multiple attempts either. Instead, I have adapted some of the original scripts to run in the notebook (with some modifications so they actually work with my data), using the external \"Constants.py\" file that points to the location of the datasets.\n",
    "1. In `./Data_Constants.py`, update `image_paths` to point to each of the four directories that you downloaded.\n",
    "\n",
    "2. Run the next two cells to pre-process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzVUQS0CHry0"
   },
   "source": [
    "## Validating\n",
    "I am using the validation and pre-processing code provided by the authors of the article, with some modifications to make it run as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making sure constants are up to date if they were changed\n",
    "importlib.reload(Constants)\n",
    "\n",
    "def validate_mimic():\n",
    "    img_dir = Path(Constants.image_paths['MIMIC'])\n",
    "    meta_dir = Path(Constants.meta_paths['MIMIC'])\n",
    "    \n",
    "    assert (meta_dir/'mimic-cxr-2.0.0-metadata.csv').is_file()\n",
    "    assert (meta_dir/'mimic-cxr-2.0.0-negbio.csv').is_file()\n",
    "    assert (meta_dir/'patients.csv').is_file()\n",
    "    # modified the file that's being checked since I don't have the full MIMIC-CXR-JPG dataset due to space limitations\n",
    "    # in the original script, the file in p19 was being checked.\n",
    "    assert (img_dir/'p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg').is_file()\n",
    "\n",
    "def validate_cxp():\n",
    "    img_dir = Path(Constants.image_paths['CXP'])\n",
    "    if (img_dir/'CheXpert-v1.0').is_dir():\n",
    "        cxp_subfolder = 'CheXpert-v1.0'\n",
    "    else:\n",
    "        cxp_subfolder = 'CheXpert-v1.0-small'\n",
    "    assert (img_dir/cxp_subfolder/'train.csv').is_file()\n",
    "    assert (img_dir/cxp_subfolder/'train/patient48822/study1/view1_frontal.jpg').is_file()\n",
    "    assert (img_dir/cxp_subfolder/'valid/patient64636/study1/view1_frontal.jpg').is_file()\n",
    "\n",
    "def validate_pad():\n",
    "    img_dir = Path(Constants.image_paths['PAD'])\n",
    "    meta_dir = Path(Constants.meta_paths['PAD'])\n",
    "    assert (meta_dir/'PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv').is_file()\n",
    "    assert (img_dir/'185566798805711692534207714722577525271_qb3lyn.png').is_file()\n",
    "\n",
    "def validate_nih():\n",
    "    img_dir = Path(Constants.image_paths['NIH'])\n",
    "    meta_dir = Path(Constants.meta_paths['NIH'])\n",
    "    assert (meta_dir/'Data_Entry_2017.csv').is_file()\n",
    "    assert (img_dir/'images/00002072_003.png').is_file()\n",
    "\n",
    "def validate_splits():\n",
    "    for dataset in Constants.df_paths:\n",
    "        for split in Constants.df_paths[dataset]:\n",
    "            assert Path(Constants.df_paths[dataset][split]).is_file()\n",
    "\n",
    "\n",
    "def validate_all():\n",
    "    validate_mimic()\n",
    "    validate_cxp()\n",
    "    validate_nih()\n",
    "    validate_pad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating paths...\n",
      "Preprocessing MIMIC-CXR...\n",
      "Preprocessing CheXpert...\n",
      "Preprocessing ChestX-ray8...\n",
      "Preprocessing PadChest... This might take a few minutes...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# making sure constants are up to date if they were changed after running this notebook\n",
    "importlib.reload(Constants)\n",
    "\n",
    "def preprocess_mimic():\n",
    "    img_dir = Path(Constants.image_paths['MIMIC'])\n",
    "    meta_dir = Path(Constants.meta_paths['MIMIC'])\n",
    "    out_folder = meta_dir/'clinicaldg'\n",
    "    out_folder.mkdir(parents = True, exist_ok = True)  \n",
    "\n",
    "    patients = pd.read_csv(meta_dir/'patients.csv')\n",
    "    labels = pd.read_csv(meta_dir/'mimic-cxr-2.0.0-negbio.csv')\n",
    "    meta = pd.read_csv(meta_dir/'mimic-cxr-2.0.0-metadata.csv')\n",
    "\n",
    "    df = meta.merge(patients, on = 'subject_id').merge(labels, on = ['subject_id', 'study_id'])\n",
    "    df['age_decile'] = pd.cut(df['anchor_age'], bins = list(range(0, 100, 10))).apply(lambda x: f'{x.left}-{x.right}').astype(str)\n",
    "    df['frontal'] = df.ViewPosition.isin(['AP', 'PA'])\n",
    "\n",
    "    df['path'] = df.apply(lambda x: os.path.join(f'p{str(x[\"subject_id\"])[:2]}', f'p{x[\"subject_id\"]}', f's{x[\"study_id\"]}', f'{x[\"dicom_id\"]}.jpg'), axis = 1)\n",
    "    df.to_csv(out_folder/\"preprocessed.csv\", index=False)\n",
    "\n",
    "def preprocess_pad():\n",
    "    # I have modified this function from the original one, because I was getting missing/ambiguous Dtype errors\n",
    "    img_dir = Path(Constants.image_paths['PAD'])\n",
    "    meta_dir = Path(Constants.meta_paths['PAD'])\n",
    "    out_folder = meta_dir/'clinicaldg'\n",
    "    out_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    dtype_spec = {\n",
    "        'ImageID': str,\n",
    "        'StudyID': str,\n",
    "        'PatientID': str,\n",
    "        'PatientBirth': str, # converting this to the integer later to avoid processing errors (due some data apparently being saved as float)\n",
    "        'PatientSex_DICOM': str,\n",
    "        'ViewPosition_DICOM': str,\n",
    "        'Projection': str,\n",
    "        'Labels': str,\n",
    "        'WindowCenter_DICOM': str,\n",
    "        'WindowWidth_DICOM': str\n",
    "    }\n",
    "\n",
    "    df = pd.read_csv(meta_dir/'PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv', dtype=dtype_spec)\n",
    "    df = df[['ImageID', 'StudyID', 'PatientID', 'PatientBirth', 'PatientSex_DICOM', 'ViewPosition_DICOM', 'Projection', 'Labels']]\n",
    "    df = df[~df[\"Labels\"].isnull()]\n",
    "    df = df[df[\"ImageID\"].apply(lambda x: os.path.exists(os.path.join(img_dir, x)))]\n",
    "    df = df[df.Projection.isin(['PA', 'L', 'AP_horizontal', 'AP'])]\n",
    "\n",
    "    df['frontal'] = ~(df['Projection'] == 'L')\n",
    "    df = df[~df['Labels'].apply(lambda x: 'exclude' in x or 'unchanged' in x)]\n",
    "\n",
    "    mapping = dict()\n",
    "    mapping['Effusion'] = ['hydropneumothorax', 'empyema', 'hemothorax']\n",
    "    mapping[\"Consolidation\"] = [\"air bronchogram\"]\n",
    "    mapping['No Finding'] = ['normal']\n",
    "\n",
    "    for pathology in Constants.take_labels:\n",
    "        mask = df[\"Labels\"].str.contains(pathology.lower())\n",
    "        if pathology in mapping:\n",
    "            for syn in mapping[pathology]:\n",
    "                mask |= df[\"Labels\"].str.contains(syn.lower())\n",
    "        df[pathology] = mask.astype(int)\n",
    "\n",
    "    df['PatientBirth'] = df['PatientBirth'].dropna().astype(float).astype(int)\n",
    "    df['Age'] = 2017 - df['PatientBirth']\n",
    "    df.reset_index(drop=True).to_csv(out_folder/\"preprocessed.csv\", index=False)\n",
    "\n",
    "\n",
    "def preprocess_cxp():\n",
    "    img_dir = Path(Constants.image_paths['CXP'])\n",
    "    out_folder = img_dir/'clinicaldg'\n",
    "    if (img_dir/'CheXpert-v1.0'/'train.csv').is_file():\n",
    "        df = pd.concat([pd.read_csv(img_dir/'CheXpert-v1.0'/'train.csv'), \n",
    "                        pd.read_csv(img_dir/'CheXpert-v1.0'/'valid.csv')],\n",
    "                        ignore_index = True)\n",
    "    elif (img_dir/'CheXpert-v1.0-small'/'train.csv').is_file(): \n",
    "        df = pd.concat([pd.read_csv(img_dir/'CheXpert-v1.0-small'/'train.csv'),\n",
    "                        pd.read_csv(img_dir/'CheXpert-v1.0-small'/'valid.csv')],\n",
    "                        ignore_index = True)\n",
    "    elif (img_dir/'train.csv').is_file():\n",
    "        raise ValueError('Please set Constants.image_paths[\"CXP\"] to be the PARENT of the current'+\n",
    "                ' directory and rerun this script.')\n",
    "    else:\n",
    "        raise ValueError(\"CheXpert files not found!\")\n",
    "\n",
    "    out_folder.mkdir(parents = True, exist_ok = True)  \n",
    "\n",
    "    df['subject_id'] = df['Path'].apply(lambda x: int(Path(x).parent.parent.name[7:]))\n",
    "    df['Path'] = df['Path'].apply(lambda x: str(x).replace(\"CheXpert-v1.0/\", \"\"))\n",
    "    df.reset_index(drop = True).to_csv(out_folder/\"preprocessed.csv\", index=False)\n",
    "\n",
    "def preprocess_nih():\n",
    "    img_dir = Path(Constants.image_paths['NIH'])\n",
    "    meta_dir = Path(Constants.meta_paths['NIH'])\n",
    "    out_folder = meta_dir/'clinicaldg'\n",
    "    out_folder.mkdir(parents = True, exist_ok = True)  \n",
    "    df = pd.read_csv(meta_dir/\"Data_Entry_2017.csv\")\n",
    "    df['labels'] = df['Finding Labels'].apply(lambda x: x.split('|'))\n",
    "\n",
    "    for label in Constants.take_labels:\n",
    "        df[label] = df['labels'].apply(lambda x: label in x)\n",
    "    df.reset_index(drop = True).to_csv(out_folder/\"preprocessed.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Validating paths...\")\n",
    "    validate_all()\n",
    "    print(\"Preprocessing MIMIC-CXR...\")\n",
    "    preprocess_mimic()\n",
    "    print(\"Preprocessing CheXpert...\")\n",
    "    preprocess_cxp()\n",
    "    print(\"Preprocessing ChestX-ray8...\")\n",
    "    preprocess_nih()\n",
    "    print(\"Preprocessing PadChest... This might take a few minutes...\")\n",
    "    preprocess_pad()\n",
    "    print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we need to resize and process the data.\n",
    "I am using the code provided by the authors of the article to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_MIMIC(split, only_frontal):  \n",
    "    copy_subjectid = split['subject_id']     \n",
    "    split = split.drop(columns = ['subject_id']).replace(\n",
    "            [[None], -1, \"[False]\", \"[True]\", \"[ True]\", 'UNABLE TO OBTAIN', 'UNKNOWN', 'MARRIED', 'LIFE PARTNER',\n",
    "             'DIVORCED', 'SEPARATED', '0-10', '10-20', '20-30', '30-40', '40-50', '50-60', '60-70', '70-80', '80-90',\n",
    "             '>=90'],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 'MARRIED/LIFE PARTNER', 'MARRIED/LIFE PARTNER', 'DIVORCED/SEPARATED',\n",
    "             'DIVORCED/SEPARATED', '0-20', '0-20', '20-40', '20-40', '40-60', '40-60', '60-80', '60-80', '80-', '80-'])\n",
    "    \n",
    "    split['subject_id'] = copy_subjectid.astype(str)\n",
    "    split['study_id'] = split['study_id'].astype(str)\n",
    "    split['Age'] = split[\"age_decile\"]\n",
    "    split['Sex'] = split[\"gender\"]\n",
    "    split = split.rename(\n",
    "        columns = {\n",
    "            'Pleural Effusion':'Effusion',   \n",
    "        })\n",
    "    split['path'] = split['path'].astype(str).apply(lambda x: os.path.join(Constants.image_paths['MIMIC'], x))\n",
    "    if only_frontal:\n",
    "        split = split[split.frontal]\n",
    "        \n",
    "    split['env'] = 'MIMIC'  \n",
    "    split.loc[split.Age == 0, 'Age'] = '0-20'\n",
    "    \n",
    "    return split[['subject_id','path','Sex',\"Age\", 'env', 'frontal', 'study_id'] + Constants.take_labels]\n",
    "\n",
    "def process_NIH(split, only_frontal = True):\n",
    "    split['Patient Age'] = np.where(split['Patient Age'].between(0,19), 19, split['Patient Age'])\n",
    "    split['Patient Age'] = np.where(split['Patient Age'].between(20,39), 39, split['Patient Age'])\n",
    "    split['Patient Age'] = np.where(split['Patient Age'].between(40,59), 59, split['Patient Age'])\n",
    "    split['Patient Age'] = np.where(split['Patient Age'].between(60,79), 79, split['Patient Age'])\n",
    "    split['Patient Age'] = np.where(split['Patient Age']>=80, 81, split['Patient Age'])\n",
    "    \n",
    "    copy_subjectid = split['Patient ID'] \n",
    "    \n",
    "    split = split.drop(columns = ['Patient ID']).replace([[None], -1, \"[False]\", \"[True]\", \"[ True]\", 19, 39, 59, 79, 81], \n",
    "                            [0, 0, 0, 1, 1, \"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-\"])\n",
    "   \n",
    "    split['subject_id'] = copy_subjectid.astype(str)\n",
    "    split['Sex'] = split['Patient Gender'] \n",
    "    split['Age'] = split['Patient Age']\n",
    "    split = split.drop(columns=[\"Patient Gender\", 'Patient Age'])\n",
    "    split['path'] = split['Image Index'].astype(str).apply(lambda x: os.path.join(Constants.image_paths['NIH'], 'images', x))\n",
    "    split['env'] = 'NIH'\n",
    "    split['frontal'] = True\n",
    "    split['study_id'] = split['subject_id'].astype(str)\n",
    "    return split[['subject_id','path','Sex',\"Age\", 'env', 'frontal','study_id'] + Constants.take_labels]\n",
    "\n",
    "\n",
    "def process_CXP(split, only_frontal):\n",
    "    split['Age'] = np.where(split['Age'].between(0,19), 19, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(20,39), 39, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(40,59), 59, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(60,79), 79, split['Age'])\n",
    "    split['Age'] = np.where(split['Age']>=80, 81, split['Age'])\n",
    "    \n",
    "    copy_subjectid = split['subject_id'] \n",
    "    split = split.drop(columns = ['subject_id']).replace([[None], -1, \"[False]\", \"[True]\", \"[ True]\", 19, 39, 59, 79, 81], \n",
    "                            [0, 0, 0, 1, 1, \"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-\"])\n",
    "    \n",
    "    split['subject_id'] = copy_subjectid.astype(str)\n",
    "    split['Sex'] = np.where(split['Sex']=='Female', 'F', split['Sex'])\n",
    "    split['Sex'] = np.where(split['Sex']=='Male', 'M', split['Sex'])\n",
    "    split = split.rename(\n",
    "        columns = {\n",
    "            'Pleural Effusion':'Effusion',\n",
    "            'Lung Opacity': 'Airspace Opacity'        \n",
    "        })\n",
    "    split['path'] = split['Path'].astype(str).apply(lambda x: os.path.join(Constants.image_paths['CXP'], x))\n",
    "    split['frontal'] = (split['Frontal/Lateral'] == 'Frontal')\n",
    "    if only_frontal:\n",
    "        split = split[split['frontal']]\n",
    "    split['env'] = 'CXP'\n",
    "    split['study_id'] = split['path'].apply(lambda x: x[x.index('patient'):x.rindex('/')])\n",
    "    return split[['subject_id','path','Sex',\"Age\", 'env', 'frontal','study_id'] + Constants.take_labels]\n",
    "\n",
    "\n",
    "def process_PAD(split, only_frontal):\n",
    "    split['Age'] = np.where(split['Age'].between(0,19), 19, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(20,39), 39, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(40,59), 59, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(60,79), 79, split['Age'])\n",
    "    split['Age'] = np.where(split['Age']>=80, 81, split['Age'])\n",
    "    \n",
    "    split = split.replace([[None], -1, \"[False]\", \"[True]\", \"[ True]\", 19, 39, 59, 79, 81], \n",
    "                            [0, 0, 0, 1, 1, \"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-\"])\n",
    "    \n",
    "    split.loc[split['Age'] == 0.0, 'Age'] = '0-20'\n",
    "    split.loc[split['Age'].isnull(), 'Age'] = '0-20'\n",
    "    split = split.rename(columns = {\n",
    "        'PatientID': 'subject_id',\n",
    "        'StudyID': 'study_id',\n",
    "        'PatientSex_DICOM' :'Sex'        \n",
    "    })\n",
    "    \n",
    "    split.loc[~split['Sex'].isin(['M', 'F', 'O']), 'Sex'] = 'O'\n",
    "    split['path'] =  split['ImageID'].astype(str).apply(lambda x: os.path.join(Constants.image_paths['PAD'], x))\n",
    "    if only_frontal:\n",
    "        split = split[split['frontal']]\n",
    "    split['env'] = 'PAD'\n",
    "    return split[['subject_id','path','Sex',\"Age\", 'env', 'frontal','study_id'] + Constants.take_labels]\n",
    "\n",
    "\n",
    "def split(df, split_portions = (0.8, 0.9), seed=0):\n",
    "    # We don't want the data splits to be affected by seed\n",
    "    # So lets temporarily set the seed to a static value...\n",
    "    rand_state = np.random.get_state()\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Split our data (irrespective of the random seed provided in train.py)\n",
    "    subject_df = pd.DataFrame({'subject_id': np.sort(df['subject_id'].unique())})\n",
    "    subject_df['random_number'] = np.random.uniform(size=len(subject_df))\n",
    "\n",
    "    train_id = subject_df[subject_df['random_number'] <= split_portions[0]].drop(columns=['random_number'])\n",
    "    valid_id = subject_df[(subject_df['random_number'] > split_portions[0]) & (subject_df['random_number'] <= split_portions[1])].drop(columns=['random_number'])\n",
    "    test_id = subject_df[subject_df['random_number'] > split_portions[1]].drop(columns=['random_number'])\n",
    "\n",
    "    train_df = df[df.subject_id.isin(train_id.subject_id)]\n",
    "    valid_df = df[df.subject_id.isin(valid_id.subject_id)]\n",
    "    test_df = df[df.subject_id.isin(test_id.subject_id)]  \n",
    "\n",
    "    # ...then return the random state back to what it was\n",
    "    np.random.set_state(rand_state)\n",
    "\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "def get_process_func(env):\n",
    "    if env == 'MIMIC':\n",
    "        return process_MIMIC\n",
    "    elif env == 'NIH':\n",
    "        return process_NIH\n",
    "    elif env == 'CXP':\n",
    "        return process_CXP\n",
    "    elif env == 'PAD':\n",
    "        return process_PAD\n",
    "    else:\n",
    "        raise NotImplementedError   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show data paths from constants\n",
    "Constants.df_paths\n",
    "\n",
    "def img_exists(path):\n",
    "    return exists(path)\n",
    "\n",
    "def is_diseased(row):\n",
    "    # diseases = Constants.take_labels[1:]\n",
    "    return int((row[Constants.take_labels[1:]]).sum() > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cell is pre-processing the data and will take a long time to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below needs to run once, after that everything is saved into the CSV file and can be loaded from there. this block of code needs to re-run only if the data changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This might take a while.\n",
      "Processing: MIMIC\n",
      "Got processing function, filtering by only frontal...\n",
      "Filtering out the data without images...\n",
      "False    167664\n",
      "True      63047\n",
      "Name: img_exists, dtype: int64\n",
      "Adding \"All\" column...\n",
      "Saving results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>env</th>\n",
       "      <th>frontal</th>\n",
       "      <th>study_id</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>img_exists</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>50414267</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>53189527</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000032</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>53911762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10000032</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>53911762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10000032</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>56699142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97453</th>\n",
       "      <td>12742782</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>54917116</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97456</th>\n",
       "      <td>12742898</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>20-40</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>53339588</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97461</th>\n",
       "      <td>12743572</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>51989892</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97463</th>\n",
       "      <td>12743572</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>52648347</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97467</th>\n",
       "      <td>12743572</td>\n",
       "      <td>/Volumes/Passport-2TB 2/data/mimic/physionet.o...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>52929291</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63047 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      subject_id                                               path Sex  \\\n",
       "0       10000032  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "2       10000032  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "4       10000032  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "5       10000032  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "6       10000032  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "...          ...                                                ...  ..   \n",
       "97453   12742782  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "97456   12742898  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "97461   12743572  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "97463   12743572  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "97467   12743572  /Volumes/Passport-2TB 2/data/mimic/physionet.o...   F   \n",
       "\n",
       "         Age    env  frontal  study_id  No Finding  Atelectasis  Cardiomegaly  \\\n",
       "0      40-60  MIMIC     True  50414267         1.0          0.0           0.0   \n",
       "2      40-60  MIMIC     True  53189527         1.0          0.0           0.0   \n",
       "4      40-60  MIMIC     True  53911762         1.0          0.0           0.0   \n",
       "5      40-60  MIMIC     True  53911762         1.0          0.0           0.0   \n",
       "6      40-60  MIMIC     True  56699142         1.0          0.0           0.0   \n",
       "...      ...    ...      ...       ...         ...          ...           ...   \n",
       "97453  40-60  MIMIC     True  54917116         1.0          0.0           0.0   \n",
       "97456  20-40  MIMIC     True  53339588         0.0          0.0           0.0   \n",
       "97461  60-80  MIMIC     True  51989892         1.0          0.0           0.0   \n",
       "97463  60-80  MIMIC     True  52648347         1.0          0.0           0.0   \n",
       "97467  60-80  MIMIC     True  52929291         1.0          0.0           0.0   \n",
       "\n",
       "       Effusion  Pneumonia  Pneumothorax  Consolidation  Edema  img_exists  \\\n",
       "0           0.0        0.0           0.0            0.0    0.0        True   \n",
       "2           0.0        0.0           0.0            0.0    0.0        True   \n",
       "4           0.0        0.0           0.0            0.0    0.0        True   \n",
       "5           0.0        0.0           0.0            0.0    0.0        True   \n",
       "6           0.0        0.0           0.0            0.0    0.0        True   \n",
       "...         ...        ...           ...            ...    ...         ...   \n",
       "97453       0.0        0.0           0.0            0.0    0.0        True   \n",
       "97456       0.0        1.0           0.0            0.0    0.0        True   \n",
       "97461       0.0        0.0           0.0            0.0    0.0        True   \n",
       "97463       0.0        0.0           0.0            0.0    0.0        True   \n",
       "97467       0.0        0.0           0.0            0.0    0.0        True   \n",
       "\n",
       "       All  \n",
       "0        0  \n",
       "2        0  \n",
       "4        0  \n",
       "5        0  \n",
       "6        0  \n",
       "...    ...  \n",
       "97453    0  \n",
       "97456    1  \n",
       "97461    0  \n",
       "97463    0  \n",
       "97467    0  \n",
       "\n",
       "[63047 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: CXP\n",
      "Got processing function, filtering by only frontal...\n",
      "Filtering out the data without images...\n",
      "True    191229\n",
      "Name: img_exists, dtype: int64\n",
      "Adding \"All\" column...\n",
      "Saving results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>env</th>\n",
       "      <th>frontal</th>\n",
       "      <th>study_id</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>img_exists</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00001/study1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00002/study2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00002/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00003/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>20-40</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00004/study1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223643</th>\n",
       "      <td>64736</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64736/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223644</th>\n",
       "      <td>64737</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64737/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223645</th>\n",
       "      <td>64738</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64738/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223646</th>\n",
       "      <td>64739</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64739/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223647</th>\n",
       "      <td>64740</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>M</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64740/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191229 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id                                               path Sex  \\\n",
       "0               1  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "1               2  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "2               2  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "4               3  /Users/noemi/ood-generalization/data/CheXpert/...   M   \n",
       "5               4  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "...           ...                                                ...  ..   \n",
       "223643      64736  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "223644      64737  /Users/noemi/ood-generalization/data/CheXpert/...   M   \n",
       "223645      64738  /Users/noemi/ood-generalization/data/CheXpert/...   M   \n",
       "223646      64739  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "223647      64740  /Users/noemi/ood-generalization/data/CheXpert/...   M   \n",
       "\n",
       "          Age  env  frontal             study_id  No Finding  Atelectasis  \\\n",
       "0       60-80  CXP     True  patient00001/study1         1.0          0.0   \n",
       "1         80-  CXP     True  patient00002/study2         0.0          0.0   \n",
       "2         80-  CXP     True  patient00002/study1         0.0          0.0   \n",
       "4       40-60  CXP     True  patient00003/study1         0.0          0.0   \n",
       "5       20-40  CXP     True  patient00004/study1         1.0          0.0   \n",
       "...       ...  ...      ...                  ...         ...          ...   \n",
       "223643  40-60  CXP     True  patient64736/study1         0.0          0.0   \n",
       "223644  60-80  CXP     True  patient64737/study1         0.0          0.0   \n",
       "223645  60-80  CXP     True  patient64738/study1         0.0          0.0   \n",
       "223646  40-60  CXP     True  patient64739/study1         0.0          0.0   \n",
       "223647    80-  CXP     True  patient64740/study1         0.0          1.0   \n",
       "\n",
       "        Cardiomegaly  Effusion  Pneumonia  Pneumothorax  Consolidation  Edema  \\\n",
       "0                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "1                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "2                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "4                0.0       0.0        0.0           0.0            0.0    1.0   \n",
       "5                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "...              ...       ...        ...           ...            ...    ...   \n",
       "223643           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "223644           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "223645           1.0       0.0        0.0           0.0            0.0    1.0   \n",
       "223646           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "223647           0.0       1.0        0.0           0.0            0.0    0.0   \n",
       "\n",
       "        img_exists  All  \n",
       "0             True    0  \n",
       "1             True    0  \n",
       "2             True    0  \n",
       "4             True    1  \n",
       "5             True    0  \n",
       "...            ...  ...  \n",
       "223643        True    0  \n",
       "223644        True    0  \n",
       "223645        True    1  \n",
       "223646        True    0  \n",
       "223647        True    1  \n",
       "\n",
       "[191229 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: NIH\n",
      "Got processing function, filtering by only frontal...\n",
      "Filtering out the data without images...\n",
      "True    112120\n",
      "Name: img_exists, dtype: int64\n",
      "Adding \"All\" column...\n",
      "Saving results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>env</th>\n",
       "      <th>frontal</th>\n",
       "      <th>study_id</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>img_exists</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>M</td>\n",
       "      <td>80-</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112115</th>\n",
       "      <td>30801</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>M</td>\n",
       "      <td>20-40</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>30801</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112116</th>\n",
       "      <td>30802</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>M</td>\n",
       "      <td>20-40</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>30802</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112117</th>\n",
       "      <td>30803</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>30803</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112118</th>\n",
       "      <td>30804</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>F</td>\n",
       "      <td>20-40</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>30804</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112119</th>\n",
       "      <td>30805</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/chestxray...</td>\n",
       "      <td>M</td>\n",
       "      <td>20-40</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>30805</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112120 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id                                               path Sex  \\\n",
       "0               1  /Users/noemi/ood-generalization/data/chestxray...   M   \n",
       "1               1  /Users/noemi/ood-generalization/data/chestxray...   M   \n",
       "2               1  /Users/noemi/ood-generalization/data/chestxray...   M   \n",
       "3               2  /Users/noemi/ood-generalization/data/chestxray...   M   \n",
       "4               3  /Users/noemi/ood-generalization/data/chestxray...   F   \n",
       "...           ...                                                ...  ..   \n",
       "112115      30801  /Users/noemi/ood-generalization/data/chestxray...   M   \n",
       "112116      30802  /Users/noemi/ood-generalization/data/chestxray...   M   \n",
       "112117      30803  /Users/noemi/ood-generalization/data/chestxray...   F   \n",
       "112118      30804  /Users/noemi/ood-generalization/data/chestxray...   F   \n",
       "112119      30805  /Users/noemi/ood-generalization/data/chestxray...   M   \n",
       "\n",
       "          Age  env  frontal study_id  No Finding  Atelectasis  Cardiomegaly  \\\n",
       "0       40-60  NIH     True        1       False        False          True   \n",
       "1       40-60  NIH     True        1       False        False          True   \n",
       "2       40-60  NIH     True        1       False        False          True   \n",
       "3         80-  NIH     True        2        True        False         False   \n",
       "4       60-80  NIH     True        3       False        False         False   \n",
       "...       ...  ...      ...      ...         ...          ...           ...   \n",
       "112115  20-40  NIH     True    30801       False        False         False   \n",
       "112116  20-40  NIH     True    30802        True        False         False   \n",
       "112117  40-60  NIH     True    30803        True        False         False   \n",
       "112118  20-40  NIH     True    30804        True        False         False   \n",
       "112119  20-40  NIH     True    30805        True        False         False   \n",
       "\n",
       "        Effusion  Pneumonia  Pneumothorax  Consolidation  Edema  img_exists  \\\n",
       "0          False      False         False          False  False        True   \n",
       "1          False      False         False          False  False        True   \n",
       "2           True      False         False          False  False        True   \n",
       "3          False      False         False          False  False        True   \n",
       "4          False      False         False          False  False        True   \n",
       "...          ...        ...           ...            ...    ...         ...   \n",
       "112115     False       True         False          False  False        True   \n",
       "112116     False      False         False          False  False        True   \n",
       "112117     False      False         False          False  False        True   \n",
       "112118     False      False         False          False  False        True   \n",
       "112119     False      False         False          False  False        True   \n",
       "\n",
       "        All  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "112115    1  \n",
       "112116    0  \n",
       "112117    0  \n",
       "112118    0  \n",
       "112119    0  \n",
       "\n",
       "[112120 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: PAD\n",
      "Got processing function, filtering by only frontal...\n",
      "Filtering out the data without images...\n",
      "True    99827\n",
      "Name: img_exists, dtype: int64\n",
      "Adding \"All\" column...\n",
      "Saving results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>env</th>\n",
       "      <th>frontal</th>\n",
       "      <th>study_id</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>img_exists</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>839860488694292331637988235681460987</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>20536686640136348236148679891455886468</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>313572750430997347502932654319389875966</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>M</td>\n",
       "      <td>80-</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>135803415504923515076821959678074435083</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50783093527901818115346441867348318648</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>113855343774216031107737439268243531979</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>93535126770783451980359712286922420997</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>3137231742710829928-247610802266403640553</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93535126770783451980359712286922420997</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>313723174271082992847610802266403640553</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144479</th>\n",
       "      <td>112930952416074060371371014599496493673</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>1284011361929414522814654121696751542351444145...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144480</th>\n",
       "      <td>282743729971423358706056731890510600934</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>1284011361929414522094646571696751542351444145...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144481</th>\n",
       "      <td>52648743308541843883453242716226652771</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>1284011361929414522086390631696751542351444145...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144482</th>\n",
       "      <td>228646130593152933811948996634154201216</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>1284011361929414522084108901696751542351444145...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144483</th>\n",
       "      <td>137424047230303610602080410284588825286</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/PadChest/...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>1284011361929414524682119191696751542351444145...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99827 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     subject_id  \\\n",
       "0          839860488694292331637988235681460987   \n",
       "2       313572750430997347502932654319389875966   \n",
       "3        50783093527901818115346441867348318648   \n",
       "6        93535126770783451980359712286922420997   \n",
       "7        93535126770783451980359712286922420997   \n",
       "...                                         ...   \n",
       "144479  112930952416074060371371014599496493673   \n",
       "144480  282743729971423358706056731890510600934   \n",
       "144481   52648743308541843883453242716226652771   \n",
       "144482  228646130593152933811948996634154201216   \n",
       "144483  137424047230303610602080410284588825286   \n",
       "\n",
       "                                                     path Sex    Age  env  \\\n",
       "0       /Users/noemi/ood-generalization/data/PadChest/...   F    80-  PAD   \n",
       "2       /Users/noemi/ood-generalization/data/PadChest/...   M    80-  PAD   \n",
       "3       /Users/noemi/ood-generalization/data/PadChest/...   F    80-  PAD   \n",
       "6       /Users/noemi/ood-generalization/data/PadChest/...   M  60-80  PAD   \n",
       "7       /Users/noemi/ood-generalization/data/PadChest/...   M  60-80  PAD   \n",
       "...                                                   ...  ..    ...  ...   \n",
       "144479  /Users/noemi/ood-generalization/data/PadChest/...   M  60-80  PAD   \n",
       "144480  /Users/noemi/ood-generalization/data/PadChest/...   F  60-80  PAD   \n",
       "144481  /Users/noemi/ood-generalization/data/PadChest/...   M  40-60  PAD   \n",
       "144482  /Users/noemi/ood-generalization/data/PadChest/...   F  60-80  PAD   \n",
       "144483  /Users/noemi/ood-generalization/data/PadChest/...   M  60-80  PAD   \n",
       "\n",
       "        frontal                                           study_id  \\\n",
       "0          True             20536686640136348236148679891455886468   \n",
       "2          True            135803415504923515076821959678074435083   \n",
       "3          True            113855343774216031107737439268243531979   \n",
       "6          True          3137231742710829928-247610802266403640553   \n",
       "7          True            313723174271082992847610802266403640553   \n",
       "...         ...                                                ...   \n",
       "144479     True  1284011361929414522814654121696751542351444145...   \n",
       "144480     True  1284011361929414522094646571696751542351444145...   \n",
       "144481     True  1284011361929414522086390631696751542351444145...   \n",
       "144482     True  1284011361929414522084108901696751542351444145...   \n",
       "144483     True  1284011361929414524682119191696751542351444145...   \n",
       "\n",
       "        No Finding  Atelectasis  Cardiomegaly  Effusion  Pneumonia  \\\n",
       "0                1            0             0         0          0   \n",
       "2                0            0             0         0          0   \n",
       "3                0            0             0         0          0   \n",
       "6                0            1             0         1          0   \n",
       "7                0            0             0         1          0   \n",
       "...            ...          ...           ...       ...        ...   \n",
       "144479           0            0             0         0          0   \n",
       "144480           1            0             0         0          0   \n",
       "144481           0            0             0         0          0   \n",
       "144482           1            0             0         0          0   \n",
       "144483           0            0             0         0          1   \n",
       "\n",
       "        Pneumothorax  Consolidation  Edema  img_exists  All  \n",
       "0                  0              0      0        True    0  \n",
       "2                  0              0      0        True    0  \n",
       "3                  0              0      0        True    0  \n",
       "6                  0              0      0        True    1  \n",
       "7                  0              0      0        True    1  \n",
       "...              ...            ...    ...         ...  ...  \n",
       "144479             0              0      0        True    0  \n",
       "144480             0              0      0        True    0  \n",
       "144481             0              0      0        True    0  \n",
       "144482             0              0      0        True    0  \n",
       "144483             0              0      0        True    1  \n",
       "\n",
       "[99827 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# loads data with random splits\n",
    "print('This might take a while.')\n",
    "\n",
    "for data_env in Constants.df_paths:\n",
    "    print('Processing:', data_env)\n",
    "    func = get_process_func(data_env)\n",
    "    print('Got processing function, filtering by only frontal...')\n",
    "    df_env = func(pd.read_csv(Constants.df_paths[data_env]), only_frontal = True)\n",
    "    print('Filtering out the data without images...')\n",
    "    df_env[\"img_exists\"] = df_env[\"path\"].apply(img_exists)\n",
    "    print(df_env[\"img_exists\"].value_counts())\n",
    "    df_env = df_env[df_env[\"img_exists\"]]\n",
    "    \n",
    "    df_env = df_env.fillna(0)\n",
    "    \n",
    "    print('Adding \"All\" column...')\n",
    "    df_env[\"All\"] = df_env.apply(is_diseased, axis=1)\n",
    "    \n",
    "    print('Saving results...')\n",
    "    df_env.to_csv(f\"{Constants.base_path}/processed/{data_env}.csv\", index=False)\n",
    "    \n",
    "    display(df_env)\n",
    "    \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the data, splitting to all, train, val and test...\n",
      "Source: MIMIC\n",
      "Data length: 63047\n",
      "MIMIC: done.\n",
      "Source: CXP\n",
      "Data length: 191229\n",
      "CXP: done.\n",
      "Source: NIH\n",
      "Data length: 112120\n",
      "NIH: done.\n",
      "Source: PAD\n",
      "Data length: 99827\n",
      "PAD: done.\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "dfs = {}\n",
    "print('Processing the data, splitting to all, train, val and test...')\n",
    "for env in Constants.df_paths:\n",
    "    func = get_process_func(env)\n",
    "    df_env = pd.read_csv(f\"{Constants.base_path}/processed/{env}.csv\")\n",
    "    \n",
    "    print('Source:', env)\n",
    "    print('Data length:', len(df_env))\n",
    "    \n",
    "    train_df, valid_df, test_df = split(df_env)\n",
    "    dfs[env] = {\n",
    "        'all': df_env,\n",
    "        'train': train_df,\n",
    "        'val': valid_df,\n",
    "        'test': test_df\n",
    "    }\n",
    "    print(f'{env}: done.')\n",
    "    \n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prop(df, column=\"Pneumonia\"):\n",
    "    num_instances = len(df)\n",
    "    num_diseased = df[df[column] == 1][column].count()\n",
    "    return num_diseased / (num_instances - num_diseased)\n",
    "\n",
    "def get_resample_class(orig_prop, new_prop, resample_method):\n",
    "    if new_prop > orig_prop:\n",
    "        if resample_method == \"over\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    if new_prop < orig_prop:\n",
    "        if resample_method == \"under\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "def calculate_num_resample(df, orig_prop, new_prop, resample_method):\n",
    "    pass\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def balance_df_label(df, sampler, label_bal=0.05154780337262089, invert=False):\n",
    "    target = df[\"Pneumonia\"] == 1\n",
    "    rus = sampler(random_state=0, sampling_strategy=label_bal if not invert else 1-label_bal - 0.23)\n",
    "    res_df, _ = rus.fit_resample(df, target)\n",
    "\n",
    "    print(f\"Previous pneumonia prop: {get_pneumonia_prop(df)} with {len(df)} instances\")\n",
    "    print(f\"Resampled pneumonia prop: {get_pneumonia_prop(res_df)} with {len(res_df)} instances\")\n",
    "\n",
    "    return res_df\n",
    "\n",
    "def balance_proportion(orig_df, new_df, resample_method=\"over\", column=\"Pneumonia\"):\n",
    "    orig_df = orig_df.fillna(0.0)\n",
    "    orig_prop = get_prop(orig_df, column)\n",
    "    new_prop = get_prop(new_df, column)\n",
    "    assert resample_method in [\"over\", \"under\"]\n",
    "    resample_class = get_resample_class(orig_prop, new_prop, resample_method)\n",
    "    print(f\"Resampling '{column}' via '{resample_method}' on class {resample_class} from {orig_prop} to {new_prop}\")\n",
    "    \n",
    "    # Estimate the number of items we'll need to resample\n",
    "    df_diseased = orig_df[orig_df[column] == 1.0]\n",
    "    df_normal = orig_df[orig_df[column] == 0.0]\n",
    "    num_diseased = len(df_diseased)\n",
    "    num_normal = len(df_normal)\n",
    "    assert num_diseased + num_normal == len(orig_df)\n",
    "    \n",
    "    if resample_method == \"over\":\n",
    "        if resample_class == 0:\n",
    "            new_num_normal = int(num_diseased / new_prop)\n",
    "            print(f\"Resampling normal samples from {num_normal} to {new_num_normal}\")\n",
    "            df_normal_rs = df_normal.sample(new_num_normal, replace=True, random_state=0)\n",
    "            resampled_df = pd.concat([df_normal_rs, df_diseased])\n",
    "        else:\n",
    "            # Resample the pneumonia class\n",
    "            # new_num_diseased = int(new_prop * num_normal)\n",
    "            # print(f\"Resampling diseased samples from {num_diseased} to {new_num_diseased}\")\n",
    "            # df_diseased_rs = df_diseased.sample(new_num_diseased, replace=True, random_state=0)\n",
    "            # resampled_df = pd.concat([df_normal, df_diseased_rs])\n",
    "            target = df[\"Pneumonia\"] == 1\n",
    "            rus = RandomOverSampler(random_state=0, sampling_strategy=new_prop)\n",
    "            resampled_df, _ = rus.fit_resample(df, target)\n",
    "    \n",
    "    resampled_df.sort_index(inplace=True)\n",
    "    print(f\"New df proportion: {get_prop(resampled_df, column)}\")\n",
    "    return resampled_df\n",
    "            \n",
    "# balance_proportion(dfs[\"MIMIC\"][\"train\"], dfs[\"MIMIC\"][\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>env</th>\n",
       "      <th>frontal</th>\n",
       "      <th>study_id</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>img_exists</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00001/study1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00002/study2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00002/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00003/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>20-40</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00004/study1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191222</th>\n",
       "      <td>64734</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64734/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191223</th>\n",
       "      <td>64735</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64735/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191225</th>\n",
       "      <td>64737</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64737/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191227</th>\n",
       "      <td>64739</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64739/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191228</th>\n",
       "      <td>64740</td>\n",
       "      <td>/Users/noemi/ood-generalization/data/CheXpert/...</td>\n",
       "      <td>M</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64740/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153411 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        subject_id                                               path Sex  \\\n",
       "0                1  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "1                2  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "2                2  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "3                3  /Users/noemi/ood-generalization/data/CheXpert/...   M   \n",
       "4                4  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "...            ...                                                ...  ..   \n",
       "191222       64734  /Users/noemi/ood-generalization/data/CheXpert/...   M   \n",
       "191223       64735  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "191225       64737  /Users/noemi/ood-generalization/data/CheXpert/...   M   \n",
       "191227       64739  /Users/noemi/ood-generalization/data/CheXpert/...   F   \n",
       "191228       64740  /Users/noemi/ood-generalization/data/CheXpert/...   M   \n",
       "\n",
       "          Age  env  frontal             study_id  No Finding  Atelectasis  \\\n",
       "0       60-80  CXP     True  patient00001/study1         1.0          0.0   \n",
       "1         80-  CXP     True  patient00002/study2         0.0          0.0   \n",
       "2         80-  CXP     True  patient00002/study1         0.0          0.0   \n",
       "3       40-60  CXP     True  patient00003/study1         0.0          0.0   \n",
       "4       20-40  CXP     True  patient00004/study1         1.0          0.0   \n",
       "...       ...  ...      ...                  ...         ...          ...   \n",
       "191222  40-60  CXP     True  patient64734/study1         0.0          1.0   \n",
       "191223  60-80  CXP     True  patient64735/study1         0.0          1.0   \n",
       "191225  60-80  CXP     True  patient64737/study1         0.0          0.0   \n",
       "191227  40-60  CXP     True  patient64739/study1         0.0          0.0   \n",
       "191228    80-  CXP     True  patient64740/study1         0.0          1.0   \n",
       "\n",
       "        Cardiomegaly  Effusion  Pneumonia  Pneumothorax  Consolidation  Edema  \\\n",
       "0                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "1                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "2                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "3                0.0       0.0        0.0           0.0            0.0    1.0   \n",
       "4                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "...              ...       ...        ...           ...            ...    ...   \n",
       "191222           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "191223           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "191225           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "191227           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "191228           0.0       1.0        0.0           0.0            0.0    0.0   \n",
       "\n",
       "        img_exists  All  \n",
       "0             True    0  \n",
       "1             True    0  \n",
       "2             True    0  \n",
       "3             True    1  \n",
       "4             True    0  \n",
       "...            ...  ...  \n",
       "191222        True    1  \n",
       "191223        True    1  \n",
       "191225        True    0  \n",
       "191227        True    0  \n",
       "191228        True    1  \n",
       "\n",
       "[153411 rows x 17 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"CXP\"][\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjW9bCkouv8O"
   },
   "source": [
    "### Metrics to evaluate my model\n",
    "\n",
    "Similar to the original paper, for each base hospital I plan to choose one additional hospital to include in evaluation (for example, evaluate a model trained on MIMIC data using MIMIC and PAD data).\n",
    "\n",
    "* analyse accuracies within each class for each hospital - the result is a group for the disease class from hospital A, the non-disease class from hospital A, the disease class from hospital B, and the non-disease class from hospital B\n",
    "* Track the worst accuracy of the four groups\n",
    "* Compute AUROC\n",
    "\n",
    "I plan to plot the results and compare them to the results provided in the paper.\n",
    "\n",
    "Since I wasn't yet able to fully complete the previous steps, and instead am stuck on the training portion, this section is a ToDo. I plan to complete it by week of April 21.\n",
    "\n",
    "There are two alternative approaches I can take, depending on the situation:\n",
    "\n",
    "* If I manage to run the original paper code, then this is what I will do, since it should be closest to the original paper\n",
    "* If I won't be able to run the original paper training and validation code on my machine, I will update the code I wrote for training and validation to take it as close as possible to the intent of the original researchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing...\n",
      "Previous pneumonia prop: 0.07020832445788779 with 50242 instances\n",
      "Resampled pneumonia prop: 0.7184424658117837 with 80674 instances\n",
      "Previous pneumonia prop: 0.02491281516815649 with 153411 instances\n",
      "Resampled pneumonia prop: 0.051542603653077855 with 157397 instances\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def balance_df_label(df, sampler, label_bal=0.05154780337262089, invert=False):\n",
    "    target = df[\"Pneumonia\"] == (1 if not invert else 0)\n",
    "    rus = sampler(random_state=42, sampling_strategy=label_bal if not invert else 1-label_bal - 0.23)\n",
    "    res_df, _ = rus.fit_resample(df, target)\n",
    "\n",
    "    print(f\"Previous pneumonia prop: {get_prop(df)} with {len(df)} instances\")\n",
    "    print(f\"Resampled pneumonia prop: {get_prop(res_df)} with {len(res_df)} instances\")\n",
    "\n",
    "    return res_df\n",
    "\n",
    "print('Balancing...')\n",
    "mimic_balanced = balance_df_label(dfs[\"MIMIC\"][\"train\"], RandomOverSampler, invert=True)\n",
    "cxp_balanced = balance_df_label(dfs[\"CXP\"][\"train\"], RandomOverSampler, invert=False)\n",
    "print('Done.')\n",
    "\n",
    "# # Balance the size of the two datasets\n",
    "# n = len(cxp_balanced)\n",
    "# mimic_balanced = mimic_balanced.sample(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a5c8b_row0_col0, #T_a5c8b_row6_col2 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #ecf4fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row0_col1, #T_a5c8b_row0_col2, #T_a5c8b_row5_col0, #T_a5c8b_row5_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #f7fbff;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row0_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #deebf7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row1_col0 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #bad6eb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row1_col1 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #dae8f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row1_col2 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #f3f8fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row1_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #c7dcef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row2_col0, #T_a5c8b_row4_col2 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #d6e5f4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row2_col1, #T_a5c8b_row4_col0 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #a8cee4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row2_col2, #T_a5c8b_row6_col0 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #f5f9fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row2_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #f2f8fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row3_col0 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #8fc2de;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row3_col1 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #58a1cf;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a5c8b_row3_col2 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #d0e1f2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row3_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #d8e7f5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row4_col1 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #d1e2f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row4_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #dbe9f6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row5_col1 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #e3eef9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row5_col2, #T_a5c8b_row6_col1 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #eaf3fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row6_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #f1f7fd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row7_col0, #T_a5c8b_row7_col1, #T_a5c8b_row8_col2, #T_a5c8b_row8_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #08306b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a5c8b_row7_col2 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #68acd5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a5c8b_row7_col3 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #4090c5;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a5c8b_row8_col0 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #3c8cc3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a5c8b_row8_col1 {\n",
       "  background-color: lightblue;\n",
       "  background-color: #e4eff9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a5c8b_row9_col0, #T_a5c8b_row9_col1, #T_a5c8b_row9_col2, #T_a5c8b_row9_col3 {\n",
       "  background-color: none;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a5c8b_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Dataset</th>\n",
       "      <th class=\"col_heading level0 col0\" >MIMIC</th>\n",
       "      <th class=\"col_heading level0 col1\" >CXP</th>\n",
       "      <th class=\"col_heading level0 col2\" >NIH</th>\n",
       "      <th class=\"col_heading level0 col3\" >PAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row0\" class=\"row_heading level0 row0\" >Pneumonia</th>\n",
       "      <td id=\"T_a5c8b_row0_col0\" class=\"data row0 col0\" >6.60%</td>\n",
       "      <td id=\"T_a5c8b_row0_col1\" class=\"data row0 col1\" >2.45%</td>\n",
       "      <td id=\"T_a5c8b_row0_col2\" class=\"data row0 col2\" >1.28%</td>\n",
       "      <td id=\"T_a5c8b_row0_col3\" class=\"data row0 col3\" >4.90%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row1\" class=\"row_heading level0 row1\" >Cardiomegaly</th>\n",
       "      <td id=\"T_a5c8b_row1_col0\" class=\"data row1 col0\" >17.64%</td>\n",
       "      <td id=\"T_a5c8b_row1_col1\" class=\"data row1 col1\" >12.26%</td>\n",
       "      <td id=\"T_a5c8b_row1_col2\" class=\"data row1 col2\" >2.48%</td>\n",
       "      <td id=\"T_a5c8b_row1_col3\" class=\"data row1 col3\" >9.08%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row2\" class=\"row_heading level0 row2\" >Edema</th>\n",
       "      <td id=\"T_a5c8b_row2_col0\" class=\"data row2 col0\" >12.05%</td>\n",
       "      <td id=\"T_a5c8b_row2_col1\" class=\"data row2 col1\" >26.00%</td>\n",
       "      <td id=\"T_a5c8b_row2_col2\" class=\"data row2 col2\" >2.05%</td>\n",
       "      <td id=\"T_a5c8b_row2_col3\" class=\"data row2 col3\" >1.20%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row3\" class=\"row_heading level0 row3\" >Effusion</th>\n",
       "      <td id=\"T_a5c8b_row3_col0\" class=\"data row3 col0\" >23.46%</td>\n",
       "      <td id=\"T_a5c8b_row3_col1\" class=\"data row3 col1\" >40.25%</td>\n",
       "      <td id=\"T_a5c8b_row3_col2\" class=\"data row3 col2\" >11.88%</td>\n",
       "      <td id=\"T_a5c8b_row3_col3\" class=\"data row3 col3\" >6.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row4\" class=\"row_heading level0 row4\" >Atelectasis</th>\n",
       "      <td id=\"T_a5c8b_row4_col0\" class=\"data row4 col0\" >20.30%</td>\n",
       "      <td id=\"T_a5c8b_row4_col1\" class=\"data row4 col1\" >15.58%</td>\n",
       "      <td id=\"T_a5c8b_row4_col2\" class=\"data row4 col2\" >10.31%</td>\n",
       "      <td id=\"T_a5c8b_row4_col3\" class=\"data row4 col3\" >5.49%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row5\" class=\"row_heading level0 row5\" >Pneumothorax</th>\n",
       "      <td id=\"T_a5c8b_row5_col0\" class=\"data row5 col0\" >3.99%</td>\n",
       "      <td id=\"T_a5c8b_row5_col1\" class=\"data row5 col1\" >9.26%</td>\n",
       "      <td id=\"T_a5c8b_row5_col2\" class=\"data row5 col2\" >4.73%</td>\n",
       "      <td id=\"T_a5c8b_row5_col3\" class=\"data row5 col3\" >0.35%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row6\" class=\"row_heading level0 row6\" >Consolidation</th>\n",
       "      <td id=\"T_a5c8b_row6_col0\" class=\"data row6 col0\" >4.64%</td>\n",
       "      <td id=\"T_a5c8b_row6_col1\" class=\"data row6 col1\" >6.81%</td>\n",
       "      <td id=\"T_a5c8b_row6_col2\" class=\"data row6 col2\" >4.16%</td>\n",
       "      <td id=\"T_a5c8b_row6_col3\" class=\"data row6 col3\" >1.56%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row7\" class=\"row_heading level0 row7\" >Any</th>\n",
       "      <td id=\"T_a5c8b_row7_col0\" class=\"data row7 col0\" >51.21%</td>\n",
       "      <td id=\"T_a5c8b_row7_col1\" class=\"data row7 col1\" >70.35%</td>\n",
       "      <td id=\"T_a5c8b_row7_col2\" class=\"data row7 col2\" >28.02%</td>\n",
       "      <td id=\"T_a5c8b_row7_col3\" class=\"data row7 col3\" >23.01%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row8\" class=\"row_heading level0 row8\" >No Finding</th>\n",
       "      <td id=\"T_a5c8b_row8_col0\" class=\"data row8 col0\" >34.60%</td>\n",
       "      <td id=\"T_a5c8b_row8_col1\" class=\"data row8 col1\" >8.89%</td>\n",
       "      <td id=\"T_a5c8b_row8_col2\" class=\"data row8 col2\" >53.84%</td>\n",
       "      <td id=\"T_a5c8b_row8_col3\" class=\"data row8 col3\" >36.19%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a5c8b_level0_row9\" class=\"row_heading level0 row9\" >Num Instances</th>\n",
       "      <td id=\"T_a5c8b_row9_col0\" class=\"data row9 col0\" >63,047</td>\n",
       "      <td id=\"T_a5c8b_row9_col1\" class=\"data row9 col1\" >191,229</td>\n",
       "      <td id=\"T_a5c8b_row9_col2\" class=\"data row9 col2\" >112,120</td>\n",
       "      <td id=\"T_a5c8b_row9_col3\" class=\"data row9 col3\" >99,827</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fbfb3f13ca0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_rows = []\n",
    "num_instances = []\n",
    "\n",
    "disease_labels = [\"Pneumonia\", \"Cardiomegaly\", \"Edema\", \"Effusion\", \"Atelectasis\", \"Pneumothorax\", \"Consolidation\"]\n",
    "target_labels = disease_labels + [\"Any\", \"No Finding\"]\n",
    "all_labels = target_labels + [\"Num Instances\"]\n",
    "\n",
    "for env in dfs:\n",
    "    df = dfs[env]['all']\n",
    "    df['Any'] = (df[disease_labels] > 0).any(axis=1).astype(int)\n",
    "    totals = {}\n",
    "    totals['Dataset'] = env\n",
    "    totals['Num Instances'] = len(df)\n",
    "    num_instances.append(totals['Num Instances'])\n",
    "\n",
    "    for label in target_labels:\n",
    "        if label in df.columns:\n",
    "            totals[label] = df[label].sum() / len(df)\n",
    "        else:\n",
    "            totals[label] = 0.0\n",
    "\n",
    "    stat_rows.append(totals)\n",
    "\n",
    "stat_df = pd.DataFrame(stat_rows)\n",
    "stat_df.set_index('Dataset', inplace=True)\n",
    "\n",
    "ordered_cols = all_labels\n",
    "stat_df = stat_df[ordered_cols]\n",
    "\n",
    "transposed_stat_df = stat_df.T\n",
    "\n",
    "# styled_stat_df = stat_df.style.background_gradient(cmap='Blues', subset=target_labels)\\\n",
    "#     .format({label: \"{:.2%}\" for label in target_labels})\n",
    "\n",
    "styled_transposed_stat_df = transposed_stat_df.style.apply(\n",
    "    lambda x: [\"background-color: lightblue\" if x.name != 'Num Instances' else \"background-color: none\" for i in x],\n",
    "    axis=1\n",
    ").background_gradient(cmap='Blues', subset=pd.IndexSlice[target_labels, :])\n",
    "styled_transposed_stat_df = styled_transposed_stat_df.format(\"{:.2%}\", subset=pd.IndexSlice[target_labels, :])\n",
    "styled_transposed_stat_df = styled_transposed_stat_df.format(\"{:,.0f}\", subset=pd.IndexSlice['Num Instances', :])\n",
    "\n",
    "styled_transposed_stat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the table from the article for comparison:\n",
    "\n",
    "![Table 1](Table_1_article.png)\n",
    "\n",
    "Looks like the distribution of the labels in the original dataset, while not the same, still is close enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": [
    "##   Model\n",
    "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
    "\n",
    "### Model architecture\n",
    "In the article, the authors use the same model architecture as Zhang et al. (2021): a **DenseNet-121** network (Huang et al., 2017) **initialized with pre-trained weights from ImageNet** (Deng et al., 2009). The final layer is replaced with a **two-output linear layer** (for binary classification). For simplicity, the authors only consider binary disease classification.\n",
    "\n",
    "### Model Training\n",
    "For training the network, all images are resized to **224 × 224** and normalized to the ImageNet (Deng et al., 2009) mean and standard deviation.\n",
    "\n",
    "During training, the following image augmentations are applied:\n",
    "* random horizontal flip\n",
    "* random rotation up to 10 degrees\n",
    "* a crop of random size (75% - 100%) and aspect ratio (3/4 to 4/3)\n",
    "\n",
    "All runs use **Adam** with **lr = 1e-5** and **batch size = 128**, which was found to be a performant configuration in early tuning ((Zhang et al., 2021) use lr = 5e-4 and batch size = 32).\n",
    "\n",
    "_[This part I haven't implemented yet]_ Training runs for **a maximum of 20k steps**, with validation occurring every 500 steps and an early stopping patience of 10 validations.\n",
    "\n",
    "All test results are obtained using the optimal model found during training as measured by the highest validation macro-F1 score (following (Fiorillo et al., 2021; Berenguer et al., 2022)) as it gives a robust ranking of model performance under imbalanced labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details for model info:\n",
    "    \n",
    "  * Model architecture: layer number/size/type, activation function, etc\n",
    "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
    "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
    "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
    "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "gBdVZoTvsSFV"
   },
   "outputs": [],
   "source": [
    "# This is the model defined and provided by the autors of the article.\n",
    "# While they are using densenet 121 for the article, the provided model code includes other options.\n",
    "\n",
    "class EmbModel(nn.Module):\n",
    "    # I had to add the num_labels parameter to reduce the resulting response to the number of labels we use\n",
    "    def __init__(self, emb_type, feature_size_override, pretrain, concat_features = 0, num_labels = 8):\n",
    "        super().__init__()\n",
    "        self.emb_type = emb_type\n",
    "        self.pretrain = pretrain\n",
    "        self.concat_features = concat_features\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        assert emb_type in [\"densenet121\", \"densenet201\", \"resnet\"], f\"Invalid emb type: {emb_type}\"\n",
    "\n",
    "        if emb_type == 'densenet121':\n",
    "            model = models.densenet121()\n",
    "            self.encoder = nn.Sequential(*list(model.children())[:-1]) #https://discuss.pytorch.org/t/densenet-transfer-learning/7776/2\n",
    "            self.emb_dim = model.classifier.in_features\n",
    "        elif emb_type == 'densenet201':\n",
    "            model = models.densenet201()\n",
    "            self.encoder = nn.Sequential(*list(model.children())[:-1]) #https://discuss.pytorch.org/t/densenet-transfer-learning/7776/2\n",
    "            self.emb_dim = model.classifier.in_features\n",
    "        elif emb_type == 'resnet':\n",
    "            model = models.resnet50()\n",
    "            self.encoder = nn.Sequential(*list(model.children())[:-1])\n",
    "            self.emb_dim = list(model.children())[-1].in_features\n",
    "\n",
    "        print(\"\\nEmb Dim:\")\n",
    "        print(self.emb_dim)\n",
    "\n",
    "        if feature_size_override:\n",
    "            print(f\"Manually setting output dim to {feature_size_override}\")\n",
    "            self.emb_dim = feature_size_override\n",
    "            print(self.emb_dim)\n",
    "            \n",
    "        self.n_outputs = self.emb_dim + concat_features\n",
    "        self.final_layer = nn.Linear(self.n_outputs, self.num_labels)\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.final_layer.weight, mode='fan_out', nonlinearity='sigmoid')\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        if isinstance(inp, dict): # dict with image and additional feature(s) to concat to embedding\n",
    "            x = inp['img']\n",
    "            concat = inp['concat']\n",
    "            assert(concat.shape[-1] == self.concat_features)\n",
    "        else: # tensor image\n",
    "            assert(self.concat_features == 0)\n",
    "            x = inp\n",
    "        \n",
    "        x = self.encoder(x).squeeze(-1).squeeze(-1)\n",
    "        if \"densenet\" in self.emb_type:\n",
    "            x = F.relu(x)\n",
    "            x = F.avg_pool2d(x, kernel_size = 7).view(x.size(0), -1)\n",
    "        \n",
    "        if isinstance(inp, dict):\n",
    "            x = torch.cat([x, concat], dim = -1)\n",
    "            \n",
    "        x = self.final_layer(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Haven't figured out how to make the training from the supplied code work yet, so I am writing my own training code using the standard approach learned in class and homeworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a data loader\n",
    "The authors of the article have a script to load the data in different configurations. I am reusing it partially but wasn't able to make it work yet because of the errors, so I am creating my own Dataset class and a data loader that can deal with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEnvDataset(Dataset):\n",
    "    def __init__(self, dataframes, subset='train', envs=None, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with data from multiple environments and a specific subset.\n",
    "        :param dataframes: A dictionary with environment keys, each containing another dict with subsets as DataFrames.\n",
    "        :param subset: The subset to load ('train', 'val', or 'test').\n",
    "        :param envs: A list of environment names to include. If None, include all.\n",
    "        :param transform: PyTorch transforms to apply to the images.\n",
    "        \"\"\"\n",
    "        if envs is None:\n",
    "            envs = list(dataframes.keys())\n",
    "        \n",
    "        self.data = pd.concat([dataframes[env][subset] for env in envs if env in dataframes], ignore_index=True)\n",
    "        \n",
    "        self.label_columns = [\"No Finding\", \"Atelectasis\", \"Cardiomegaly\", \"Effusion\", \"Pneumonia\", \n",
    "                              \"Pneumothorax\", \"Consolidation\", \"Edema\"]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['path']\n",
    "        image = Image.open(img_path).convert('RGB')  # Convert to RGB to handle potential grayscale images\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        labels = torch.tensor(self.data.iloc[idx][self.label_columns].values.astype(float), dtype=torch.float32)\n",
    "        if np.isnan(labels).any():\n",
    "            raise ValueError(\"NaN values found in labels\")\n",
    "        \n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = MultiEnvDataset(dfs, subset='val', envs=['MIMIC', 'CXP'], transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "val_dataset = MultiEnvDataset(dfs, subset='test', envs=['MIMIC', 'CXP'], transform=transform)\n",
    "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have not been able to make the training work yet, see the issue below. The original paper provides separate scripts to do the training, which require some packages that seem to be not compatible with my platform. Still figuring out how to either make the original scripts work, or write my own training in a way that it provides results similar to the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emb Dim:\n",
      "1024\n",
      "Manually setting output dim to 1024\n",
      "1024\n",
      "running loss 263.2264404296875\n",
      "running loss 514.5382080078125\n",
      "running loss 767.6939086914062\n",
      "running loss 1016.5777893066406\n",
      "running loss 1267.855239868164\n",
      "running loss 1511.8975677490234\n",
      "running loss 1747.3689422607422\n",
      "running loss 1981.6729736328125\n",
      "running loss 2208.825973510742\n",
      "running loss 2434.0503845214844\n",
      "running loss 2658.7277221679688\n",
      "running loss 2872.757781982422\n",
      "running loss 3078.2845611572266\n",
      "running loss 3282.9491424560547\n",
      "running loss 3500.4393920898438\n",
      "running loss 3702.9176330566406\n",
      "running loss 3903.872085571289\n",
      "running loss 4100.029113769531\n",
      "running loss 4295.556610107422\n",
      "running loss 4467.040328979492\n",
      "running loss 4652.415817260742\n",
      "running loss 4831.527069091797\n",
      "running loss 5008.1651611328125\n",
      "running loss 5164.4727783203125\n",
      "running loss 5334.4490966796875\n",
      "running loss 5519.285690307617\n",
      "running loss 5691.533920288086\n",
      "running loss 5856.534912109375\n",
      "running loss 6021.951202392578\n",
      "running loss 6167.427459716797\n",
      "running loss 6322.334060668945\n",
      "running loss 6465.437225341797\n",
      "running loss 6621.335006713867\n",
      "running loss 6755.909362792969\n",
      "running loss 6893.804168701172\n",
      "running loss 7029.446838378906\n",
      "running loss 7159.954345703125\n",
      "running loss 7293.2901611328125\n",
      "running loss 7420.679954528809\n",
      "running loss 7557.0693435668945\n",
      "running loss 7660.711280822754\n",
      "running loss 7779.509391784668\n",
      "running loss 7895.223670959473\n",
      "running loss 8005.603904724121\n",
      "running loss 8114.697242736816\n",
      "running loss 8226.752082824707\n",
      "running loss 8323.348770141602\n",
      "running loss 8421.37954711914\n",
      "running loss 8513.052742004395\n",
      "running loss 8617.317276000977\n",
      "running loss 8731.37776184082\n",
      "running loss 8809.246154785156\n",
      "running loss 8884.446670532227\n",
      "running loss 8969.108947753906\n",
      "running loss 9052.517570495605\n",
      "running loss 9125.895866394043\n",
      "running loss 9200.753684997559\n",
      "running loss 9287.989990234375\n",
      "running loss 9366.517456054688\n",
      "running loss 9451.8193359375\n",
      "running loss 9539.018249511719\n",
      "running loss 9611.504661560059\n",
      "running loss 9692.316856384277\n",
      "running loss 9756.752700805664\n",
      "running loss 9822.772445678711\n",
      "running loss 9882.612689971924\n",
      "running loss 9945.622009277344\n",
      "running loss 10013.120765686035\n",
      "running loss 10074.977058410645\n",
      "running loss 10135.496696472168\n",
      "running loss 10204.92961883545\n",
      "running loss 10266.638999938965\n",
      "running loss 10324.971649169922\n",
      "running loss 10388.593769073486\n",
      "running loss 10450.188953399658\n",
      "running loss 10507.183879852295\n",
      "running loss 10560.791805267334\n",
      "running loss 10615.801383972168\n",
      "running loss 10676.982074737549\n",
      "running loss 10736.466766357422\n",
      "running loss 10793.28406906128\n",
      "running loss 10854.838947296143\n",
      "running loss 10911.342350006104\n",
      "running loss 10966.253261566162\n",
      "running loss 11014.77388381958\n",
      "running loss 11065.643642425537\n",
      "running loss 11118.004154205322\n",
      "running loss 11173.210079193115\n",
      "running loss 11230.999080657959\n",
      "running loss 11280.911113739014\n",
      "running loss 11331.638748168945\n",
      "running loss 11389.954246520996\n",
      "running loss 11448.507640838623\n",
      "running loss 11498.174304962158\n",
      "running loss 11547.400192260742\n",
      "running loss 11599.760276794434\n",
      "running loss 11650.623268127441\n",
      "running loss 11698.527076721191\n",
      "running loss 11748.927616119385\n",
      "running loss 11796.289585113525\n",
      "running loss 11848.02409362793\n",
      "running loss 11899.132415771484\n",
      "running loss 11948.436485290527\n",
      "running loss 11998.694435119629\n",
      "running loss 12049.453651428223\n",
      "running loss 12099.234474182129\n",
      "running loss 12152.207180023193\n",
      "running loss 12202.978454589844\n",
      "running loss 12253.137775421143\n",
      "running loss 12303.006317138672\n",
      "running loss 12350.035808563232\n",
      "running loss 12402.813835144043\n",
      "running loss 12448.386596679688\n",
      "running loss 12496.214660644531\n",
      "running loss 12544.965816497803\n",
      "running loss 12588.70524597168\n",
      "running loss 12640.012870788574\n",
      "running loss 12690.197288513184\n",
      "running loss 12744.215274810791\n",
      "running loss 12799.199321746826\n",
      "running loss 12847.462581634521\n",
      "running loss 12895.682788848877\n",
      "running loss 12944.68099975586\n",
      "running loss 12992.084686279297\n",
      "running loss 13040.460922241211\n",
      "running loss 13087.255855560303\n",
      "running loss 13137.719722747803\n",
      "running loss 13183.335948944092\n",
      "running loss 13233.157001495361\n",
      "running loss 13280.848705291748\n",
      "running loss 13330.21603012085\n",
      "running loss 13378.96224975586\n",
      "running loss 13426.184028625488\n",
      "running loss 13475.988376617432\n",
      "running loss 13529.004596710205\n",
      "running loss 13576.091049194336\n",
      "running loss 13623.205905914307\n",
      "running loss 13675.88892364502\n",
      "running loss 13725.351512908936\n",
      "running loss 13771.685642242432\n",
      "running loss 13817.531429290771\n",
      "running loss 13864.055099487305\n",
      "running loss 13913.31328201294\n",
      "running loss 13962.460708618164\n",
      "running loss 14013.788032531738\n",
      "running loss 14067.27596282959\n",
      "running loss 14116.08564376831\n",
      "running loss 14172.31273651123\n",
      "running loss 14227.02986907959\n",
      "running loss 14278.583694458008\n",
      "running loss 14324.254917144775\n",
      "running loss 14371.17650604248\n",
      "running loss 14417.864459991455\n",
      "running loss 14464.447261810303\n",
      "running loss 14512.727481842041\n",
      "running loss 14559.30135345459\n",
      "running loss 14609.74719619751\n",
      "running loss 14663.944187164307\n",
      "running loss 14712.865699768066\n",
      "running loss 14760.463802337646\n",
      "running loss 14807.770729064941\n",
      "running loss 14854.171257019043\n",
      "running loss 14903.6439743042\n",
      "running loss 14949.490509033203\n",
      "running loss 15000.562110900879\n",
      "running loss 15049.299255371094\n",
      "running loss 15096.13342666626\n",
      "running loss 15146.884189605713\n",
      "running loss 15194.760795593262\n",
      "running loss 15239.684482574463\n",
      "running loss 15282.924556732178\n",
      "running loss 15332.531700134277\n",
      "running loss 15381.179214477539\n",
      "running loss 15428.891220092773\n",
      "running loss 15479.853492736816\n",
      "running loss 15526.856483459473\n",
      "running loss 15575.951271057129\n",
      "running loss 15622.238121032715\n",
      "running loss 15671.231842041016\n",
      "running loss 15716.44707107544\n",
      "running loss 15768.276332855225\n",
      "running loss 15816.30612564087\n",
      "running loss 15867.786472320557\n",
      "running loss 15916.656875610352\n",
      "running loss 15971.457942962646\n",
      "running loss 16021.628757476807\n",
      "running loss 16069.260257720947\n",
      "running loss 16116.124195098877\n",
      "running loss 16165.465824127197\n",
      "running loss 16211.13319015503\n",
      "running loss 16257.81816482544\n",
      "running loss 16304.021411895752\n",
      "running loss 16350.321594238281\n",
      "running loss 16400.017135620117\n",
      "running loss 16413.089469909668\n",
      "Epoch: 1.00, Train Loss: 0.66, Validation Loss: 0.37\n",
      "running loss 42.45112228393555\n",
      "running loss 89.3318977355957\n",
      "running loss 132.06999588012695\n",
      "running loss 177.13267517089844\n",
      "running loss 223.38393020629883\n",
      "running loss 267.8280029296875\n",
      "running loss 314.33104705810547\n",
      "running loss 360.1613578796387\n",
      "running loss 408.38878631591797\n",
      "running loss 454.4367370605469\n",
      "running loss 502.46258544921875\n",
      "running loss 549.7138519287109\n",
      "running loss 593.5496826171875\n",
      "running loss 642.1297874450684\n",
      "running loss 687.5514221191406\n",
      "running loss 729.4453964233398\n",
      "running loss 775.2053871154785\n",
      "running loss 818.8695793151855\n",
      "running loss 864.7737312316895\n",
      "running loss 912.6025466918945\n",
      "running loss 961.8764610290527\n",
      "running loss 1010.7445297241211\n",
      "running loss 1057.4910278320312\n",
      "running loss 1105.6694564819336\n",
      "running loss 1149.8942260742188\n",
      "running loss 1197.8647918701172\n",
      "running loss 1244.286148071289\n",
      "running loss 1293.9929275512695\n",
      "running loss 1340.5834655761719\n",
      "running loss 1388.1890411376953\n",
      "running loss 1435.6765747070312\n",
      "running loss 1483.9369812011719\n",
      "running loss 1529.9716682434082\n",
      "running loss 1573.0248947143555\n",
      "running loss 1616.699306488037\n",
      "running loss 1661.393653869629\n",
      "running loss 1711.4137344360352\n",
      "running loss 1759.8415985107422\n",
      "running loss 1810.2802124023438\n",
      "running loss 1858.2897911071777\n",
      "running loss 1907.5538215637207\n",
      "running loss 1949.5085258483887\n",
      "running loss 1992.4625129699707\n",
      "running loss 2040.8167190551758\n",
      "running loss 2087.316436767578\n",
      "running loss 2134.2613677978516\n",
      "running loss 2179.969467163086\n",
      "running loss 2222.3659286499023\n",
      "running loss 2266.685199737549\n",
      "running loss 2315.743137359619\n",
      "running loss 2359.655216217041\n",
      "running loss 2401.856159210205\n",
      "running loss 2447.4629707336426\n",
      "running loss 2497.6397857666016\n",
      "running loss 2541.120719909668\n",
      "running loss 2588.4553871154785\n",
      "running loss 2636.037281036377\n",
      "running loss 2678.1400566101074\n",
      "running loss 2727.0403747558594\n",
      "running loss 2771.373073577881\n",
      "running loss 2817.019229888916\n",
      "running loss 2864.090675354004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running loss 2912.8115234375\n",
      "running loss 2960.2079696655273\n",
      "running loss 3009.55073928833\n",
      "running loss 3054.4355545043945\n",
      "running loss 3101.1442489624023\n",
      "running loss 3144.3744506835938\n",
      "running loss 3190.251163482666\n",
      "running loss 3237.353401184082\n",
      "running loss 3286.4815063476562\n",
      "running loss 3331.6801681518555\n",
      "running loss 3380.8242988586426\n",
      "running loss 3428.854206085205\n",
      "running loss 3475.562110900879\n",
      "running loss 3518.5119819641113\n",
      "running loss 3565.770092010498\n",
      "running loss 3610.592571258545\n",
      "running loss 3659.9098587036133\n",
      "running loss 3710.173740386963\n",
      "running loss 3751.1669731140137\n",
      "running loss 3799.3882331848145\n",
      "running loss 3849.5416984558105\n",
      "running loss 3895.4034461975098\n",
      "running loss 3937.5432624816895\n",
      "running loss 3984.0265464782715\n",
      "running loss 4027.1926651000977\n",
      "running loss 4072.9758529663086\n",
      "running loss 4117.023204803467\n",
      "running loss 4163.359088897705\n",
      "running loss 4211.375995635986\n",
      "running loss 4258.674472808838\n",
      "running loss 4303.471160888672\n",
      "running loss 4347.153945922852\n",
      "running loss 4396.068622589111\n",
      "running loss 4441.4503746032715\n",
      "running loss 4488.311561584473\n",
      "running loss 4535.040584564209\n",
      "running loss 4585.439701080322\n",
      "running loss 4628.101001739502\n",
      "running loss 4674.413074493408\n",
      "running loss 4721.819255828857\n",
      "running loss 4765.396369934082\n",
      "running loss 4814.362762451172\n",
      "running loss 4865.417552947998\n",
      "running loss 4913.82780456543\n",
      "running loss 4953.691452026367\n",
      "running loss 5001.301746368408\n",
      "running loss 5043.974155426025\n",
      "running loss 5092.115978240967\n",
      "running loss 5136.269771575928\n",
      "running loss 5182.309257507324\n",
      "running loss 5231.833297729492\n",
      "running loss 5276.650459289551\n",
      "running loss 5326.066158294678\n",
      "running loss 5368.054489135742\n",
      "running loss 5412.519458770752\n",
      "running loss 5457.377510070801\n",
      "running loss 5504.551322937012\n",
      "running loss 5547.9135818481445\n",
      "running loss 5596.989971160889\n",
      "running loss 5639.785087585449\n",
      "running loss 5684.6651611328125\n",
      "running loss 5726.015930175781\n",
      "running loss 5777.912857055664\n",
      "running loss 5820.463039398193\n",
      "running loss 5867.232128143311\n",
      "running loss 5913.773197174072\n",
      "running loss 5960.416213989258\n",
      "running loss 6002.496231079102\n",
      "running loss 6046.907760620117\n",
      "running loss 6089.70414352417\n",
      "running loss 6137.69002532959\n",
      "running loss 6182.910911560059\n",
      "running loss 6231.793472290039\n",
      "running loss 6275.7735023498535\n",
      "running loss 6320.52876663208\n",
      "running loss 6368.511116027832\n",
      "running loss 6411.466915130615\n",
      "running loss 6456.929683685303\n",
      "running loss 6501.755310058594\n",
      "running loss 6549.1900062561035\n",
      "running loss 6593.416694641113\n",
      "running loss 6642.116436004639\n",
      "running loss 6686.643672943115\n",
      "running loss 6737.117595672607\n",
      "running loss 6782.315643310547\n",
      "running loss 6829.9642906188965\n",
      "running loss 6876.279556274414\n",
      "running loss 6922.847454071045\n",
      "running loss 6965.512577056885\n",
      "running loss 7016.5288734436035\n",
      "running loss 7059.05184173584\n",
      "running loss 7107.086235046387\n",
      "running loss 7152.692604064941\n",
      "running loss 7202.879848480225\n",
      "running loss 7247.961334228516\n",
      "running loss 7293.595348358154\n",
      "running loss 7341.429763793945\n",
      "running loss 7385.623558044434\n",
      "running loss 7429.6084060668945\n",
      "running loss 7474.953426361084\n",
      "running loss 7522.121597290039\n",
      "running loss 7570.539932250977\n",
      "running loss 7614.110954284668\n",
      "running loss 7663.822456359863\n",
      "running loss 7714.057922363281\n",
      "running loss 7756.040481567383\n",
      "running loss 7799.134952545166\n",
      "running loss 7848.331470489502\n",
      "running loss 7898.281356811523\n",
      "running loss 7941.817085266113\n",
      "running loss 7989.635982513428\n",
      "running loss 8035.302082061768\n",
      "running loss 8084.002025604248\n",
      "running loss 8132.990497589111\n",
      "running loss 8179.127983093262\n",
      "running loss 8223.65592956543\n",
      "running loss 8269.752140045166\n",
      "running loss 8311.863334655762\n",
      "running loss 8359.11184310913\n",
      "running loss 8403.826725006104\n",
      "running loss 8448.430995941162\n",
      "running loss 8491.54602432251\n",
      "running loss 8537.93673324585\n",
      "running loss 8583.768077850342\n",
      "running loss 8628.840282440186\n",
      "running loss 8672.730026245117\n",
      "running loss 8715.987743377686\n",
      "running loss 8764.307682037354\n",
      "running loss 8811.981819152832\n",
      "running loss 8855.474704742432\n",
      "running loss 8898.440982818604\n",
      "running loss 8947.426219940186\n",
      "running loss 8959.548305511475\n"
     ]
    }
   ],
   "source": [
    "model = EmbModel(emb_type=\"densenet121\", feature_size_override=1024, pretrain=False, num_labels=8)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "def train_model_one_epoch(model, train_loader, loss_func, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        if torch.isnan(outputs).any():\n",
    "            raise ValueError(\"NaN detected in model outputs\")\n",
    "        \n",
    "        loss = loss_func(outputs, labels)\n",
    "        if torch.isnan(loss).any():\n",
    "            raise ValueError(\"NaN detected in loss computation\")\n",
    "            \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        print('running loss', running_loss)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "def validate_model(model, val_loader, loss_func):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "num_epoch = 10\n",
    "# model training loop: it is better to print the training/validation losses during the training\n",
    "for i in range(num_epoch):\n",
    "    train_loss = train_model_one_epoch(model, train_loader, loss_func, optimizer)\n",
    "    valid_loss = validate_model(model, val_loader, loss_func)\n",
    "    print(\"Epoch: %.2f, Train Loss: %.2f, Validation Loss: %.2f\" % (i+1, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This block of code doesn't work yet, I am currently figuring out how to make it work. Use the training block in the cell above (it is very slow-running though)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import wandb\n",
    "import json\n",
    "\n",
    "def train_models(df, pred_col, pred_vals, fix_col, fix_vals, results_dict, task_type):   \n",
    "    for fix_val in tqdm(fix_vals):\n",
    "        # Subset the dataframe to just have this val in the column\n",
    "        df_fix = df[df[fix_col] == fix_val]\n",
    "        # Drop this column to avoid any oddities during training\n",
    "        df_fix = df_fix.drop(columns=fix_col)\n",
    "        \n",
    "        # Get just the 2/4 classes that we're trying to \n",
    "        for pred_val in tqdm(pred_vals):\n",
    "            df_pred = df_fix[df_fix[pred_col].isin(pred_val)]\n",
    "            \n",
    "            # We have the final dataframe, but we need to create a perfectly balanced \n",
    "            # version of it\n",
    "            grouped = df_pred.groupby(pred_col)\n",
    "            # print(\"Count per class:\", grouped[\"emb0\"].count())\n",
    "            min_group_size = grouped.count()[\"emb0\"].min()\n",
    "            df_bal = grouped.sample(n=min_group_size, random_state=0)\n",
    "            # print(\"Count per class after balancing:\", df_bal.groupby(pred_col)[\"emb0\"].count())\n",
    "            \n",
    "            # Note that we may have a single class remaining in our dataset (if we're doing the baseline \n",
    "            # CXP vs CXP prediction, for example). We need to check that and manually change our dataset\n",
    "            # If that is the case\n",
    "            df_bal = df_bal.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "            \n",
    "            if len(pred_val) == 1:\n",
    "                print(f\"INFO: SINGLE PRED VAL: {pred_val} for col: {pred_col}... Subsetting through the middle\")\n",
    "                mid_val = len(df_bal) // 2\n",
    "                \n",
    "                df_bal.loc[:mid_val, pred_col] = \"0\"\n",
    "                df_bal.loc[mid_val:, pred_col] = \"1\"\n",
    "            \n",
    "            # Now lets pass this dataframe into our train method\n",
    "            acc = train_model(df_bal, pred_col)\n",
    "            \n",
    "            # Store the results in our global dictionary\n",
    "            results_dict[task_type].append({\n",
    "                \"fix_val\": fix_val,\n",
    "                \"pred_val\": pred_val,\n",
    "                \"min_group_size\": min_group_size,\n",
    "                \"df_size\": len(df_bal),\n",
    "                \"acc\": acc,\n",
    "            })\n",
    "            \n",
    "def train_model(df, pred_col, max_iter=5000):\n",
    "    X, y = df.drop(columns=pred_col), df[pred_col]\n",
    "    \n",
    "    model = make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-5, max_iter=max_iter))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    return acc            \n",
    "            \n",
    "def main():\n",
    "#     wandb.init(project=\"ood-generalization\",\n",
    "#             job_type=\"emb_train\", \n",
    "#             entity=\"basedrhys\",\n",
    "#               name=f\"row {row_idx}\")\n",
    "    \n",
    "    results_dict = {}\n",
    "    results_dict[\"env_pred\"] = []\n",
    "    results_dict[\"label_pred\"] = []\n",
    "\n",
    "    # Environment Prediction Task\n",
    "    env_fix_col = \"targets\"\n",
    "    env_fix_vals = [0, 1]\n",
    "\n",
    "    env_pred_col = \"env\"\n",
    "    env_pred_vals = [(\"CXP\", ), (\"MIMIC\", ), (\"NIH\", ), (\"PAD\", ), (\"CXP\",\"NIH\"), (\"CXP\",\"PAD\"), (\"MIMIC\",\"CXP\"), (\"MIMIC\",\"NIH\"), (\"MIMIC\",\"PAD\"), (\"NIH\",\"PAD\"), (\"CXP\", \"MIMIC\", \"NIH\", \"PAD\")]\n",
    "\n",
    "    train_models(df=ml_df, \n",
    "                 pred_col=env_pred_col,\n",
    "                 pred_vals=env_pred_vals,\n",
    "                 fix_col=env_fix_col,\n",
    "                 fix_vals=env_fix_vals,\n",
    "                 results_dict=results_dict,\n",
    "                 task_type=\"env_pred\")\n",
    "    \n",
    "    # Label prediction task\n",
    "    label_fix_col = \"env\"\n",
    "    label_fix_vals = [\"CXP\", \"MIMIC\", \"NIH\", \"PAD\"]\n",
    "\n",
    "    label_pred_col = \"targets\"\n",
    "    label_pred_vals = [(0, 1), (0, ), (1, )]\n",
    "    \n",
    "    train_models(df=ml_df,\n",
    "                 pred_col=label_pred_col,\n",
    "                 pred_vals=label_pred_vals,\n",
    "                 fix_col=label_fix_col,\n",
    "                 fix_vals=label_fix_vals,\n",
    "                 results_dict=results_dict,\n",
    "                 task_type=\"label_pred\")\n",
    "    \n",
    "#     wandb.log(results_dict)\n",
    "    \n",
    "    output_dir = row[\"output_dir\"]\n",
    "    \n",
    "    print(\"Outputting JSON to\", output_dir)\n",
    "    \n",
    "    with open(f\"{output_dir}/emb_test_results.json\", mode=\"w\") as f:\n",
    "        json.dump(results_dict, f, indent=True)\n",
    "    \n",
    "    return results_dict\n",
    "row_idx = 2\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOdhGrbwwG71"
   },
   "source": [
    "_You don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper_\n",
    "\n",
    "Once available, I plan to compare my model performance using different datasets with the results posted in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "# Discussion\n",
    "\n",
    "## Is the paper reproducible?\n",
    "It is too early to tell right now, but at least a portion of the code provided is runnable with minimal updates. I was able to reproduce the initial dataset statistics, so at least that portion is definitely reproducible. The rest will depend on whether I am able to solve and run the training.\n",
    "\n",
    "## If the paper is _not_ reproducible, explain the results\n",
    "TBD depending on whether the paper results will be reproducible or not.\n",
    "\n",
    "## What was easy and what was difficult\n",
    "The authors did a great job documenting some parts of the project, for example, access to data. Following the instructions was very easy, and while MIMIC-CXR-JPG dataset access took some time to get, overall the process was a breeze.\n",
    "\n",
    "Downloading the datasets is a hassle though, I ran out of space on my laptop, had to buy an external drive and restart the download process for MIMIC-CXR-JPG a few times.\n",
    "\n",
    "There are a few notebooks and standalone scripts provided to process the data. While it is possible to figure out what steps need to be done in what order, many of the parts of the process are not documented. 'pyproject.toml' did not run successfully for me, and I've been stuck trying to figure out why and how to run it (I have a suspicion my processor architecture is not supported, but not enough experience to tell for sure yet).\n",
    "\n",
    "In parallel, I opted to re-implement the training and model validation myself. There is code for training and validation in the project, which has a lot of comments (great!), but the process itself is not well documented, so the reproducer is left figuring out which steps in the code are needed and which are not, and how to adapt it to use for their experiment. The code is very general and there is a lot of it. There are some pointers in the readme, but they are at this point not sufficient for reproducing things successfully without additional modification.\n",
    "\n",
    "wandb isn't really working for me either yet, and I am yet to figure out why it is needed and whether it is necessary to reproduce the results. \n",
    "\n",
    "The data is not processed evenly / equally for each dataset, there are different values for the same labels (NaN, True/False, 1/0, 1.1/0.0). I had to write some processing code to make sure we mitigate those differences. \n",
    "\n",
    "Additional complication is due to the fact that the amount of data is very large. Any training or processing takes a long time, the notebook kernel dies frequently and the overall process is frustrating.\n",
    "\n",
    "I tried to avoid multiple separate files and scripts, and pulled many of the data preprocessing into my notebook. However, this increased the runtime of the notebook significantly. Additional factor affecting the runtime is the size of the input data, even when working on one dataset. I doubt it would be possible to achieve the 8 minute runtime, but will try to do so.\n",
    "  \n",
    "## Suggestions for the author\n",
    "\n",
    "Trim the codebase leaving only relevant parts. Add documentation for the training and validation process. Add some background on why wandb is used and how to use it for this project correctly. Provide a suggested order of execution for the notebooks.\n",
    "\n",
    "## Plans for the next phase\n",
    "\n",
    "In the remaining time until the final submission May 7 deadline, my plans are:\n",
    "* Further update the data processing functions so they are producing similar type results (right now while compatible, it's a mix of _int_, _float_ and _True/False_, I would like to homogenize the resulting dataset further)\n",
    "* Finalize the training for the model and compute worst per-group accuracy for all data combinations listed in the article (so far I had the most issues with training, as the code supplied with the article didn't work and I had to come up with my own in which I try to replicate the experiment as close to the article description as possible)\n",
    "* Plot the results and finalize the writeup (compute both worst per-group accuracy, and AUROC and compare to the results of the article)\n",
    "* Prepare a subset of data and optimize the notebook to run under 8 minutes if at all possible (as per the original requirements) - this might be complicated as the main focus of this article is dealing with more data and all datasets are quite large. Randomly picking samples from each dataset might further introduce some unintended spurious correlations\n",
    "* Check the main hypothesis on both balanced and unbalanced datasets, time permitting \n",
    "* Prepare a video presentation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# References\n",
    "\n",
    "1.   Rhys Compton; Lily Zhang; Aahlad Puli; Rajesh Ranganath, When More is Less: Incorporating Additional Datasets Can Hurt Performance By Introducing Spurious Correlations, arXiv preprint, 2023-08-09, Accepted at MLHC 2023, doi: [10.48550/arXiv.2308.04431](https://doi.org/10.48550/arXiv.2308.04431)\n",
    "2.   Haoran Zhang, Natalie Dullerud, Laleh Seyyed-Kalantari, Quaid Morris, Shalmali Joshi, and Marzyeh Ghassemi. An empirical framework for domain generalization in clinical settings. In Proceedings of the Conference on Health, Inference, and Learning, pages 279–290, 2021, doi: [10.48550/arXiv.2103.11163](https://doi.org/10.48550/arXiv.2103.11163)\n",
    "3.   Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700–4708, 2017, doi: [10.48550/arXiv.1608.06993](https://doi.org/10.48550/arXiv.1608.06993)\n",
    "4.   Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large- scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255, 2009, doi: [10.1109/CVPR.2009.5206848](https://doi.org/10.1109/CVPR.2009.5206848)\n",
    "5.   John R Zech, Marcus A Badgeley, Manway Liu, Anthony B Costa, Joseph J Titano, and Eric Karl Oermann. Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study. PLoS medicine, 15(11): e1002683, 2018, doi: [10.1371/journal.pmed.1002683](https://doi.org/10.1371/journal.pmed.1002683)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
