{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j01aH0PR4Sg-"
   },
   "source": [
    "# Nina Stawski's (group 90) final project report [DRAFT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illinois ID: ninas2\n",
    "\n",
    "\n",
    "[GitHub repo link](https://github.com/nstawski/dlh-final-project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sfk8Zrul_E8V"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "# disabling the cell since I am not using it, but keeping in the notebook in case I need it in the future.\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "## Background of the problem\n",
    "\n",
    "### Type of problem\n",
    "  \n",
    "  This is a data preparation and processing problem. The authors of the article are testing a common belief that adding more data improves the resulting model performance. Their main hypothesis, which they subsequently prove, is that incorporating more data does not necessary improve the model performance. It can introduce spurious correlations, and hurt the resulting model performance rather than helping it.\n",
    "\n",
    "### What is the importance/meaning of solving the problem\n",
    "  \n",
    "  The paper is challenging a common belief, meaning a lot of researchers are likely trying to incorporate as much data as they can expecting it would improve the performance of their models. The outcome of this research would provide guidance on the possible pitfalls and the cases where you wouldn't want to add external data - so it could set a new standard of processing and incorporating data for everyone in the field.\n",
    "\n",
    "### The difficulty of the problem\n",
    "\n",
    "  The problem is non-obvious and the paper is challenging the common belief held in the industry. The authors are putting a lot of state-of-the-art approaches to the test, and attempt to quantify the results as well as provide new standards and explanations. This is extremely hard to do so I believe the problem is difficult.\n",
    "\n",
    "### The state of the art methods and effectiveness\n",
    "\n",
    "  The \"industry standard\" way of improving model performance is adding more data from additional datasets, which the authors of this article prove to not be effective, and even being harmful in many cases.\n",
    "\n",
    "  One of the main issues causing the model performance decrease when adding more data from other sources is spurious correlations, which in case of x-rays could be coming even from the scanner artifacts, or other hospital-specific data. One of the state-of-the-art ways to mitigate this is balancing a dataset to reduce the influence of hospital-specific factors. While balancing definitely improved the situation, the resulting model performance was still in many cases worse than with a single-hospital dataset.\n",
    "\n",
    "\n",
    "## Paper explanation\n",
    "### What did the paper propose\n",
    "The paper used four most-used chest x-ray datasets - MIMIC-CXR-JPG, CheXpert, PadChest, ChestXray8 - to disprove a popular belief that adding more data always would improve the performance of your model. They postulate that, for the specific x-ray data, even the scanners themselves, the way hospitals produce data, or send specific patients to specific places to do their scan, can introduce spurious correlations which, in many cases, significantly affect the worst group performance.\n",
    "\n",
    "### What is the innovations of the method\n",
    "Existing research (for example, John R Zech, Marcus A Badgeley, Manway Liu, Anthony B Costa, Joseph J Titano, and Eric Karl Oermann. Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study. PLoS medicine, 15(11): e1002683, 2018.) proves that adding a second dataset improves the average per-group accuracy. In contrast, the paper I am reproducing focuses on the worst per-group accuracy.\n",
    "\n",
    "### How well the proposed method work (in its own metrics)\n",
    "According to the article authors, their method works really well and proves that in nearly 50% of cases adding a second dataset, and even balancing it to reduce spurious correllations doesn't get the model to perform better than without that additional dataset. The models pick up on hospital-specific features even if those features weren't explicitly defined in the original data. They postulate that every CNN model, regardless of training disease or datasets, learns embeddings that can distinguish any of the hospital sources with near-perfect accuracy, even if the embeddings were trained via one or two hospitalsâ€™ data.\n",
    "\n",
    "### What is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem).\n",
    "\n",
    "The article cautions against blindly adding more datasets, and provides a number of approaches you can take if you still decide to do so. The conclusion is adding more data shouldn't be done blindly. The authors of the article definitely discourage the researchers from the most common approach of throwing data at the problem to improve model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    "# Scope of Reproducibility:\n",
    "\n",
    "List hypotheses from the paper you will test and the corresponding experiments you will run.\n",
    "\n",
    "![Paper overview](https://raw.githubusercontent.com/basedrhys/ood-generalization/5d8ff09eba4c0b4b20b5ae2814fe865bed1dfb0e/img/high_level_overview.png)\n",
    "\n",
    "## Hypothesis 1\n",
    "\n",
    "In 43% of training dataset/disease tasks, adding data from an external source hurts worst-group performance.\n",
    "\n",
    "\n",
    "## Hypothesis 2\n",
    "\n",
    "Balancing the dataset to reduce spurious correlations is often beneficial, but in the scenarios where adding an additional data source hurts generalization performance, it does not always improve generalization; in some cases, training on a balanced dataset achieves lower worst-group accuracy than training on datasets from one or two hospitals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rRksCB1vbYwJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false --no-raise-error\n",
    "# disabling the cell since I am not using it, but keeping in the notebook in case I need it in the future.\n",
    "\n",
    "# no code is required for this section\n",
    "'''\n",
    "if you want to use an image outside this notebook for explanaition,\n",
    "you can upload it to your google drive and show it with OpenCV or matplotlib\n",
    "'''\n",
    "# mount this notebook to your google drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# define dirs to workspace and data\n",
    "img_dir = '/content/gdrive/My Drive/Colab Notebooks/<path-to-your-image>'\n",
    "\n",
    "import cv2\n",
    "img = cv2.imread(img_dir)\n",
    "cv2.imshow(\"Title\", img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "# Methodology\n",
    "\n",
    "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
    "\n",
    "The methodology at least contains two subsections **data** and **model** in your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: importlib in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: torch in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (0.15.2a0)\n",
      "Requirement already satisfied: numpy in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from torchvision) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from requests->torchvision) (2024.2.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from torch->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from torch->torchvision) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from torch->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from torch->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from torch->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from torch->torchvision) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from jinja2->torch->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from sympy->torch->torchvision) (1.3.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.4-cp310-cp310-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.1-cp310-cp310-win_amd64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.51.0-cp310-cp310-win_amd64.whl.metadata (162 kB)\n",
      "     -------------------------------------- 162.8/162.8 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.5-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\stan\\miniconda3\\envs\\nina\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.4-cp310-cp310-win_amd64.whl (7.7 MB)\n",
      "   ---------------------------------------- 7.7/7.7 MB 17.5 MB/s eta 0:00:00\n",
      "Downloading contourpy-1.2.1-cp310-cp310-win_amd64.whl (187 kB)\n",
      "   --------------------------------------- 187.5/187.5 kB 11.1 MB/s eta 0:00:00\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.51.0-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 2.2/2.2 MB 23.2 MB/s eta 0:00:00\n",
      "Downloading kiwisolver-1.4.5-cp310-cp310-win_amd64.whl (56 kB)\n",
      "   ---------------------------------------- 56.1/56.1 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "   ---------------------------------------- 103.2/103.2 kB ? eta 0:00:00\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.51.0 kiwisolver-1.4.5 matplotlib-3.8.4 pyparsing-3.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install importlib\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "yu61Jp1xrnKk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Data_Constants' from 'C:\\\\Users\\\\Stan\\\\Documents\\\\GitHub\\\\dlh-final-project\\\\Data_Constants.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import  packages you need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "from os.path import exists\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import json\n",
    "import random\n",
    "from IPython.display import display\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score\n",
    "# from google.colab import drive\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import Data_Constants as Constants\n",
    "\n",
    "#making sure all referenced files are reloaded\n",
    "import importlib\n",
    "importlib.reload(Constants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "# torch.set_default_device('cuda')\n",
    "\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "#  Data\n",
    "The study is using four datasets: MIMIC-CXR-JPG, CheXpert, PadChest, ChestXray8\n",
    "\n",
    "The datasets are being filtered to include only frontal (PA/AP) images. Instances are labeled with one or more pathologies. Each dataset has a different set of diseases but they are preprocessed using code derived from ClinicalDG2 (Zhang et al., 2021) to extract the eight common labels and homogenize the datasets. Additionally, authors of the article created the Any label which indicates a positive label for any of the seven common disease labels, resulting in nine different binary labels. All experiments use the labels in a binary manner; a pathology is chosen as the target label, with an instance labeled 1 if the pathology of interest is present and 0 otherwise. \n",
    "\n",
    "The autors apply an 80%/10%/10% subject-wise train/val/test split, with the same split used across seeds.\n",
    "\n",
    "### MIMIC-CXR\n",
    "\n",
    "1. [Obtain access](https://mimic-cxr.mit.edu/about/access/) to the MIMIC-CXR-JPG Database Database on PhysioNet and download the [dataset](https://physionet.org/content/mimic-cxr-jpg/2.0.0/). The best option is downloading from the GCP bucket:\n",
    "\n",
    "```sh\n",
    "gcloud auth login\n",
    "mkdir MIMIC-CXR-JPG\n",
    "gsutil -m rsync -d -r gs://mimic-cxr-jpg-2.0.0.physionet.org MIMIC-CXR-JPG\n",
    "```\n",
    "\n",
    "2. In order to obtain gender information for each patient, you will need to obtain access to [MIMIC-IV](https://physionet.org/content/mimiciv/0.4/). Download `core/patients.csv.gz` and place the file in the `MIMIC-CXR-JPG` directory.\n",
    "\n",
    "### CheXpert\n",
    "1. Sign up with your email address [here](https://stanfordmlgroup.github.io/competitions/chexpert/).\n",
    "\n",
    "2. Download either the original or the downsampled dataset (we recommend the downsampled version - `CheXpert-v1.0-small.zip`) and extract it.\n",
    "\n",
    "### ChestX-ray8\n",
    "\n",
    "1. Download the `images` folder and `Data_Entry_2017_v2020.csv` from the [NIH website](https://nihcc.app.box.com/v/ChestXray-NIHCC).\n",
    "\n",
    "2. Unzip all of the files in the `images` folder.\n",
    "\n",
    "### PadChest\n",
    "\n",
    "1. The paper uses a resized version of PadChest, which can be downloaded [here](https://academictorrents.com/details/96ebb4f92b85929eadfb16761f310a6d04105797).\n",
    "\n",
    "2. Unzip `images-224.tar`.\n",
    "\n",
    "  * Statistics: include basic descriptive statistics of the dataset like size, cross validation split, label distribution, etc.\n",
    "  * Data process: how do you munipulate the data, e.g., change the class labels, split the dataset to train/valid/test, refining the dataset.\n",
    "  * Illustration: printing results, plotting figures for illustration.\n",
    "  * You can upload your raw dataset to Google Drive and mount this Colab to the same directory. If your raw dataset is too large, you can upload the processed dataset and have a code to load the processed dataset.\n",
    "  \n",
    "## Data Processing\n",
    "The original pre-processing for the article was done using the scripts outside of the Jupyter Notebook. Some of them didnt' work for me, and the installation process didn't succeed despite multiple attempts either. Instead, I have adapted some of the original scripts to run in the notebook (with some modifications so they actually work with my data), using the external \"Constants.py\" file that points to the location of the datasets.\n",
    "1. In `./Data_Constants.py`, update `image_paths` to point to each of the four directories that you downloaded.\n",
    "\n",
    "2. Run the next two cells to pre-process the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzVUQS0CHry0"
   },
   "source": [
    "## Validating\n",
    "I am using the validation and pre-processing code provided by the authors of the article, with some modifications to make it run as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making sure constants are up to date if they were changed\n",
    "importlib.reload(Constants)\n",
    "\n",
    "def validate_mimic():\n",
    "    img_dir = Path(Constants.image_paths['MIMIC'])\n",
    "    meta_dir = Path(Constants.meta_paths['MIMIC'])\n",
    "    \n",
    "    print('meta_dir', meta_dir, os.getcwd())\n",
    "    print('meta_dir', meta_dir/'mimic-cxr-2.0.0-metadata.csv')\n",
    "    assert (meta_dir/'mimic-cxr-2.0.0-metadata.csv').is_file()\n",
    "    assert (meta_dir/'mimic-cxr-2.0.0-negbio.csv').is_file()\n",
    "    assert (meta_dir/'patients.csv').is_file()\n",
    "    # modified the file that's being checked since I don't have the full MIMIC-CXR-JPG dataset due to space limitations\n",
    "    # in the original script, the file in p19 was being checked.\n",
    "    assert (img_dir/'p10/p10000032/s50414267/02aa804e-bde0afdd-112c0b34-7bc16630-4e384014.jpg').is_file()\n",
    "\n",
    "def validate_cxp():\n",
    "    img_dir = Path(Constants.image_paths['CXP'])\n",
    "    if (img_dir/'CheXpert-v1.0').is_dir():\n",
    "        cxp_subfolder = 'CheXpert-v1.0'\n",
    "    else:\n",
    "        cxp_subfolder = 'CheXpert-v1.0-small'\n",
    "    assert (img_dir/cxp_subfolder/'train.csv').is_file()\n",
    "    assert (img_dir/cxp_subfolder/'train/patient48822/study1/view1_frontal.jpg').is_file()\n",
    "    assert (img_dir/cxp_subfolder/'valid/patient64636/study1/view1_frontal.jpg').is_file()\n",
    "\n",
    "def validate_pad():\n",
    "    img_dir = Path(Constants.image_paths['PAD'])\n",
    "    meta_dir = Path(Constants.meta_paths['PAD'])\n",
    "    assert (meta_dir/'PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv').is_file()\n",
    "    assert (img_dir/'185566798805711692534207714722577525271_qb3lyn.png').is_file()\n",
    "\n",
    "def validate_nih():\n",
    "    img_dir = Path(Constants.image_paths['NIH'])\n",
    "    meta_dir = Path(Constants.meta_paths['NIH'])\n",
    "    assert (meta_dir/'Data_Entry_2017.csv').is_file()\n",
    "    assert (img_dir/'images/00002072_003.png').is_file()\n",
    "\n",
    "def validate_splits():\n",
    "    for dataset in Constants.df_paths:\n",
    "        for split in Constants.df_paths[dataset]:\n",
    "            assert Path(Constants.df_paths[dataset][split]).is_file()\n",
    "\n",
    "\n",
    "def validate_all():\n",
    "    validate_mimic()\n",
    "    validate_cxp()\n",
    "    validate_nih()\n",
    "    validate_pad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making sure constants are up to date if they were changed after running this notebook\n",
    "importlib.reload(Constants)\n",
    "\n",
    "def preprocess_mimic():\n",
    "    img_dir = Path(Constants.image_paths['MIMIC'])\n",
    "    meta_dir = Path(Constants.meta_paths['MIMIC'])\n",
    "    out_folder = meta_dir/'clinicaldg'\n",
    "    out_folder.mkdir(parents = True, exist_ok = True)  \n",
    "\n",
    "    patients = pd.read_csv(meta_dir/'patients.csv')\n",
    "    labels = pd.read_csv(meta_dir/'mimic-cxr-2.0.0-negbio.csv')\n",
    "    meta = pd.read_csv(meta_dir/'mimic-cxr-2.0.0-metadata.csv')\n",
    "\n",
    "    df = meta.merge(patients, on = 'subject_id').merge(labels, on = ['subject_id', 'study_id'])\n",
    "    df['age_decile'] = pd.cut(df['anchor_age'], bins = list(range(0, 100, 10))).apply(lambda x: f'{x.left}-{x.right}').astype(str)\n",
    "    df['frontal'] = df.ViewPosition.isin(['AP', 'PA'])\n",
    "\n",
    "    df['path'] = df.apply(lambda x: os.path.join(f'p{str(x[\"subject_id\"])[:2]}', f'p{x[\"subject_id\"]}', f's{x[\"study_id\"]}', f'{x[\"dicom_id\"]}.jpg'), axis = 1)\n",
    "    df.to_csv(out_folder/\"preprocessed.csv\", index=False)\n",
    "\n",
    "def preprocess_pad():\n",
    "    # I have modified this function from the original one, because I was getting missing/ambiguous Dtype errors\n",
    "    img_dir = Path(Constants.image_paths['PAD'])\n",
    "    meta_dir = Path(Constants.meta_paths['PAD'])\n",
    "    out_folder = meta_dir/'clinicaldg'\n",
    "    out_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    dtype_spec = {\n",
    "        'ImageID': str,\n",
    "        'StudyID': str,\n",
    "        'PatientID': str,\n",
    "        'PatientBirth': str, # converting this to the integer later to avoid processing errors (due some data apparently being saved as float)\n",
    "        'PatientSex_DICOM': str,\n",
    "        'ViewPosition_DICOM': str,\n",
    "        'Projection': str,\n",
    "        'Labels': str,\n",
    "        'WindowCenter_DICOM': str,\n",
    "        'WindowWidth_DICOM': str\n",
    "    }\n",
    "\n",
    "    df = pd.read_csv(meta_dir/'PADCHEST_chest_x_ray_images_labels_160K_01.02.19.csv', dtype=dtype_spec)\n",
    "    df = df[['ImageID', 'StudyID', 'PatientID', 'PatientBirth', 'PatientSex_DICOM', 'ViewPosition_DICOM', 'Projection', 'Labels']]\n",
    "    df = df[~df[\"Labels\"].isnull()]\n",
    "    df = df[df[\"ImageID\"].apply(lambda x: os.path.exists(os.path.join(img_dir, x)))]\n",
    "    df = df[df.Projection.isin(['PA', 'L', 'AP_horizontal', 'AP'])]\n",
    "\n",
    "    df['frontal'] = ~(df['Projection'] == 'L')\n",
    "    df = df[~df['Labels'].apply(lambda x: 'exclude' in x or 'unchanged' in x)]\n",
    "\n",
    "    mapping = dict()\n",
    "    mapping['Effusion'] = ['hydropneumothorax', 'empyema', 'hemothorax']\n",
    "    mapping[\"Consolidation\"] = [\"air bronchogram\"]\n",
    "    mapping['No Finding'] = ['normal']\n",
    "\n",
    "    for pathology in Constants.take_labels:\n",
    "        mask = df[\"Labels\"].str.contains(pathology.lower())\n",
    "        if pathology in mapping:\n",
    "            for syn in mapping[pathology]:\n",
    "                mask |= df[\"Labels\"].str.contains(syn.lower())\n",
    "        df[pathology] = mask.astype(int)\n",
    "\n",
    "    df['PatientBirth'] = df['PatientBirth'].dropna().astype(float).astype(int)\n",
    "    df['Age'] = 2017 - df['PatientBirth']\n",
    "    df.reset_index(drop=True).to_csv(out_folder/\"preprocessed.csv\", index=False)\n",
    "\n",
    "\n",
    "def preprocess_cxp():\n",
    "    img_dir = Path(Constants.image_paths['CXP'])\n",
    "    out_folder = img_dir/'clinicaldg'\n",
    "    if (img_dir/'CheXpert-v1.0'/'train.csv').is_file():\n",
    "        df = pd.concat([pd.read_csv(img_dir/'CheXpert-v1.0'/'train.csv'), \n",
    "                        pd.read_csv(img_dir/'CheXpert-v1.0'/'valid.csv')],\n",
    "                        ignore_index = True)\n",
    "    elif (img_dir/'CheXpert-v1.0-small'/'train.csv').is_file(): \n",
    "        df = pd.concat([pd.read_csv(img_dir/'CheXpert-v1.0-small'/'train.csv'),\n",
    "                        pd.read_csv(img_dir/'CheXpert-v1.0-small'/'valid.csv')],\n",
    "                        ignore_index = True)\n",
    "    elif (img_dir/'train.csv').is_file():\n",
    "        raise ValueError('Please set Constants.image_paths[\"CXP\"] to be the PARENT of the current'+\n",
    "                ' directory and rerun this script.')\n",
    "    else:\n",
    "        raise ValueError(\"CheXpert files not found!\")\n",
    "\n",
    "    out_folder.mkdir(parents = True, exist_ok = True)  \n",
    "\n",
    "    df['subject_id'] = df['Path'].apply(lambda x: int(Path(x).parent.parent.name[7:]))\n",
    "    df['Path'] = df['Path'].apply(lambda x: str(x).replace(\"CheXpert-v1.0/\", \"\"))\n",
    "    df.reset_index(drop = True).to_csv(out_folder/\"preprocessed.csv\", index=False)\n",
    "\n",
    "def preprocess_nih():\n",
    "    img_dir = Path(Constants.image_paths['NIH'])\n",
    "    meta_dir = Path(Constants.meta_paths['NIH'])\n",
    "    out_folder = meta_dir/'clinicaldg'\n",
    "    out_folder.mkdir(parents = True, exist_ok = True)  \n",
    "    df = pd.read_csv(meta_dir/\"Data_Entry_2017.csv\")\n",
    "    df['labels'] = df['Finding Labels'].apply(lambda x: x.split('|'))\n",
    "\n",
    "    for label in Constants.take_labels:\n",
    "        df[label] = df['labels'].apply(lambda x: label in x)\n",
    "    df.reset_index(drop = True).to_csv(out_folder/\"preprocessed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(\"Validating paths...\")\n",
    "    validate_all()\n",
    "    print(\"Preprocessing MIMIC-CXR...\")\n",
    "    preprocess_mimic()\n",
    "    print(\"Preprocessing CheXpert...\")\n",
    "    preprocess_cxp()\n",
    "    print(\"Preprocessing ChestX-ray8...\")\n",
    "    preprocess_nih()\n",
    "    print(\"Preprocessing PadChest... This might take a few minutes...\")\n",
    "    preprocess_pad()\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next, we need to resize and process the data.\n",
    "I am using the code provided by the authors of the article to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_MIMIC(split, only_frontal):  \n",
    "    copy_subjectid = split['subject_id']     \n",
    "    split = split.drop(columns = ['subject_id']).replace(\n",
    "            [[None], -1, \"[False]\", \"[True]\", \"[ True]\", 'UNABLE TO OBTAIN', 'UNKNOWN', 'MARRIED', 'LIFE PARTNER',\n",
    "             'DIVORCED', 'SEPARATED', '0-10', '10-20', '20-30', '30-40', '40-50', '50-60', '60-70', '70-80', '80-90',\n",
    "             '>=90'],\n",
    "            [0, 0, 0, 1, 1, 0, 0, 'MARRIED/LIFE PARTNER', 'MARRIED/LIFE PARTNER', 'DIVORCED/SEPARATED',\n",
    "             'DIVORCED/SEPARATED', '0-20', '0-20', '20-40', '20-40', '40-60', '40-60', '60-80', '60-80', '80-', '80-'])\n",
    "    \n",
    "    split['subject_id'] = copy_subjectid.astype(str)\n",
    "    split['study_id'] = split['study_id'].astype(str)\n",
    "    split['Age'] = split[\"age_decile\"]\n",
    "    split['Sex'] = split[\"gender\"]\n",
    "    split = split.rename(\n",
    "        columns = {\n",
    "            'Pleural Effusion':'Effusion',   \n",
    "        })\n",
    "    split['path'] = split['path'].astype(str).apply(lambda x: os.path.join(Constants.image_paths['MIMIC'], x))\n",
    "    if only_frontal:\n",
    "        split = split[split.frontal]\n",
    "        \n",
    "    split['env'] = 'MIMIC'  \n",
    "    split.loc[split.Age == 0, 'Age'] = '0-20'\n",
    "    \n",
    "    return split[['subject_id','path','Sex',\"Age\", 'env', 'frontal', 'study_id'] + Constants.take_labels]\n",
    "\n",
    "def process_NIH(split, only_frontal = True):\n",
    "    split['Patient Age'] = np.where(split['Patient Age'].between(0,19), 19, split['Patient Age'])\n",
    "    split['Patient Age'] = np.where(split['Patient Age'].between(20,39), 39, split['Patient Age'])\n",
    "    split['Patient Age'] = np.where(split['Patient Age'].between(40,59), 59, split['Patient Age'])\n",
    "    split['Patient Age'] = np.where(split['Patient Age'].between(60,79), 79, split['Patient Age'])\n",
    "    split['Patient Age'] = np.where(split['Patient Age']>=80, 81, split['Patient Age'])\n",
    "    \n",
    "    copy_subjectid = split['Patient ID'] \n",
    "    \n",
    "    split = split.drop(columns = ['Patient ID']).replace([[None], -1, \"[False]\", \"[True]\", \"[ True]\", 19, 39, 59, 79, 81], \n",
    "                            [0, 0, 0, 1, 1, \"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-\"])\n",
    "   \n",
    "    split['subject_id'] = copy_subjectid.astype(str)\n",
    "    split['Sex'] = split['Patient Gender'] \n",
    "    split['Age'] = split['Patient Age']\n",
    "    split = split.drop(columns=[\"Patient Gender\", 'Patient Age'])\n",
    "    split['path'] = split['Image Index'].astype(str).apply(lambda x: os.path.join(Constants.image_paths['NIH'], 'images', x))\n",
    "    split['env'] = 'NIH'\n",
    "    split['frontal'] = True\n",
    "    split['study_id'] = split['subject_id'].astype(str)\n",
    "    return split[['subject_id','path','Sex',\"Age\", 'env', 'frontal','study_id'] + Constants.take_labels]\n",
    "\n",
    "\n",
    "def process_CXP(split, only_frontal):\n",
    "    split['Age'] = np.where(split['Age'].between(0,19), 19, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(20,39), 39, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(40,59), 59, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(60,79), 79, split['Age'])\n",
    "    split['Age'] = np.where(split['Age']>=80, 81, split['Age'])\n",
    "    \n",
    "    copy_subjectid = split['subject_id'] \n",
    "    split = split.drop(columns = ['subject_id']).replace([[None], -1, \"[False]\", \"[True]\", \"[ True]\", 19, 39, 59, 79, 81], \n",
    "                            [0, 0, 0, 1, 1, \"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-\"])\n",
    "    \n",
    "    split['subject_id'] = copy_subjectid.astype(str)\n",
    "    split['Sex'] = np.where(split['Sex']=='Female', 'F', split['Sex'])\n",
    "    split['Sex'] = np.where(split['Sex']=='Male', 'M', split['Sex'])\n",
    "    split = split.rename(\n",
    "        columns = {\n",
    "            'Pleural Effusion':'Effusion',\n",
    "            'Lung Opacity': 'Airspace Opacity'        \n",
    "        })\n",
    "    split['path'] = split['Path'].astype(str).apply(lambda x: os.path.join(Constants.image_paths['CXP'], x))\n",
    "    split['frontal'] = (split['Frontal/Lateral'] == 'Frontal')\n",
    "    if only_frontal:\n",
    "        split = split[split['frontal']]\n",
    "    split['env'] = 'CXP'\n",
    "    split['study_id'] = split['path'].apply(lambda x: x[x.index('patient'):x.rindex('/')])\n",
    "    return split[['subject_id','path','Sex',\"Age\", 'env', 'frontal','study_id'] + Constants.take_labels]\n",
    "\n",
    "\n",
    "def process_PAD(split, only_frontal):\n",
    "    split['Age'] = np.where(split['Age'].between(0,19), 19, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(20,39), 39, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(40,59), 59, split['Age'])\n",
    "    split['Age'] = np.where(split['Age'].between(60,79), 79, split['Age'])\n",
    "    split['Age'] = np.where(split['Age']>=80, 81, split['Age'])\n",
    "    \n",
    "    split = split.replace([[None], -1, \"[False]\", \"[True]\", \"[ True]\", 19, 39, 59, 79, 81], \n",
    "                            [0, 0, 0, 1, 1, \"0-20\", \"20-40\", \"40-60\", \"60-80\", \"80-\"])\n",
    "    \n",
    "    split.loc[split['Age'] == 0.0, 'Age'] = '0-20'\n",
    "    split.loc[split['Age'].isnull(), 'Age'] = '0-20'\n",
    "    split = split.rename(columns = {\n",
    "        'PatientID': 'subject_id',\n",
    "        'StudyID': 'study_id',\n",
    "        'PatientSex_DICOM' :'Sex'        \n",
    "    })\n",
    "    \n",
    "    split.loc[~split['Sex'].isin(['M', 'F', 'O']), 'Sex'] = 'O'\n",
    "    split['path'] =  split['ImageID'].astype(str).apply(lambda x: os.path.join(Constants.image_paths['PAD'], x))\n",
    "    if only_frontal:\n",
    "        split = split[split['frontal']]\n",
    "    split['env'] = 'PAD'\n",
    "    return split[['subject_id','path','Sex',\"Age\", 'env', 'frontal','study_id'] + Constants.take_labels]\n",
    "\n",
    "\n",
    "def split(df, split_portions = (0.8, 0.9), seed=0):\n",
    "    # We don't want the data splits to be affected by seed\n",
    "    # So lets temporarily set the seed to a static value...\n",
    "    rand_state = np.random.get_state()\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Split our data (irrespective of the random seed provided in train.py)\n",
    "    subject_df = pd.DataFrame({'subject_id': np.sort(df['subject_id'].unique())})\n",
    "    subject_df['random_number'] = np.random.uniform(size=len(subject_df))\n",
    "\n",
    "    train_id = subject_df[subject_df['random_number'] <= split_portions[0]].drop(columns=['random_number'])\n",
    "    valid_id = subject_df[(subject_df['random_number'] > split_portions[0]) & (subject_df['random_number'] <= split_portions[1])].drop(columns=['random_number'])\n",
    "    test_id = subject_df[subject_df['random_number'] > split_portions[1]].drop(columns=['random_number'])\n",
    "\n",
    "    train_df = df[df.subject_id.isin(train_id.subject_id)]\n",
    "    valid_df = df[df.subject_id.isin(valid_id.subject_id)]\n",
    "    test_df = df[df.subject_id.isin(test_id.subject_id)]  \n",
    "\n",
    "    # ...then return the random state back to what it was\n",
    "    np.random.set_state(rand_state)\n",
    "\n",
    "    return train_df, valid_df, test_df\n",
    "\n",
    "def get_process_func(env):\n",
    "    if env == 'MIMIC':\n",
    "        return process_MIMIC\n",
    "    elif env == 'NIH':\n",
    "        return process_NIH\n",
    "    elif env == 'CXP':\n",
    "        return process_CXP\n",
    "    elif env == 'PAD':\n",
    "        return process_PAD\n",
    "    else:\n",
    "        raise NotImplementedError   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show data paths from constants\n",
    "Constants.df_paths\n",
    "\n",
    "def img_exists(path):\n",
    "    return exists(path)\n",
    "\n",
    "def is_diseased(row):\n",
    "    # diseases = Constants.take_labels[1:]\n",
    "    return int((row[Constants.take_labels[1:]]).sum() > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cell is pre-processing the data and will take a long time to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below needs to run once, after that everything is saved into the CSV file and can be loaded from there. this block of code needs to re-run only if the data changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This might take a while.\n",
      "Processing: MIMIC\n",
      "Got processing function, filtering by only frontal...\n",
      "Filtering out the data without images...\n",
      "True     230693\n",
      "False        18\n",
      "Name: img_exists, dtype: int64\n",
      "Adding \"All\" column...\n",
      "Saving results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>env</th>\n",
       "      <th>frontal</th>\n",
       "      <th>study_id</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>img_exists</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>50414267</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>53189527</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000032</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>53911762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10000032</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>53911762</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10000032</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>56699142</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357521</th>\n",
       "      <td>19999733</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...</td>\n",
       "      <td>F</td>\n",
       "      <td>0-20</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>57132437</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357522</th>\n",
       "      <td>19999733</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...</td>\n",
       "      <td>F</td>\n",
       "      <td>0-20</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>57132437</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357524</th>\n",
       "      <td>19999987</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>55368167</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357525</th>\n",
       "      <td>19999987</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>58621812</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357526</th>\n",
       "      <td>19999987</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>MIMIC</td>\n",
       "      <td>True</td>\n",
       "      <td>58971208</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230693 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id                                               path Sex  \\\n",
       "0        10000032  C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...   F   \n",
       "2        10000032  C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...   F   \n",
       "4        10000032  C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...   F   \n",
       "5        10000032  C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...   F   \n",
       "6        10000032  C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...   F   \n",
       "...           ...                                                ...  ..   \n",
       "357521   19999733  C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...   F   \n",
       "357522   19999733  C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...   F   \n",
       "357524   19999987  C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...   F   \n",
       "357525   19999987  C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...   F   \n",
       "357526   19999987  C:\\Nina\\e-root\\data\\mimic\\physionet.org\\files\\...   F   \n",
       "\n",
       "          Age    env  frontal  study_id  No Finding  Atelectasis  \\\n",
       "0       40-60  MIMIC     True  50414267         1.0          0.0   \n",
       "2       40-60  MIMIC     True  53189527         1.0          0.0   \n",
       "4       40-60  MIMIC     True  53911762         1.0          0.0   \n",
       "5       40-60  MIMIC     True  53911762         1.0          0.0   \n",
       "6       40-60  MIMIC     True  56699142         1.0          0.0   \n",
       "...       ...    ...      ...       ...         ...          ...   \n",
       "357521   0-20  MIMIC     True  57132437         1.0          0.0   \n",
       "357522   0-20  MIMIC     True  57132437         1.0          0.0   \n",
       "357524  40-60  MIMIC     True  55368167         0.0          1.0   \n",
       "357525  40-60  MIMIC     True  58621812         0.0          1.0   \n",
       "357526  40-60  MIMIC     True  58971208         0.0          1.0   \n",
       "\n",
       "        Cardiomegaly  Effusion  Pneumonia  Pneumothorax  Consolidation  Edema  \\\n",
       "0                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "2                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "4                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "5                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "6                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "...              ...       ...        ...           ...            ...    ...   \n",
       "357521           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "357522           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "357524           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "357525           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "357526           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "\n",
       "        img_exists  All  \n",
       "0             True    0  \n",
       "2             True    0  \n",
       "4             True    0  \n",
       "5             True    0  \n",
       "6             True    0  \n",
       "...            ...  ...  \n",
       "357521        True    0  \n",
       "357522        True    0  \n",
       "357524        True    1  \n",
       "357525        True    1  \n",
       "357526        True    1  \n",
       "\n",
       "[230693 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: CXP\n",
      "Got processing function, filtering by only frontal...\n",
      "Filtering out the data without images...\n",
      "True    191229\n",
      "Name: img_exists, dtype: int64\n",
      "Adding \"All\" column...\n",
      "Saving results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>env</th>\n",
       "      <th>frontal</th>\n",
       "      <th>study_id</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>img_exists</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00001/study1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00002/study2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00002/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00003/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>F</td>\n",
       "      <td>20-40</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00004/study1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223643</th>\n",
       "      <td>64736</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64736/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223644</th>\n",
       "      <td>64737</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64737/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223645</th>\n",
       "      <td>64738</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64738/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223646</th>\n",
       "      <td>64739</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64739/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223647</th>\n",
       "      <td>64740</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>M</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64740/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>191229 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id                                               path Sex  \\\n",
       "0               1  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   F   \n",
       "1               2  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   F   \n",
       "2               2  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   F   \n",
       "4               3  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   M   \n",
       "5               4  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   F   \n",
       "...           ...                                                ...  ..   \n",
       "223643      64736  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   F   \n",
       "223644      64737  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   M   \n",
       "223645      64738  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   M   \n",
       "223646      64739  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   F   \n",
       "223647      64740  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   M   \n",
       "\n",
       "          Age  env  frontal             study_id  No Finding  Atelectasis  \\\n",
       "0       60-80  CXP     True  patient00001/study1         1.0          0.0   \n",
       "1         80-  CXP     True  patient00002/study2         0.0          0.0   \n",
       "2         80-  CXP     True  patient00002/study1         0.0          0.0   \n",
       "4       40-60  CXP     True  patient00003/study1         0.0          0.0   \n",
       "5       20-40  CXP     True  patient00004/study1         1.0          0.0   \n",
       "...       ...  ...      ...                  ...         ...          ...   \n",
       "223643  40-60  CXP     True  patient64736/study1         0.0          0.0   \n",
       "223644  60-80  CXP     True  patient64737/study1         0.0          0.0   \n",
       "223645  60-80  CXP     True  patient64738/study1         0.0          0.0   \n",
       "223646  40-60  CXP     True  patient64739/study1         0.0          0.0   \n",
       "223647    80-  CXP     True  patient64740/study1         0.0          1.0   \n",
       "\n",
       "        Cardiomegaly  Effusion  Pneumonia  Pneumothorax  Consolidation  Edema  \\\n",
       "0                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "1                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "2                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "4                0.0       0.0        0.0           0.0            0.0    1.0   \n",
       "5                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "...              ...       ...        ...           ...            ...    ...   \n",
       "223643           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "223644           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "223645           1.0       0.0        0.0           0.0            0.0    1.0   \n",
       "223646           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "223647           0.0       1.0        0.0           0.0            0.0    0.0   \n",
       "\n",
       "        img_exists  All  \n",
       "0             True    0  \n",
       "1             True    0  \n",
       "2             True    0  \n",
       "4             True    1  \n",
       "5             True    0  \n",
       "...            ...  ...  \n",
       "223643        True    0  \n",
       "223644        True    0  \n",
       "223645        True    1  \n",
       "223646        True    0  \n",
       "223647        True    1  \n",
       "\n",
       "[191229 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: NIH\n",
      "Got processing function, filtering by only frontal...\n",
      "Filtering out the data without images...\n",
      "True    112120\n",
      "Name: img_exists, dtype: int64\n",
      "Adding \"All\" column...\n",
      "Saving results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>env</th>\n",
       "      <th>frontal</th>\n",
       "      <th>study_id</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>img_exists</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\chestxray8\\images\\00000001...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\chestxray8\\images\\00000001...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\chestxray8\\images\\00000001...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\chestxray8\\images\\00000002...</td>\n",
       "      <td>M</td>\n",
       "      <td>80-</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\chestxray8\\images\\00000003...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112115</th>\n",
       "      <td>30801</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\chestxray8\\images\\00030801...</td>\n",
       "      <td>M</td>\n",
       "      <td>20-40</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>30801</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112116</th>\n",
       "      <td>30802</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\chestxray8\\images\\00030802...</td>\n",
       "      <td>M</td>\n",
       "      <td>20-40</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>30802</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112117</th>\n",
       "      <td>30803</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\chestxray8\\images\\00030803...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>30803</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112118</th>\n",
       "      <td>30804</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\chestxray8\\images\\00030804...</td>\n",
       "      <td>F</td>\n",
       "      <td>20-40</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>30804</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112119</th>\n",
       "      <td>30805</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\chestxray8\\images\\00030805...</td>\n",
       "      <td>M</td>\n",
       "      <td>20-40</td>\n",
       "      <td>NIH</td>\n",
       "      <td>True</td>\n",
       "      <td>30805</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>112120 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subject_id                                               path Sex  \\\n",
       "0               1  C:\\Nina\\e-root\\data\\chestxray8\\images\\00000001...   M   \n",
       "1               1  C:\\Nina\\e-root\\data\\chestxray8\\images\\00000001...   M   \n",
       "2               1  C:\\Nina\\e-root\\data\\chestxray8\\images\\00000001...   M   \n",
       "3               2  C:\\Nina\\e-root\\data\\chestxray8\\images\\00000002...   M   \n",
       "4               3  C:\\Nina\\e-root\\data\\chestxray8\\images\\00000003...   F   \n",
       "...           ...                                                ...  ..   \n",
       "112115      30801  C:\\Nina\\e-root\\data\\chestxray8\\images\\00030801...   M   \n",
       "112116      30802  C:\\Nina\\e-root\\data\\chestxray8\\images\\00030802...   M   \n",
       "112117      30803  C:\\Nina\\e-root\\data\\chestxray8\\images\\00030803...   F   \n",
       "112118      30804  C:\\Nina\\e-root\\data\\chestxray8\\images\\00030804...   F   \n",
       "112119      30805  C:\\Nina\\e-root\\data\\chestxray8\\images\\00030805...   M   \n",
       "\n",
       "          Age  env  frontal study_id  No Finding  Atelectasis  Cardiomegaly  \\\n",
       "0       40-60  NIH     True        1       False        False          True   \n",
       "1       40-60  NIH     True        1       False        False          True   \n",
       "2       40-60  NIH     True        1       False        False          True   \n",
       "3         80-  NIH     True        2        True        False         False   \n",
       "4       60-80  NIH     True        3       False        False         False   \n",
       "...       ...  ...      ...      ...         ...          ...           ...   \n",
       "112115  20-40  NIH     True    30801       False        False         False   \n",
       "112116  20-40  NIH     True    30802        True        False         False   \n",
       "112117  40-60  NIH     True    30803        True        False         False   \n",
       "112118  20-40  NIH     True    30804        True        False         False   \n",
       "112119  20-40  NIH     True    30805        True        False         False   \n",
       "\n",
       "        Effusion  Pneumonia  Pneumothorax  Consolidation  Edema  img_exists  \\\n",
       "0          False      False         False          False  False        True   \n",
       "1          False      False         False          False  False        True   \n",
       "2           True      False         False          False  False        True   \n",
       "3          False      False         False          False  False        True   \n",
       "4          False      False         False          False  False        True   \n",
       "...          ...        ...           ...            ...    ...         ...   \n",
       "112115     False       True         False          False  False        True   \n",
       "112116     False      False         False          False  False        True   \n",
       "112117     False      False         False          False  False        True   \n",
       "112118     False      False         False          False  False        True   \n",
       "112119     False      False         False          False  False        True   \n",
       "\n",
       "        All  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "112115    1  \n",
       "112116    0  \n",
       "112117    0  \n",
       "112118    0  \n",
       "112119    0  \n",
       "\n",
       "[112120 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: PAD\n",
      "Got processing function, filtering by only frontal...\n",
      "Filtering out the data without images...\n",
      "True    99827\n",
      "Name: img_exists, dtype: int64\n",
      "Adding \"All\" column...\n",
      "Saving results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>env</th>\n",
       "      <th>frontal</th>\n",
       "      <th>study_id</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>img_exists</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>839860488694292331637988235681460987</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\PadChest\\images-224\\205366...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>20536686640136348236148679891455886468</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>313572750430997347502932654319389875966</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\PadChest\\images-224\\135803...</td>\n",
       "      <td>M</td>\n",
       "      <td>80-</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>135803415504923515076821959678074435083</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50783093527901818115346441867348318648</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\PadChest\\images-224\\113855...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>113855343774216031107737439268243531979</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>93535126770783451980359712286922420997</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\PadChest\\images-224\\313723...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>3137231742710829928-247610802266403640553</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>93535126770783451980359712286922420997</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\PadChest\\images-224\\313723...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>313723174271082992847610802266403640553</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144479</th>\n",
       "      <td>112930952416074060371371014599496493673</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\PadChest\\images-224\\128401...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>1284011361929414522814654121696751542351444145...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144480</th>\n",
       "      <td>282743729971423358706056731890510600934</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\PadChest\\images-224\\128401...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>1284011361929414522094646571696751542351444145...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144481</th>\n",
       "      <td>52648743308541843883453242716226652771</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\PadChest\\images-224\\128401...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>1284011361929414522086390631696751542351444145...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144482</th>\n",
       "      <td>228646130593152933811948996634154201216</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\PadChest\\images-224\\128401...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>1284011361929414522084108901696751542351444145...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144483</th>\n",
       "      <td>137424047230303610602080410284588825286</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\PadChest\\images-224\\128401...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>PAD</td>\n",
       "      <td>True</td>\n",
       "      <td>1284011361929414524682119191696751542351444145...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99827 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     subject_id  \\\n",
       "0          839860488694292331637988235681460987   \n",
       "2       313572750430997347502932654319389875966   \n",
       "3        50783093527901818115346441867348318648   \n",
       "6        93535126770783451980359712286922420997   \n",
       "7        93535126770783451980359712286922420997   \n",
       "...                                         ...   \n",
       "144479  112930952416074060371371014599496493673   \n",
       "144480  282743729971423358706056731890510600934   \n",
       "144481   52648743308541843883453242716226652771   \n",
       "144482  228646130593152933811948996634154201216   \n",
       "144483  137424047230303610602080410284588825286   \n",
       "\n",
       "                                                     path Sex    Age  env  \\\n",
       "0       C:\\Nina\\e-root\\data\\PadChest\\images-224\\205366...   F    80-  PAD   \n",
       "2       C:\\Nina\\e-root\\data\\PadChest\\images-224\\135803...   M    80-  PAD   \n",
       "3       C:\\Nina\\e-root\\data\\PadChest\\images-224\\113855...   F    80-  PAD   \n",
       "6       C:\\Nina\\e-root\\data\\PadChest\\images-224\\313723...   M  60-80  PAD   \n",
       "7       C:\\Nina\\e-root\\data\\PadChest\\images-224\\313723...   M  60-80  PAD   \n",
       "...                                                   ...  ..    ...  ...   \n",
       "144479  C:\\Nina\\e-root\\data\\PadChest\\images-224\\128401...   M  60-80  PAD   \n",
       "144480  C:\\Nina\\e-root\\data\\PadChest\\images-224\\128401...   F  60-80  PAD   \n",
       "144481  C:\\Nina\\e-root\\data\\PadChest\\images-224\\128401...   M  40-60  PAD   \n",
       "144482  C:\\Nina\\e-root\\data\\PadChest\\images-224\\128401...   F  60-80  PAD   \n",
       "144483  C:\\Nina\\e-root\\data\\PadChest\\images-224\\128401...   M  60-80  PAD   \n",
       "\n",
       "        frontal                                           study_id  \\\n",
       "0          True             20536686640136348236148679891455886468   \n",
       "2          True            135803415504923515076821959678074435083   \n",
       "3          True            113855343774216031107737439268243531979   \n",
       "6          True          3137231742710829928-247610802266403640553   \n",
       "7          True            313723174271082992847610802266403640553   \n",
       "...         ...                                                ...   \n",
       "144479     True  1284011361929414522814654121696751542351444145...   \n",
       "144480     True  1284011361929414522094646571696751542351444145...   \n",
       "144481     True  1284011361929414522086390631696751542351444145...   \n",
       "144482     True  1284011361929414522084108901696751542351444145...   \n",
       "144483     True  1284011361929414524682119191696751542351444145...   \n",
       "\n",
       "        No Finding  Atelectasis  Cardiomegaly  Effusion  Pneumonia  \\\n",
       "0                1            0             0         0          0   \n",
       "2                0            0             0         0          0   \n",
       "3                0            0             0         0          0   \n",
       "6                0            1             0         1          0   \n",
       "7                0            0             0         1          0   \n",
       "...            ...          ...           ...       ...        ...   \n",
       "144479           0            0             0         0          0   \n",
       "144480           1            0             0         0          0   \n",
       "144481           0            0             0         0          0   \n",
       "144482           1            0             0         0          0   \n",
       "144483           0            0             0         0          1   \n",
       "\n",
       "        Pneumothorax  Consolidation  Edema  img_exists  All  \n",
       "0                  0              0      0        True    0  \n",
       "2                  0              0      0        True    0  \n",
       "3                  0              0      0        True    0  \n",
       "6                  0              0      0        True    1  \n",
       "7                  0              0      0        True    1  \n",
       "...              ...            ...    ...         ...  ...  \n",
       "144479             0              0      0        True    0  \n",
       "144480             0              0      0        True    0  \n",
       "144481             0              0      0        True    0  \n",
       "144482             0              0      0        True    0  \n",
       "144483             0              0      0        True    1  \n",
       "\n",
       "[99827 rows x 17 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# loads data with random splits\n",
    "print('This might take a while.')\n",
    "\n",
    "for data_env in Constants.df_paths:\n",
    "    print('Processing:', data_env)\n",
    "    func = get_process_func(data_env)\n",
    "    print('Got processing function, filtering by only frontal...')\n",
    "    df_env = func(pd.read_csv(Constants.df_paths[data_env]), only_frontal = True)\n",
    "    print('Filtering out the data without images...')\n",
    "    df_env[\"img_exists\"] = df_env[\"path\"].apply(img_exists)\n",
    "    print(df_env[\"img_exists\"].value_counts())\n",
    "    df_env = df_env[df_env[\"img_exists\"]]\n",
    "    \n",
    "    df_env = df_env.fillna(0) \n",
    "    \n",
    "    print('Adding \"All\" column...')\n",
    "    df_env[\"All\"] = df_env.apply(is_diseased, axis=1)\n",
    "    \n",
    "    print('Saving results...')\n",
    "    df_env.to_csv(f\"{Constants.base_path}\\\\processed\\\\{data_env}.csv\", index=False)\n",
    "    \n",
    "    display(df_env)\n",
    "    \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing the data, splitting to all, train, val and test...\n",
      "Source: MIMIC\n",
      "Data length: 230693\n",
      "MIMIC: done.\n",
      "Source: CXP\n",
      "Data length: 191229\n",
      "CXP: done.\n",
      "Source: NIH\n",
      "Data length: 112120\n",
      "NIH: done.\n",
      "Source: PAD\n",
      "Data length: 99827\n",
      "PAD: done.\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "dfs = {}\n",
    "print('Processing the data, splitting to all, train, val and test...')\n",
    "for env in Constants.df_paths:\n",
    "    func = get_process_func(env)\n",
    "    df_env = pd.read_csv(f\"{Constants.base_path}/processed/{env}.csv\")\n",
    "    \n",
    "    print('Source:', env)\n",
    "    print('Data length:', len(df_env))\n",
    "    \n",
    "    train_df, valid_df, test_df = split(df_env)\n",
    "    dfs[env] = {\n",
    "        'all': df_env,\n",
    "        'train': train_df,\n",
    "        'val': valid_df,\n",
    "        'test': test_df\n",
    "    }\n",
    "    print(f'{env}: done.')\n",
    "    \n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prop(df, column=\"Pneumonia\"):\n",
    "    num_instances = len(df)\n",
    "    num_diseased = df[df[column] == 1][column].count()\n",
    "    return num_diseased / (num_instances - num_diseased)\n",
    "\n",
    "def get_resample_class(orig_prop, new_prop, resample_method):\n",
    "    if new_prop > orig_prop:\n",
    "        if resample_method == \"over\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    if new_prop < orig_prop:\n",
    "        if resample_method == \"under\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "def calculate_num_resample(df, orig_prop, new_prop, resample_method):\n",
    "    pass\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def balance_df_label(df, sampler, label_bal=0.05154780337262089, invert=False):\n",
    "    target = df[\"Pneumonia\"] == 1\n",
    "    rus = sampler(random_state=0, sampling_strategy=label_bal if not invert else 1-label_bal - 0.23)\n",
    "    res_df, _ = rus.fit_resample(df, target)\n",
    "\n",
    "    print(f\"Previous pneumonia prop: {get_pneumonia_prop(df)} with {len(df)} instances\")\n",
    "    print(f\"Resampled pneumonia prop: {get_pneumonia_prop(res_df)} with {len(res_df)} instances\")\n",
    "\n",
    "    return res_df\n",
    "\n",
    "def balance_proportion(orig_df, new_df, resample_method=\"over\", column=\"Pneumonia\"):\n",
    "    orig_df = orig_df.fillna(0.0)\n",
    "    orig_prop = get_prop(orig_df, column)\n",
    "    new_prop = get_prop(new_df, column)\n",
    "    assert resample_method in [\"over\", \"under\"]\n",
    "    resample_class = get_resample_class(orig_prop, new_prop, resample_method)\n",
    "    print(f\"Resampling '{column}' via '{resample_method}' on class {resample_class} from {orig_prop} to {new_prop}\")\n",
    "    \n",
    "    # Estimate the number of items we'll need to resample\n",
    "    df_diseased = orig_df[orig_df[column] == 1.0]\n",
    "    df_normal = orig_df[orig_df[column] == 0.0]\n",
    "    num_diseased = len(df_diseased)\n",
    "    num_normal = len(df_normal)\n",
    "    assert num_diseased + num_normal == len(orig_df)\n",
    "    \n",
    "    if resample_method == \"over\":\n",
    "        if resample_class == 0:\n",
    "            new_num_normal = int(num_diseased / new_prop)\n",
    "            print(f\"Resampling normal samples from {num_normal} to {new_num_normal}\")\n",
    "            df_normal_rs = df_normal.sample(new_num_normal, replace=True, random_state=0)\n",
    "            resampled_df = pd.concat([df_normal_rs, df_diseased])\n",
    "        else:\n",
    "            # Resample the pneumonia class\n",
    "            # new_num_diseased = int(new_prop * num_normal)\n",
    "            # print(f\"Resampling diseased samples from {num_diseased} to {new_num_diseased}\")\n",
    "            # df_diseased_rs = df_diseased.sample(new_num_diseased, replace=True, random_state=0)\n",
    "            # resampled_df = pd.concat([df_normal, df_diseased_rs])\n",
    "            target = df[\"Pneumonia\"] == 1\n",
    "            rus = RandomOverSampler(random_state=0, sampling_strategy=new_prop)\n",
    "            resampled_df, _ = rus.fit_resample(df, target)\n",
    "    \n",
    "    resampled_df.sort_index(inplace=True)\n",
    "    print(f\"New df proportion: {get_prop(resampled_df, column)}\")\n",
    "    return resampled_df\n",
    "            \n",
    "# balance_proportion(dfs[\"MIMIC\"][\"train\"], dfs[\"MIMIC\"][\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>path</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>env</th>\n",
       "      <th>frontal</th>\n",
       "      <th>study_id</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Atelectasis</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Effusion</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pneumothorax</th>\n",
       "      <th>Consolidation</th>\n",
       "      <th>Edema</th>\n",
       "      <th>img_exists</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00001/study1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00002/study2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>F</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00002/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00003/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>F</td>\n",
       "      <td>20-40</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient00004/study1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191222</th>\n",
       "      <td>64734</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>M</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64734/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191223</th>\n",
       "      <td>64735</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>F</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64735/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191225</th>\n",
       "      <td>64737</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>M</td>\n",
       "      <td>60-80</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64737/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191227</th>\n",
       "      <td>64739</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>F</td>\n",
       "      <td>40-60</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64739/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191228</th>\n",
       "      <td>64740</td>\n",
       "      <td>C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...</td>\n",
       "      <td>M</td>\n",
       "      <td>80-</td>\n",
       "      <td>CXP</td>\n",
       "      <td>True</td>\n",
       "      <td>patient64740/study1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>153411 rows Ã— 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        subject_id                                               path Sex  \\\n",
       "0                1  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   F   \n",
       "1                2  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   F   \n",
       "2                2  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   F   \n",
       "3                3  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   M   \n",
       "4                4  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   F   \n",
       "...            ...                                                ...  ..   \n",
       "191222       64734  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   M   \n",
       "191223       64735  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   F   \n",
       "191225       64737  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   M   \n",
       "191227       64739  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   F   \n",
       "191228       64740  C:\\Nina\\e-root\\data\\CheXpert\\CheXpert-v1.0-sma...   M   \n",
       "\n",
       "          Age  env  frontal             study_id  No Finding  Atelectasis  \\\n",
       "0       60-80  CXP     True  patient00001/study1         1.0          0.0   \n",
       "1         80-  CXP     True  patient00002/study2         0.0          0.0   \n",
       "2         80-  CXP     True  patient00002/study1         0.0          0.0   \n",
       "3       40-60  CXP     True  patient00003/study1         0.0          0.0   \n",
       "4       20-40  CXP     True  patient00004/study1         1.0          0.0   \n",
       "...       ...  ...      ...                  ...         ...          ...   \n",
       "191222  40-60  CXP     True  patient64734/study1         0.0          1.0   \n",
       "191223  60-80  CXP     True  patient64735/study1         0.0          1.0   \n",
       "191225  60-80  CXP     True  patient64737/study1         0.0          0.0   \n",
       "191227  40-60  CXP     True  patient64739/study1         0.0          0.0   \n",
       "191228    80-  CXP     True  patient64740/study1         0.0          1.0   \n",
       "\n",
       "        Cardiomegaly  Effusion  Pneumonia  Pneumothorax  Consolidation  Edema  \\\n",
       "0                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "1                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "2                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "3                0.0       0.0        0.0           0.0            0.0    1.0   \n",
       "4                0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "...              ...       ...        ...           ...            ...    ...   \n",
       "191222           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "191223           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "191225           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "191227           0.0       0.0        0.0           0.0            0.0    0.0   \n",
       "191228           0.0       1.0        0.0           0.0            0.0    0.0   \n",
       "\n",
       "        img_exists  All  \n",
       "0             True    0  \n",
       "1             True    0  \n",
       "2             True    0  \n",
       "3             True    1  \n",
       "4             True    0  \n",
       "...            ...  ...  \n",
       "191222        True    1  \n",
       "191223        True    1  \n",
       "191225        True    0  \n",
       "191227        True    0  \n",
       "191228        True    1  \n",
       "\n",
       "[153411 rows x 17 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[\"CXP\"][\"train\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LjW9bCkouv8O"
   },
   "source": [
    "### Metrics to evaluate my model\n",
    "\n",
    "Similar to the original paper, for each base hospital I plan to choose one additional hospital to include in evaluation (for example, evaluate a model trained on MIMIC data using MIMIC and PAD data).\n",
    "\n",
    "* analyse accuracies within each class for each hospital - the result is a group for the disease class from hospital A, the non-disease class from hospital A, the disease class from hospital B, and the non-disease class from hospital B\n",
    "* Track the worst accuracy of the four groups\n",
    "* Compute AUROC\n",
    "\n",
    "I plan to plot the results and compare them to the results provided in the paper.\n",
    "\n",
    "Since I wasn't yet able to fully complete the previous steps, and instead am stuck on the training portion, this section is a ToDo. I plan to complete it by week of April 21.\n",
    "\n",
    "There are two alternative approaches I can take, depending on the situation:\n",
    "\n",
    "* If I manage to run the original paper code, then this is what I will do, since it should be closest to the original paper\n",
    "* If I won't be able to run the original paper training and validation code on my machine, I will update the code I wrote for training and validation to take it as close as possible to the intent of the original researchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balancing...\n",
      "Previous pneumonia prop: 0.07293672707023365 with 184970 instances\n",
      "Resampled pneumonia prop: 0.7184505440961507 with 296254 instances\n",
      "Previous pneumonia prop: 0.02491281516815649 with 153411 instances\n",
      "Resampled pneumonia prop: 0.051542603653077855 with 157397 instances\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def balance_df_label(df, sampler, label_bal=0.05154780337262089, invert=False):\n",
    "    target = df[\"Pneumonia\"] == (1 if not invert else 0)\n",
    "    rus = sampler(random_state=42, sampling_strategy=label_bal if not invert else 1-label_bal - 0.23)\n",
    "    res_df, _ = rus.fit_resample(df, target)\n",
    "\n",
    "    print(f\"Previous pneumonia prop: {get_prop(df)} with {len(df)} instances\")\n",
    "    print(f\"Resampled pneumonia prop: {get_prop(res_df)} with {len(res_df)} instances\")\n",
    "\n",
    "    return res_df\n",
    "\n",
    "print('Balancing...')\n",
    "mimic_balanced = balance_df_label(dfs[\"MIMIC\"][\"train\"], RandomOverSampler, invert=True)\n",
    "cxp_balanced = balance_df_label(dfs[\"CXP\"][\"train\"], RandomOverSampler, invert=False)\n",
    "print('Done.')\n",
    "\n",
    "# # Balance the size of the two datasets\n",
    "# n = len(cxp_balanced)\n",
    "# mimic_balanced = mimic_balanced.sample(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'MIMIC'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-e5eae8654dc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#keep only every 20th sample for the dataset to reduce the size of the data to train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m30\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdfs2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'all'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtrain_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'MIMIC'"
     ]
    }
   ],
   "source": [
    "stat_rows = []\n",
    "num_instances = []\n",
    "\n",
    "disease_labels = [\"Pneumonia\", \"Cardiomegaly\", \"Edema\", \"Effusion\", \"Atelectasis\", \"Pneumothorax\", \"Consolidation\"]\n",
    "target_labels = disease_labels + [\"Any\", \"No Finding\"]\n",
    "all_labels = target_labels + [\"Num Instances\"]\n",
    "\n",
    "dfs2 = {}\n",
    "\n",
    "for env in dfs:\n",
    "    df = dfs[env]['all']\n",
    "    df['Any'] = (df[disease_labels] > 0).any(axis=1).astype(int)\n",
    "    \n",
    "    #keep only every 20th sample for the dataset to reduce the size of the data to train\n",
    "    df2 = df[df.index % 30 == 0]\n",
    "    dfs2[env] = {}\n",
    "    dfs2[env]['all'] = df2\n",
    "    \n",
    "    train_df, valid_df, test_df = split(df2)\n",
    "    dfs2[env] = {\n",
    "        'all': df_env,\n",
    "        'train': train_df,\n",
    "        'val': valid_df,\n",
    "        'test': test_df\n",
    "    }\n",
    "    \n",
    "    totals = {}\n",
    "    totals['Dataset'] = env\n",
    "#     totals['Num Instances'] = len(df)\n",
    "    totals['Num Instances'] = len(df2)\n",
    "    num_instances.append(totals['Num Instances'])\n",
    "\n",
    "    for label in target_labels:\n",
    "#         if label in df.columns:\n",
    "#             totals[label] = df[label].sum() / len(df)\n",
    "        if label in df2.columns:\n",
    "            totals[label] = df2[label].sum() / len(df2)\n",
    "        else:\n",
    "            totals[label] = 0.0\n",
    "\n",
    "    stat_rows.append(totals)\n",
    "\n",
    "stat_df = pd.DataFrame(stat_rows)\n",
    "stat_df.set_index('Dataset', inplace=True)\n",
    "\n",
    "ordered_cols = all_labels\n",
    "stat_df = stat_df[ordered_cols]\n",
    "\n",
    "transposed_stat_df = stat_df.T\n",
    "\n",
    "# styled_stat_df = stat_df.style.background_gradient(cmap='Blues', subset=target_labels)\\\n",
    "#     .format({label: \"{:.2%}\" for label in target_labels})\n",
    "\n",
    "styled_transposed_stat_df = transposed_stat_df.style.apply(\n",
    "    lambda x: [\"background-color: lightblue\" if x.name != 'Num Instances' else \"background-color: none\" for i in x],\n",
    "    axis=1\n",
    ").background_gradient(cmap='Blues', subset=pd.IndexSlice[target_labels, :])\n",
    "styled_transposed_stat_df = styled_transposed_stat_df.format(\"{:.2%}\", subset=pd.IndexSlice[target_labels, :])\n",
    "styled_transposed_stat_df = styled_transposed_stat_df.format(\"{:,.0f}\", subset=pd.IndexSlice['Num Instances', :])\n",
    "\n",
    "styled_transposed_stat_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the table from the article for comparison:\n",
    "\n",
    "![Table 1](Table_1_article.png)\n",
    "\n",
    "Looks like the distribution of the labels in the original dataset, while not the same, still is close enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": [
    "##   Model\n",
    "The model includes the model definitation which usually is a class, model training, and other necessary parts.\n",
    "\n",
    "### Model architecture\n",
    "In the article, the authors use the same model architecture as Zhang et al. (2021): a **DenseNet-121** network (Huang et al., 2017) **initialized with pre-trained weights from ImageNet** (Deng et al., 2009). The final layer is replaced with a **two-output linear layer** (for binary classification). For simplicity, the authors only consider binary disease classification.\n",
    "\n",
    "### Model Training\n",
    "For training the network, all images are resized to **224 Ã— 224** and normalized to the ImageNet (Deng et al., 2009) mean and standard deviation.\n",
    "\n",
    "During training, the following image augmentations are applied:\n",
    "* random horizontal flip\n",
    "* random rotation up to 10 degrees\n",
    "* a crop of random size (75% - 100%) and aspect ratio (3/4 to 4/3)\n",
    "\n",
    "All runs use **Adam** with **lr = 1e-5** and **batch size = 128**, which was found to be a performant configuration in early tuning ((Zhang et al., 2021) use lr = 5e-4 and batch size = 32).\n",
    "\n",
    "_[This part I haven't implemented yet]_ Training runs for **a maximum of 20k steps**, with validation occurring every 500 steps and an early stopping patience of 10 validations.\n",
    "\n",
    "All test results are obtained using the optimal model found during training as measured by the highest validation macro-F1 score (following (Fiorillo et al., 2021; Berenguer et al., 2022)) as it gives a robust ranking of model performance under imbalanced labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details for model info:\n",
    "    \n",
    "  * Model architecture: layer number/size/type, activation function, etc\n",
    "  * Training objectives: loss function, optimizer, weight of each loss term, etc\n",
    "  * Others: whether the model is pretrained, Monte Carlo simulation for uncertainty analysis, etc\n",
    "  * The code of model should have classes of the model, functions of model training, model validation, etc.\n",
    "  * If your model training is done outside of this notebook, please upload the trained model here and develop a function to load and test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gBdVZoTvsSFV"
   },
   "outputs": [],
   "source": [
    "# This is the model defined and provided by the autors of the article.\n",
    "# While they are using densenet 121 for the article, the provided model code includes other options.\n",
    "\n",
    "class EmbModel(nn.Module):\n",
    "    # I had to add the num_labels parameter to reduce the resulting response to the number of labels we use\n",
    "    def __init__(self, emb_type, feature_size_override, pretrain, concat_features = 0, num_labels = 8):\n",
    "        super().__init__()\n",
    "        self.emb_type = emb_type\n",
    "        self.pretrain = pretrain\n",
    "        self.concat_features = concat_features\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        assert emb_type in [\"densenet121\", \"densenet201\", \"resnet\"], f\"Invalid emb type: {emb_type}\"\n",
    "\n",
    "        if emb_type == 'densenet121':\n",
    "            model = models.densenet121()\n",
    "            self.encoder = nn.Sequential(*list(model.children())[:-1]) #https://discuss.pytorch.org/t/densenet-transfer-learning/7776/2\n",
    "            self.emb_dim = model.classifier.in_features\n",
    "        elif emb_type == 'densenet201':\n",
    "            model = models.densenet201()\n",
    "            self.encoder = nn.Sequential(*list(model.children())[:-1]) #https://discuss.pytorch.org/t/densenet-transfer-learning/7776/2\n",
    "            self.emb_dim = model.classifier.in_features\n",
    "        elif emb_type == 'resnet':\n",
    "            model = models.resnet50()\n",
    "            self.encoder = nn.Sequential(*list(model.children())[:-1])\n",
    "            self.emb_dim = list(model.children())[-1].in_features\n",
    "\n",
    "        print(\"\\nEmb Dim:\")\n",
    "        print(self.emb_dim)\n",
    "\n",
    "        if feature_size_override:\n",
    "            print(f\"Manually setting output dim to {feature_size_override}\")\n",
    "            self.emb_dim = feature_size_override\n",
    "            print(self.emb_dim)\n",
    "            \n",
    "        self.n_outputs = self.emb_dim + concat_features\n",
    "        self.final_layer = nn.Linear(self.n_outputs, self.num_labels)\n",
    "        \n",
    "        nn.init.kaiming_normal_(self.final_layer.weight, mode='fan_out', nonlinearity='sigmoid')\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        if isinstance(inp, dict): # dict with image and additional feature(s) to concat to embedding\n",
    "            x = inp['img']\n",
    "            concat = inp['concat']\n",
    "            assert(concat.shape[-1] == self.concat_features)\n",
    "        else: # tensor image\n",
    "            assert(self.concat_features == 0)\n",
    "            x = inp\n",
    "        \n",
    "        x = self.encoder(x).squeeze(-1).squeeze(-1)\n",
    "        if \"densenet\" in self.emb_type:\n",
    "            x = F.relu(x)\n",
    "            x = F.avg_pool2d(x, kernel_size = 7).view(x.size(0), -1)\n",
    "        \n",
    "        if isinstance(inp, dict):\n",
    "            x = torch.cat([x, concat], dim = -1)\n",
    "            \n",
    "        x = self.final_layer(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "Haven't figured out how to make the training from the supplied code work yet, so I am writing my own training code using the standard approach learned in class and homeworks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a data loader\n",
    "The authors of the article have a script to load the data in different configurations. I am reusing it partially but wasn't able to make it work yet because of the errors, so I am creating my own Dataset class and a data loader that can deal with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEnvDataset(Dataset):\n",
    "    def __init__(self, dataframes, subset='train', envs=None, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the dataset with data from multiple environments and a specific subset.\n",
    "        :param dataframes: A dictionary with environment keys, each containing another dict with subsets as DataFrames.\n",
    "        :param subset: The subset to load ('train', 'val', or 'test').\n",
    "        :param envs: A list of environment names to include. If None, include all.\n",
    "        :param transform: PyTorch transforms to apply to the images.\n",
    "        \"\"\"\n",
    "        if envs is None:\n",
    "            envs = list(dataframes.keys())\n",
    "        \n",
    "        self.data = pd.concat([dataframes[env][subset] for env in envs if env in dataframes], ignore_index=True)\n",
    "        \n",
    "        self.label_columns = [\"No Finding\", \"Atelectasis\", \"Cardiomegaly\", \"Effusion\", \"Pneumonia\", \n",
    "                              \"Pneumothorax\", \"Consolidation\", \"Edema\"]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.data.iloc[idx]['path']\n",
    "        image = Image.open(img_path).convert('RGB')  # Convert to RGB to handle potential grayscale images\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "\n",
    "        labels = Tensor(self.data.iloc[idx][self.label_columns].values.astype(float), device='cuda').cuda()\n",
    "        if torch.isnan(labels).any():\n",
    "            raise ValueError(\"NaN values found in labels\")\n",
    "        \n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# train_dataset = MultiEnvDataset(dfs, subset='val', envs=['MIMIC', 'CXP'], transform=transform)\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# val_dataset = MultiEnvDataset(dfs, subset='test', envs=['MIMIC', 'CXP'], transform=transform)\n",
    "# val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "\n",
    "# for images, labels in train_loader:\n",
    "#     print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs_list = [[\"CXP\"], [\"MIMIC\"], [\"NIH\"], [\"PAD\"], [\"CXP\",\"NIH\"], [\"CXP\",\"PAD\"], [\"MIMIC\",\"CXP\"], [\"MIMIC\",\"NIH\"], [\"MIMIC\",\"PAD\"], [\"NIH\",\"PAD\"], [\"CXP\", \"MIMIC\", \"NIH\", \"PAD\"]]\n",
    "datasets = []\n",
    "for env in envs_list:\n",
    "    elem = {\n",
    "        \"env\": env\n",
    "    }\n",
    "    for subset in [\"train\", \"val\"]:\n",
    "        elem[subset] = {}\n",
    "        elem[subset][\"dataset\"] = MultiEnvDataset(dfs, subset=subset, envs=env, transform=transform)\n",
    "        elem[subset][\"loader\"] = torch.utils.data.DataLoader(elem[subset][\"dataset\"], batch_size=128, shuffle=True)\n",
    "    datasets.append(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have not been able to make the training work yet, see the issue below. The original paper provides separate scripts to do the training, which require some packages that seem to be not compatible with my platform. Still figuring out how to either make the original scripts work, or write my own training in a way that it provides results similar to the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Emb Dim:\n",
      "1024\n",
      "Manually setting output dim to 1024\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "model = EmbModel(emb_type=\"densenet121\", feature_size_override=1024, pretrain=False, num_labels=8)\n",
    "model.cuda()\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "max_batches = 10\n",
    "\n",
    "def calculate_accuracies(outputs, labels):\n",
    "    sigmoids = torch.sigmoid(outputs) > 0.5\n",
    "    sigmoids = sigmoids.to(labels.device)\n",
    "    correct_pred = (sigmoids == labels)\n",
    "    accuracies = correct_pred.float().mean(axis=0)\n",
    "    return accuracies\n",
    "\n",
    "def calculate_f1(outputs, labels):\n",
    "    predictions = torch.sigmoid(outputs) > 0.5\n",
    "    predictions = predictions.to(labels.device)\n",
    "\n",
    "    predictions = predictions.detach().cpu().numpy()\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "\n",
    "    f1 = f1_score(labels, predictions, average=None)\n",
    "    return f1\n",
    "\n",
    "def train_model_one_epoch(model, train_loader, loss_func, optimizer):\n",
    "    print(\"Starting training...\")\n",
    "    start = datetime.now()\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    total_accuracy = []\n",
    "    total_f1_scores = []\n",
    "    \n",
    "    print('number of batches:', len(train_loader))\n",
    "    for batch, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.cuda()\n",
    "        if batch % 100 == 0:\n",
    "            mid = datetime.now()\n",
    "            print('batch', batch, 'time passed:', mid-start)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        if torch.isnan(outputs).any():\n",
    "            raise ValueError(\"NaN detected in model outputs\")\n",
    "\n",
    "        loss = loss_func(outputs, labels)\n",
    "        if torch.isnan(loss).any():\n",
    "            raise ValueError(\"NaN detected in loss computation\")\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        accuracies = calculate_accuracies(outputs, labels)\n",
    "        f1_scores = calculate_f1(outputs, labels)\n",
    "        total_accuracy.append(accuracies)\n",
    "        total_f1_scores.append(f1_scores)\n",
    "        \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    end = datetime.now()\n",
    "    print(\"epoch done in\", end-start, \"number of batches:\", batch)\n",
    "    epoch_accuracy = torch.stack(total_accuracy).mean(dim=0)\n",
    "    epoch_f1 = torch.tensor(total_f1_scores).mean(dim=0)\n",
    "    return epoch_loss, epoch_accuracy, epoch_f1\n",
    "\n",
    "def validate_model(model, val_loader, loss_func):\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    total_accuracy = []\n",
    "    total_f1_scores = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.cuda()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            accuracies = calculate_accuracies(outputs, labels)\n",
    "            f1_scores = calculate_f1(outputs, labels)\n",
    "            total_accuracy.append(accuracies)\n",
    "            total_f1_scores.append(f1_scores)\n",
    "            \n",
    "    epoch_loss = running_loss / len(val_loader.dataset)\n",
    "    epoch_accuracy = torch.stack(total_accuracy).mean(dim=0)\n",
    "    epoch_f1 = torch.tensor(total_f1_scores).mean(dim=0)\n",
    "    return epoch_loss, epoch_accuracy, epoch_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset env: ['CXP']\n",
      "Starting training...\n",
      "number of batches: 147\n",
      "batch 0 time passed: 0:00:01.297831\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-05b548381b02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Processing dataset env:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"env\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"val\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loader\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidate_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"test\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"loader\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-28-758e8f089a60>\u001b[0m in \u001b[0;36mtrain_model_one_epoch\u001b[1;34m(model, train_loader, loss_func, optimizer)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"NaN detected in loss computation\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\gpu\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epoch = 10\n",
    "        \n",
    "metrics_df = pd.DataFrame(columns=[\"epoch\", \"train_loss\", \"valid_loss\", \"train_accuracy\", \"valid_accuracy\", \"train_f1\", \"valid_f1\"])\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(\"Processing dataset env:\", dataset[\"env\"])\n",
    "    for i in range(num_epoch):\n",
    "        train_loss, train_accuracy, train_f1 = train_model_one_epoch(model, dataset[\"val\"][\"loader\"], loss_func, optimizer)\n",
    "        valid_loss, valid_accuracy, valid_f1 = validate_model(model, dataset[\"test\"][\"loader\"], loss_func)\n",
    "        \n",
    "        print(\"Epoch: %.2f, Train Loss: %.2f, Validation Loss: %.2f\" % (i+1, train_loss, valid_loss))\n",
    "        \n",
    "        # Convert tensors to CPU for DataFrame update\n",
    "        train_accuracy = train_accuracy.cpu().numpy()\n",
    "        valid_accuracy = valid_accuracy.cpu().numpy()\n",
    "        train_f1 = train_f1.cpu().numpy()\n",
    "        valid_f1 = valid_f1.cpu().numpy()\n",
    "\n",
    "        # Append metrics to DataFrame\n",
    "        metrics_df = metrics_df.append({\n",
    "            \"epoch\": i + 1,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"valid_loss\": valid_loss,\n",
    "            \"train_accuracy\": np.mean(train_accuracy),\n",
    "            \"valid_accuracy\": np.mean(valid_accuracy),\n",
    "            \"worst_train_accuracy\": np.min(train_accuracy),\n",
    "            \"worst_valid_accuracy\": np.min(valid_accuracy),\n",
    "            \"train_f1\": np.mean(train_f1),\n",
    "            \"valid_f1\": np.mean(valid_f1)\n",
    "        }, ignore_index=True)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "metrics_df.to_csv(f\"{Constants.base_path}/training_metrics.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epoch = 10\n",
    "# # model training loop: it is better to print the training/validation losses during the training\n",
    "# for i in range(num_epoch):\n",
    "#     train_loss = train_model_one_epoch(model, train_loader, loss_func, optimizer)\n",
    "#     valid_loss = validate_model(model, val_loader, loss_func)\n",
    "#     print(\"Epoch: %.2f, Train Loss: %.2f, Validation Loss: %.2f\" % (i+1, train_loss, valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "\n",
    "torch.save(model.state_dict(), \"model/model-snapshot-\" + dt_string + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ml_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-b0eea71e6f77>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresults_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[0mrow_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-41-b0eea71e6f77>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[0menv_pred_vals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"CXP\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"MIMIC\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"NIH\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"PAD\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"CXP\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"NIH\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"CXP\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"PAD\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"MIMIC\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"CXP\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"MIMIC\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"NIH\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"MIMIC\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"PAD\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"NIH\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"PAD\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"CXP\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"MIMIC\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"NIH\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"PAD\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     train_models(df=ml_df, \n\u001b[0m\u001b[0;32m     86\u001b[0m                  \u001b[0mpred_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv_pred_col\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                  \u001b[0mpred_vals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0menv_pred_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ml_df' is not defined"
     ]
    }
   ],
   "source": [
    "# This block of code doesn't work yet, I am currently figuring out how to make it work. Use the training block in the cell above (it is very slow-running though)\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import wandb\n",
    "import json\n",
    "\n",
    "def train_models(df, pred_col, pred_vals, fix_col, fix_vals, results_dict, task_type):   \n",
    "    for fix_val in tqdm(fix_vals):\n",
    "        # Subset the dataframe to just have this val in the column\n",
    "        df_fix = df[df[fix_col] == fix_val]\n",
    "        # Drop this column to avoid any oddities during training\n",
    "        df_fix = df_fix.drop(columns=fix_col)\n",
    "        \n",
    "        # Get just the 2/4 classes that we're trying to \n",
    "        for pred_val in tqdm(pred_vals):\n",
    "            df_pred = df_fix[df_fix[pred_col].isin(pred_val)]\n",
    "            \n",
    "            # We have the final dataframe, but we need to create a perfectly balanced \n",
    "            # version of it\n",
    "            grouped = df_pred.groupby(pred_col)\n",
    "            # print(\"Count per class:\", grouped[\"emb0\"].count())\n",
    "            min_group_size = grouped.count()[\"emb0\"].min()\n",
    "            df_bal = grouped.sample(n=min_group_size, random_state=0)\n",
    "            # print(\"Count per class after balancing:\", df_bal.groupby(pred_col)[\"emb0\"].count())\n",
    "            \n",
    "            # Note that we may have a single class remaining in our dataset (if we're doing the baseline \n",
    "            # CXP vs CXP prediction, for example). We need to check that and manually change our dataset\n",
    "            # If that is the case\n",
    "            df_bal = df_bal.sample(frac=1, random_state=0).reset_index(drop=True)\n",
    "            \n",
    "            if len(pred_val) == 1:\n",
    "                print(f\"INFO: SINGLE PRED VAL: {pred_val} for col: {pred_col}... Subsetting through the middle\")\n",
    "                mid_val = len(df_bal) // 2\n",
    "                \n",
    "                df_bal.loc[:mid_val, pred_col] = \"0\"\n",
    "                df_bal.loc[mid_val:, pred_col] = \"1\"\n",
    "            \n",
    "            # Now lets pass this dataframe into our train method\n",
    "            acc = train_model(df_bal, pred_col)\n",
    "            \n",
    "            # Store the results in our global dictionary\n",
    "            results_dict[task_type].append({\n",
    "                \"fix_val\": fix_val,\n",
    "                \"pred_val\": pred_val,\n",
    "                \"min_group_size\": min_group_size,\n",
    "                \"df_size\": len(df_bal),\n",
    "                \"acc\": acc,\n",
    "            })\n",
    "            \n",
    "def train_model(df, pred_col, max_iter=5000):\n",
    "    X, y = df.drop(columns=pred_col), df[pred_col]\n",
    "    \n",
    "    model = make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-5, max_iter=max_iter))\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    return acc            \n",
    "            \n",
    "def main():\n",
    "#     wandb.init(project=\"ood-generalization\",\n",
    "#             job_type=\"emb_train\", \n",
    "#             entity=\"basedrhys\",\n",
    "#               name=f\"row {row_idx}\")\n",
    "    \n",
    "    results_dict = {}\n",
    "    results_dict[\"env_pred\"] = []\n",
    "    results_dict[\"label_pred\"] = []\n",
    "\n",
    "    # Environment Prediction Task\n",
    "    env_fix_col = \"targets\"\n",
    "    env_fix_vals = [0, 1]\n",
    "\n",
    "    env_pred_col = \"env\"\n",
    "    env_pred_vals = [(\"CXP\", ), (\"MIMIC\", ), (\"NIH\", ), (\"PAD\", ), (\"CXP\",\"NIH\"), (\"CXP\",\"PAD\"), (\"MIMIC\",\"CXP\"), (\"MIMIC\",\"NIH\"), (\"MIMIC\",\"PAD\"), (\"NIH\",\"PAD\"), (\"CXP\", \"MIMIC\", \"NIH\", \"PAD\")]\n",
    "\n",
    "    train_models(df=ml_df, \n",
    "                 pred_col=env_pred_col,\n",
    "                 pred_vals=env_pred_vals,\n",
    "                 fix_col=env_fix_col,\n",
    "                 fix_vals=env_fix_vals,\n",
    "                 results_dict=results_dict,\n",
    "                 task_type=\"env_pred\")\n",
    "    \n",
    "    # Label prediction task\n",
    "    label_fix_col = \"env\"\n",
    "    label_fix_vals = [\"CXP\", \"MIMIC\", \"NIH\", \"PAD\"]\n",
    "\n",
    "    label_pred_col = \"targets\"\n",
    "    label_pred_vals = [(0, 1), (0, ), (1, )]\n",
    "    \n",
    "    train_models(df=ml_df,\n",
    "                 pred_col=label_pred_col,\n",
    "                 pred_vals=label_pred_vals,\n",
    "                 fix_col=label_fix_col,\n",
    "                 fix_vals=label_fix_vals,\n",
    "                 results_dict=results_dict,\n",
    "                 task_type=\"label_pred\")\n",
    "    print(results_dict)\n",
    "#     wandb.log(results_dict)\n",
    "    \n",
    "    output_dir = row[\"output_dir\"]\n",
    "    \n",
    "    print(\"Outputting JSON to\", output_dir)\n",
    "    \n",
    "    with open(f\"{output_dir}/emb_test_results.json\", mode=\"w\") as f:\n",
    "        json.dump(results_dict, f, indent=True)\n",
    "    \n",
    "    return results_dict\n",
    "row_idx = 2\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOdhGrbwwG71"
   },
   "source": [
    "_You don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper_\n",
    "\n",
    "Once available, I plan to compare my model performance using different datasets with the results posted in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "# Discussion\n",
    "\n",
    "## Is the paper reproducible?\n",
    "It is too early to tell right now, but at least a portion of the code provided is runnable with minimal updates. I was able to reproduce the initial dataset statistics, so at least that portion is definitely reproducible. The rest will depend on whether I am able to solve and run the training.\n",
    "\n",
    "## If the paper is _not_ reproducible, explain the results\n",
    "TBD depending on whether the paper results will be reproducible or not.\n",
    "\n",
    "## What was easy and what was difficult\n",
    "The authors did a great job documenting some parts of the project, for example, access to data. Following the instructions was very easy, and while MIMIC-CXR-JPG dataset access took some time to get, overall the process was a breeze.\n",
    "\n",
    "Downloading the datasets is a hassle though, I ran out of space on my laptop, had to buy an external drive and restart the download process for MIMIC-CXR-JPG a few times.\n",
    "\n",
    "There are a few notebooks and standalone scripts provided to process the data. While it is possible to figure out what steps need to be done in what order, many of the parts of the process are not documented. 'pyproject.toml' did not run successfully for me, and I've been stuck trying to figure out why and how to run it (I have a suspicion my processor architecture is not supported, but not enough experience to tell for sure yet).\n",
    "\n",
    "In parallel, I opted to re-implement the training and model validation myself. There is code for training and validation in the project, which has a lot of comments (great!), but the process itself is not well documented, so the reproducer is left figuring out which steps in the code are needed and which are not, and how to adapt it to use for their experiment. The code is very general and there is a lot of it. There are some pointers in the readme, but they are at this point not sufficient for reproducing things successfully without additional modification.\n",
    "\n",
    "wandb isn't really working for me either yet, and I am yet to figure out why it is needed and whether it is necessary to reproduce the results. \n",
    "\n",
    "The data is not processed evenly / equally for each dataset, there are different values for the same labels (NaN, True/False, 1/0, 1.1/0.0). I had to write some processing code to make sure we mitigate those differences. \n",
    "\n",
    "Additional complication is due to the fact that the amount of data is very large. Any training or processing takes a long time, the notebook kernel dies frequently and the overall process is frustrating.\n",
    "\n",
    "I tried to avoid multiple separate files and scripts, and pulled many of the data preprocessing into my notebook. However, this increased the runtime of the notebook significantly. Additional factor affecting the runtime is the size of the input data, even when working on one dataset. I doubt it would be possible to achieve the 8 minute runtime, but will try to do so.\n",
    "  \n",
    "## Suggestions for the author\n",
    "\n",
    "Trim the codebase leaving only relevant parts. Add documentation for the training and validation process. Add some background on why wandb is used and how to use it for this project correctly. Provide a suggested order of execution for the notebooks.\n",
    "\n",
    "## Plans for the next phase\n",
    "\n",
    "In the remaining time until the final submission May 7 deadline, my plans are:\n",
    "* Further update the data processing functions so they are producing similar type results (right now while compatible, it's a mix of _int_, _float_ and _True/False_, I would like to homogenize the resulting dataset further)\n",
    "* Finalize the training for the model and compute worst per-group accuracy for all data combinations listed in the article (so far I had the most issues with training, as the code supplied with the article didn't work and I had to come up with my own in which I try to replicate the experiment as close to the article description as possible)\n",
    "* Plot the results and finalize the writeup (compute both worst per-group accuracy, and AUROC and compare to the results of the article)\n",
    "* Prepare a subset of data and optimize the notebook to run under 8 minutes if at all possible (as per the original requirements) - this might be complicated as the main focus of this article is dealing with more data and all datasets are quite large. Randomly picking samples from each dataset might further introduce some unintended spurious correlations\n",
    "* Check the main hypothesis on both balanced and unbalanced datasets, time permitting \n",
    "* Prepare a video presentation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "# References\n",
    "\n",
    "1.   Rhys Compton; Lily Zhang; Aahlad Puli; Rajesh Ranganath, When More is Less: Incorporating Additional Datasets Can Hurt Performance By Introducing Spurious Correlations, arXiv preprint, 2023-08-09, Accepted at MLHC 2023, doi: [10.48550/arXiv.2308.04431](https://doi.org/10.48550/arXiv.2308.04431)\n",
    "2.   Haoran Zhang, Natalie Dullerud, Laleh Seyyed-Kalantari, Quaid Morris, Shalmali Joshi, and Marzyeh Ghassemi. An empirical framework for domain generalization in clinical settings. In Proceedings of the Conference on Health, Inference, and Learning, pages 279â€“290, 2021, doi: [10.48550/arXiv.2103.11163](https://doi.org/10.48550/arXiv.2103.11163)\n",
    "3.   Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700â€“4708, 2017, doi: [10.48550/arXiv.1608.06993](https://doi.org/10.48550/arXiv.1608.06993)\n",
    "4.   Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large- scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248â€“255, 2009, doi: [10.1109/CVPR.2009.5206848](https://doi.org/10.1109/CVPR.2009.5206848)\n",
    "5.   John R Zech, Marcus A Badgeley, Manway Liu, Anthony B Costa, Joseph J Titano, and Eric Karl Oermann. Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: a cross-sectional study. PLoS medicine, 15(11): e1002683, 2018, doi: [10.1371/journal.pmed.1002683](https://doi.org/10.1371/journal.pmed.1002683)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
